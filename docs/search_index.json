[["index.html", "Theory Construction and Statistical Modeling Course Information", " Theory Construction and Statistical Modeling Kyle M. Lang Last updated: 2022-10-25 Course Information In order to test a theory, we must express the theory as a statistical model and then test this model on quantitative (numeric) data. In this course we will use datasets from different disciplines within the social sciences (educational sciences, psychology, and sociology) to explain and illustrate theories and practices that are used in all social science disciplines to statistically model social science theories. This course uses existing tutorial datasets to practice the process of translating verbal theories into testable statistical models. If you are interested in the methods of acquiring high quality data to test your own theory, we recommend following the course Conducting a Survey which is taught from November to January. Most information about the course is available in this GitBook. Course-related communication will be through https://uu.blackboard.com (Log in with your student ID and password). "],["acknowledgement.html", "Acknowledgement", " Acknowledgement This course was originally developed by dr. Caspar van Lissa. Indeed, you will still see Caspar in the lecture recordings. I (dr. Kyle M. Lang) have modified Caspar’s original materials and take full responsibility for any errors or inaccuracies introduced through these modifications. Credit for any particularly effective piece of pedagogy should probably go to Caspar. You can view the original version of this course here on Caspar’s GitHub page. "],["instructors.html", "Instructors", " Instructors Coordinator: dr. Kyle M. Lang Lectures: dr. Kyle M. Lang Practicals: Rianne Kraakman Daniëlle Remmerswaal Laura Jochim "],["course-overview.html", "Course overview", " Course overview This course comprises three parts: Factor analysis: You will learn different ways of defining and estimating unobserved constructs. Path analysis: You will learn how to conduct regressions and ANOVAs as structural equation models with observed variables. Full structural equation modeling: You will combine the first two topics by estimating path models using latent variables. Each of these three themes will be evaluated with a separate assignment. Your course grade will be based on the weighted average of these three assignment grades. Schedule Course Week Calendar Week Topic Assignment 0 36 Preperation, Working w/ R 1 37 Introduction to statistical modeling 2 38 Exploratory factor analyis (EFA) 3 39 Confirmatory factor analysis (CFA) 4 40 N/A Project 1 Due 5 41 Mediation 6 42 Moderation 7 43 Full SEM Project 2 Due 8 44 Wrap-up 9 45 N/A Project 3 Due "],["learning-goals.html", "Learning goals", " Learning goals In this course you will learn how to translate a social scientific theory into a statistical model, how to analyze your data with these models, and how to interpret and report your results following APA standards. After completing the course, you will be able to: Translate a verbal theory into a conceptual model, and translate a conceptual model into a statistical model. Independently analyze data using the free, open-source statistical software R. Apply a latent variable model to a real-life problem wherein the observed variables are only indirect indicators of an unobserved construct. Use a path model to represent the hypothesized causal relations among several variables, including relationships such as mediation and moderation. Explain to a fellow student how structural equation modeling combines latent variable models with path models and the benefits of doing so. Reflect critically on the decisions involved in defining and estimating structural equation models. "],["resources.html", "Resources", " Resources Literature You do not need a separate book for this course! Most of the information is contained within this GitBook and the course readings (which you will be able to access via links in this GitBook). All literature is freely available online, as long as you are logging in from within the UU-domain (i.e., from the UU campus or through an appropriate VPN). All readings are linked in this GitBook via either direct download links or DOIs. If you run into any trouble accessing a given article, searching for the title using Google Scholar or the University Library will probably due the trick. Software You will do all of your statistical analyses with the statistical programming language/environment R and the add-on package lavaan. If you want to expand your learning, you can follow this excellent lavaan tutorial. Doing so is entirely optional, though. "],["reading-questions.html", "Reading questions", " Reading questions Along with every article, we will provide reading questions. You will not be graded on the reading questions, but it is important to prepare the reading questions before every lecture. The reading questions serve several important purposes: Provide relevant background knowledge for the lecture Help you recognize and understand the key terms and concepts Make you aware of important publications that shaped the field Help you extract the relevant insights from the literature "],["weekly-preparation.html", "Weekly preparation", " Weekly preparation Before every class meeting (both lectures and practicals) you need to do the assigned homework (delineated in the GitBook chapter for that week). This course follows a flipped classroom procedure, so you must complete the weekly homework to meaningfully participate in, and benefit from, the class meetings. Background knowledge We assume you have basic knowledge about multivariate statistics before entering this course. You do not need any prior experience working with R. If you wish to refresh your knowledge, we recommend the chapters on ANOVA, multiple regression, and exploratory factor analysis from Field’s Discovering Statistics using R. If you cannot access the Field book, many other introductory statistics textbooks cover these topics equally well. So, use whatever you have lying around from past statistics courses. You could also try one of the following open-access options: Applied Statistics with R Introduction to Modern Statistics Introduction to Statistical Learning "],["grading.html", "Grading", " Grading Your grade for the course is based on a “portfolio” composed of the three take-home assignments: Latent variable modeling Deadline: 2022-10-07 23:59 Group assignment Contributes 25% of your course grade Path modeling Deadline: 2022-10-28 23:59 Group assignment Contributes 25% of your course grade Full Structural equation modeling Deadline: 2022-11-11 23:59 Individual assignment Contributes 50% of your course grade The specifics of the assignments are explicated in the Assignments chapter of this GitBook "],["attendance.html", "Attendance", " Attendance Attendance is not mandatory, but we strongly encourage you to attend all lectures and practicals. In our experience, students who actively participate tend to pass the course, whereas those who do not participate tend to drop out or fail. The lectures and practicals build on each other, so, in the unfortunate event that you have to miss a class meeting, please make sure you have caught up with the material before the next session. "],["assignments.html", "Assignments", " Assignments This chapter contains the details and binding information about the three assignments that comprise the portfolio upon which your course grade is based. For each assignment, you will use R to analyze some real-world data, and you will write up your results in a concise report. The required components of these analyses/reports are delineated in the following three sections. You will submit this report via Blackboard. You will complete the first two assignments in your Assignment Group. You will complete the third assignment individually. The first two assignments each contribute 25% of your course grade. The third assignment contributes 50% of your course grade. "],["assignment-1-latent-variable-model.html", "Assignment 1: Latent Variable Model", " Assignment 1: Latent Variable Model In the first assignment, you will work in groups to apply a latent variable model to a real-world problem wherein the observed variables are indirect indicators of an unobserved social scientific construct. The components of the first assignment are described below. Find a suitable dataset, and describe the data. (150 words). Data you have collected yourself (e.g., for a previous course) Open data (e.g., data provided with a published paper) The “Coping with COVID-19” dataset (if you can’t find anything else) State the research question, and define the theoretical latent variable model. (150 words) Be sure to clearly explicate all parts of your model. If you do an EFA, you won’t have a single pre-defined model to describe. Describe your analytic process. Explicate all the settings you will use (e.g., items analyzed, estimation method, rotation). Translate your theoretical latent variable model into R code, and estimate the model. Include the code used to define and estimate your model as an appendix. Explain your rationale for important modeling decisions. (300 words) Motivate your choice of latent variable model (i.e., EFA, CFA, PCA). Discuss assumptions. Discuss other important decisions that could have influence your results. Report and interpret the results in APA style. Provide relevant output in a suitable format. Include measures of model fit, if applicable. Discuss the results. (300 words) Use your results to answer the research question. Consider the strengths and limitations of your analysis. Evaluation You can find a rubric delineating the evaluation criteria for each component of the first assignment here. See the Grading section below for more information on how the component scores represented in the rubric are combined into an overall assignment grade. Submission Assignment 1 is due at 23:59 on Friday 7 October 2022 Submit your report via the Assignment 1 portal on Blackboard. "],["assignment-2-path-model.html", "Assignment 2: Path Model", " Assignment 2: Path Model For the second assignment, you will work in groups to apply a path model that describes how several variables could be causally related. The components of the second assignment are described below. Find a suitable dataset, and describe the data. (250 words) Data you have collected yourself (e.g., for a previous course) Open data (e.g., data provided with a published paper) The “Coping with COVID-19” dataset (if you can’t find anything else) State the research question, and define the theoretical path model. (200 words) This model can be a re-analysis of a question that was originally tested using regression or ANOVA in a published paper. Translate your theoretical path model into lavaan syntax, and estimate the model. Include the code used to define and estimate your model as an appendix. Explain your rationale for important modeling decisions. (300 words) Discuss the conceptual fit between your theory and your model. Evaluate the model assumptions. If applicable, discuss the differences and similarities between your path model and the regression or ANOVA results from the original paper. Discuss other important decisions that could have influence your results. Report and interpret the results in APA style. Provide relevant output in a suitable format. Include measures of explained variance for the dependent variables. Discuss the results. (300 words) Use your results to answer the research question. Consider the strengths and limitations of your analysis. Evaluation You can find a rubric delineating the evaluation criteria for each component of the first assignment here. See the Grading section below for more information on how the component scores represented in the rubric are combined into an overall assignment grade. Submission Assignment 2 is due at 23:59 on Friday 28 October 2022 Submit your report via the Assignment 2 portal on Blackboard. "],["assignment-3-full-structural-equation-model.html", "Assignment 3: Full Structural Equation Model", " Assignment 3: Full Structural Equation Model In the third assignment, you will work individually to apply a full SEM that describes how several (latent) variables could be causally related. The components of the third assignment are described below. Find a suitable dataset, and describe the data. (250 words) Data you have collected yourself (e.g., for a previous course) Open data (e.g., data provided with a published paper) The “Coping with COVID-19” dataset (if you can’t find anything else) State the research question, and define the theoretical SEM. (300 words) This model can be a re-analysis of a question that was originally tested using regression or ANOVA in a published paper. The structural component of this model must include, at least, three variables. The model must include, at least, one latent variable. Translate your theoretical SEM into lavaan syntax, and estimate the model. Include the code used to define and estimate your model as an appendix. Explain your rationale for important modeling decisions. (400 words) Discuss the conceptual fit between your theory and your model. Evaluate the model assumptions. If applicable, discuss the differences and similarities between your path model and the regression or ANOVA results from the original paper. Discuss other important decisions that could have influence your results. Report and interpret the results in APA style. Provide relevant output in a suitable format. Include measures of model fit. Include measures of explained variance for the dependent variables. Discuss the results. (500 words) Use your results to answer the research question. Consider the strengths and limitations of your analysis. Submission Assignment 3 is due at 23:59 on Friday 11 November 2022 Submit your report via the Assignment 3 portal on Blackboard. "],["data.html", "Data", " Data If you have your own data, that’s great, and we strongly encourage you to use those data for the projects. That being said, most students will not have their own data. If you’re one of the majority without data, you’ll have to find an existing dataset to analyze (in case you were wondering, you won’t have time to collect new data). Sources Those who need data can make use of one of the myriad publicly available datasets available online. Of course these datasets will vary widely in terms of their applicability to the project and the amount of work required to prepare the variables for analysis. The following links point to various sources of publicly available data. Kaggle datasets GSS Sata Explorer StatLine Open Data Open Data from the Dutch Government Google Cloud Platform Google dataset search Tips When searching for a suitable dataset, consider the following tips: It will take longer than you expect. Start searching for data early. Plan on devoting a good chunk of time to tracking down a useful dataset. Realize that even after you find the data, you will also have to do a good bit of cleaning to get the variables ready for analysis. Expect to be disappointed. You may find a dataset that looks good on paper but won’t work for the project (e.g., too much missing data, poor coverage for key relations, jenky distributions, etc.). You usually won’t discover these issue until you’ve already downloaded the data and started exploring/cleaning. Make sure you check the actual data you’ve downloaded before you declare success in your search. Keep the project requirements in mind when searching for data. You will need, at least, three interesting variables to model in some sort of causal process (required for Assignment 3). You will need, at least, one latent variable (required for Assignments 1 &amp; 3). To define a latent variable, you need multiple items (at least three) measuring the same hypothetical construct. In practice, these multiple indicators usually come from scales (e.g., NEO-PI Big Five Personality Inventory, Beck Depression Inventory). If you find a dataset that looks suitable, but you want to confirm, send me (Kyle) a link to the dataset, and I’ll let you know if it will work for the assignments. "],["procedures.html", "Procedures", " Procedures Formating You must submit your assignment reports in PDF format. Each report should include a title page. The title page should include the following information: The name of the assignment. The names of all assignment authors (i.e., all group members for Assignments 1 &amp; 2, your name for Assignment 3). The Assignment Group number (only for Assignments 1 &amp; 2). You must include the code used to define and run your model(s) as an appendix. Try to format the text in this appendix clearly. Use a monospace font. Length In the preceding three sections, some line-items in the descriptions of the assignments concluded with a word count. These sections of your report must conform to the indicated word counts. E.g., the Data section of Assignment 1 must contain no more that 150 words, and the Discussion section of Assignment 3 must contain no more than 500 words. Include the word counts for these sections as part of the section heading (e.g., in parentheses following the section title). You only need to include word counts for the sections for which a word count is indicated above. In all other sections, you may use as many words as necessary to adequately explain yourself (though concision and parsimony are still encouraged). Note that the assignments are not intended to be full-blown papers! You only get a few hundred words to describe your data, justify your theoretical model, and discuss the results. The focus should be on the definition of your model, how this model relates to theory (introduction), and what you have learned from your estimated model (discussion). Submission You will submit your reports through Blackboard. Each assignment has a corresponding item in the “Assignments” section of the BB page through which you will submit your reports. For Assignments 1 &amp; 2, you may only submit one report per group. Designate one group member to submit the report. The grade for this submission will apply to all group members. If something goes wrong with the submission, or you notice a mistake (before the deadline) that you want to correct, you may upload a new version of your report. We will grade the final submitted version. The submissions will be screened with SafeAssign. "],["grading-1.html", "Grading", " Grading Each assignment comprises 6 components (i.e., the line-items labelled 1–6 in the assignment descriptions). Each of these 6 elements will be scored as: Insufficient (0 points) Sufficient (1 point) Excellent (1.5 points) The grader may also choose to assign partial points for some sections. There is also one point awarded for satisfying the formatting and submission requirements. The individual assignment grades will be computed as follows: Sum the 6 component grades. Add the formatting/submission score to the sum of the component grades. So, given a full point for formatting/submission, we have the following landmarks: All components marked as Insufficient \\(\\rightarrow\\) Assignment Grade = 1 All components marked as Sufficient \\(\\rightarrow\\) Assignment Grade = 7 All components marked as Excellent \\(\\rightarrow\\) Assignment Grade = 10 The final course grade, \\(G_{course}\\), will be computed as the weighted average of the three individual assignment grades, \\(G_{a1}\\), \\(G_{a2}\\), \\(G_{a3}\\): \\[ G_{course} = 0.25 G_{a1} + 0.25 G_{a2} + 0.5 G_{a3} \\] "],["rules.html", "Rules", " Rules Resources For all three assignments, you may use any reference materials you like, including: All course materials The course GitBook Additional books and papers The internet Collaboration You will complete the first two assignments in groups. Although you will work in groups, your group may not work together with other groups. You will complete the final assignment individually. For this assignment, you may not work with anyone else. For all three assignments, you are obligated to submit original work (i.e., work conducted for this course by you or your group). Submitting an assignment that violates this condition constitutes fraud. Such cases of fraud will be addressed according to the University’s standard policy. Academic integrity Hopefully, you also feel a moral obligation to obey the rules. For this course, we have implemented an examination that allows you to showcase what you have learned in a more realistic way than a written exam would allow. This assessment format spares you the stress of long exams (the two exams for this course used to be 4 hours each) and the attendant studying/cramming. The assignments will also help you assess your ability to independently analyse data, which is important to know for your future courses and/or career. However, this format also assumes that you complete the assignments in good faith. So, I simply ask that you hold up your end of the bargain, and submit your original work to show us what you’ve learned. Strict stuff By submitting your assignments (both group and individual), you confirm the following: You have completed the assignment yourself (or with your group) You are submitting work that you have written yourself (or with your group) You are using your own UU credentials to submit the assignment You have not had outside help that violates the conditions delineated above while completing the assignment All assignments will be submitted via SafeAssign in Blackboard and, thereby, checked for plagiarism. If fraud or plagiarism is detected or suspected, we will inform the Board of Examiners in the usual manner. In the event of demonstrable fraud, the sanctions delineated in Article 5.15 of the Education and Examination Regulations (EER) will apply. "],["software-setup.html", "Software Setup", " Software Setup This chapter will help you prepare for the course by showing how to install R and RStudio on your computer. If you’re already using R, there may be nothing new for you here. That being said, you should look over this chapter to ensure that your current setup will be compatible with the course requirements. If you have never used R before, this chapter is essential! The information is this chapter will be crucial for getting your computer ready for the course. "],["typographic-conventions.html", "0.1 Typographic Conventions", " 0.1 Typographic Conventions Throughout this GitBook, we (try to) use a consistent set of typographic conventions: Functions are typeset in a code font, and the name of the function is always followed by parentheses E.g., sum(), mean() Other R objects (e.g., data objects, function arguments) are in also typeset in a code font but without parentheses E.g., seTE, method.tau Sometimes, we’ll use the package name followed by two colons (::, the so-called *scope-resolution operator), like lavaan::sem(). This command is valid R code and will run if you copy it into your R console. The lavaan:: part of the command tells R that we want to use the sem() from the lavaan package. "],["installing-software.html", "0.2 Installing software", " 0.2 Installing software Before we start the course, we have to install three things: R: A free program for statistical programming RStudio: An integrated development environment (IDE) which makes it easier to work with R. Several packages: Separate pieces of ‘add-on’ software for R with functions to do specific analyses. Packages also include documentation describing how to use their functions and sample data. 0.2.1 Installing R The latest version of R is available here. Click the appropriate link for your operating system and follow the instructions for installing the latest stable release. Depending on which OS you select, you may be given an option to install different components (e.g., base, contrib, Rtools). For this course, you will only need the base package. 0.2.2 Installing RStudio Download the Free Desktop version of RStudio from the download page of the RStudio website. 0.2.3 Installing packages To participate in this course, you will need a few essential R packages. Here’s an overview of the packages and why we need them: Package Description lavaan A sophisticated and user-friendly package for structural equation modeling ggplot2 A flexible and user-friendly package for making graphs tidySEM Plotting and tabulating the output of SEM-models semTools Comparing models, establishing measurement invariance across groups psych Descriptive statistics foreign Loading data from SPSS ‘.sav’ files readxl Loading data from Excel ‘.xslx’ files To install these packages, we use the install.packages() function in R. Open RStudio Inside RStudio, find the window named Console on left side of the screen. Copy the following code into the console and hit Enter/Return to run the command. install.packages(c(&quot;lavaan&quot;, &quot;ggplot2&quot;, &quot;tidySEM&quot;, &quot;semTools&quot;, &quot;psych&quot;, &quot;foreign&quot;, &quot;readxl&quot;), dependencies = TRUE) "],["getting-the-course-data.html", "0.3 Getting the course data", " 0.3 Getting the course data All of the data files you will need for the course are available in this SurfDrive directory. Follow the link to download a ZIP archive containing the data you will need to complete the practical exercises. Extract these data files to a convenient location on your computer. "],["r-tutorial.html", "R Tutorial", " R Tutorial Welcome to the world of R! This chapter provides a tutorial based on “R: How to get started” by Ihnwhi Heo, Duco Veen, and Rens van de Schoot. If you already have a good deal of R experience, and you don’t see anything new in this tutorial, feel free to skip this chapter. If you haven’t worked with R before, I strongly encourage you to spend some time carefully working through this tutorial. The content of this chapter will go a long way towards familiarizing you with the basic R commands that you will need to use at the beginning of the course. "],["who-r-you.html", "0.4 Who R you?", " 0.4 Who R you? R is a piece of free software for statistical computation and graphics. R is also fully open-source, which means anyone (even you!) can improve, develop, and contribute to R You can find the official manual from the R Core Team here: An introduction to R R itself looks a bit old-fashioned and tedious: "],["rstudio.html", "0.5 RStudio", " 0.5 RStudio Thankfully, we have a great user interface for R called RStudio! RStudio helps you use and learn R more easily. Although you are interacting with RStudio, you are still using R. Don’t ever write something like, “We conducted all analysis with RStudio…” For this course, all tutorials and practicals will use RStudio. 0.5.1 No ‘pane’, no gain! When you open RStudio, the screen should look something like the following image. You will notice that the window is divided into ‘panes’ (a pane is a division of a window). Before we explain these three panes, I want you to add another, which you will see if you open an R script. An R script is simply a plain-text file wherein you will write the R code that executes your analyses. When you open an R script (or create a new one, which is really the same thing), a fourth pane appears. 0.5.2 Create a new R script To create a new R script, Click the icon with a plus sign on the paper (highlighted below by the red square). When you click this icon, a new script is generated and appears in a fourth pane on the upper left side of the screen (if you’re using the default layout). Note that this script is not yet saved anywhere. If you close this script without saving, you will lose all its contents. The four panes help organize your workflow. Rstudio is an integrated development environment (IDE) that is meant to integrate all of the tasks you may need to do for any R-based analysis, programming, or development. Hence, in RStudio, the intended workflow will have you do everything in one window, and the four panes make this workflow more efficient (in theory, at least). 0.5.3 What do the four panes do? Note that the following descriptions apply to the default layout. You can change the orientation and content of the panes (although you must always have four). Source Pane: Located in the top left quadrant. This pane is also called the “Editor”, because this is where we edit scripts. We will usually type our code in the source pane. Console Pane: Located in the bottom left quadrant. This pane is for direct communication with R. We can type commands here that are immediately evaluated (whereas the commands in a script are only evaluated when we explicitly run them). Furthermore, all output from the R commands that we run (either via the console or a script) is printed in the console pane. The two panes on the right side of the window contain various tabs. Two of these tabs are especially useful. Environment Tab: Located in the upper right quadrant The environment tab contains all the ‘objects’ currently loaded in your R session. You can always check what objects are loaded under the environment tab. The ‘environment’ is also called the ‘workspace’ Plots Tab: Located in the lower right quadrant The plots tab shows any graphs and figures we draw via R commands. If you zoom by clicking the magnifying glass icon, you can see enlarged versions of the plots. "],["rstudio-projects.html", "0.6 RStudio projects", " 0.6 RStudio projects To keep all your work organized, you can use an RStudio Project. One advantage of using RStudio projects is that the project directory is automatically set as the working directory. If you save your data in the directory that contains the “.Rproj” file, you will can load the data without specifying the file path. 0.6.1 Starting a new project in Rstudio In Rstudio, click on the New project button: In the pop-up dialog, click New Directory. Click New Project. Type the desired directory name in the dialog box Give a meaningful name, e.g., “TCSM_Course”. Use the Browse button if you need to change the location of the directory that you will use to store this project. "],["loading-data.html", "0.7 Loading data", " 0.7 Loading data Statistical analysis cannot happen without data. In R, you can load data in various ways. Let’s go over a couple of these. To complete the following exercise, first download the LifeSat.sav data. 0.7.1 Via clicky-box options Click through the following menu options: File &gt; Import Dataset Choose the type of dataset. For this exercise, the data are stored as an SPSS .sav file. Thus, select the From SPSS option. You may encounter an Install Required Packages pop-up dialog with a message that asks you whether you want to install the haven package. A package is an a piece of add-on software for R. To do most analyses, you will need some packages above-and-beyond the Base R software. In this case, haven contains functions that allow R to read SPSS data files. Click ‘Yes’ to install the package. At this point, the Import Statistical Data pop-up dialog should appear. Provide the location of your data file in the File/URL field. You can type the file path directly. You can also click Browse to find the file via a GUI interface. You should now see your data in the Data Preview area. In the Import Options section, you can set the name and format of your data file. All of your your selections are being translated to the R-code required to load your file. In the Code Preview area, the steps required to load the data with your selected options are expressed in terms of code. Finally, click Import at the lower right side of the window to load your data. 0.7.2 Via R code Of course, we don’t have to use tedious clicky-box processes. Once you get more familiar with R, it will be much easier to load data with R code. For this exercise, we will use the read.spss() function from the foreign package. Copy the following code into an R-script. Note that this code assumes you have saved your data in the working directory (this is the project directory if you’re using an R project). Otherwise, you will need to specify the file path to your data file as the first argument. library(foreign) LifeSat &lt;- read.spss(&quot;LifeSat.sav&quot;, to.data.frame = TRUE, use.value.labels = FALSE) To run the code: Place your cursor on the first line of code. Press the Ctrl/Cmd + Enter keys together. The code should now be evaluated by R, and R will tell you the result of its actions in the console pane. 0.7.2.1 Data frames When we read data into R, the data are most often stored as a special type of object called a data frame. Data frames are the preferred way to handle data in R. Data frames can hold variables with different types (e.g., a numeric depression rating, a categorical grouping factor representing employment status, and a character string recording the response to an open-ended question). The option to.data.frame = TRUE in the read.spss() call above loads the data into a data frame. 0.7.3 From Excel files We can also load data that are stored in Excel files. One way to do so is via the readxl package. ## Load the package: library(readxl) ## Read the first sheet of &#39;LifeSat.xlsx&#39; into &#39;LifeSat&#39;: LifeSat &lt;- read_excel(&quot;LifeSat.xlsx&quot;, sheet = 1) "],["exploring-data-via-r-functions.html", "0.8 Exploring data via R functions", " 0.8 Exploring data via R functions If you look a bit more closely at the R code that we used to load the data, you will see two special commands: library() and read.spss(). These commands are called functions. Any R command that is written as a name followed by parentheses, e.g., mean(), is a function. Functions are the driving force behind all R-based data analysis. Functions tell R to perform a specific (potentially very complicated) task. Rather than having to write out along sequence of commands every time we want to do some task, we can simply call the appropriate function. When using functions, you need to provide appropriate inputs to specify the behavior of the function and give the function data on which to operate. These inputs are called function arguments. Let’s explore three new functions and their arguments. These functions can help us understand our data. 0.8.1 head() Our data comprise many rows. We can use the head() function to inspect the first several rows. To use the head() function, you only need to provide one argument: The name of the dataset. ## Inspect the first several rows head(LifeSat) “Wait, what is the hash tag (#) doing there?” Don’t be surprised. The hash tag creates a “comment”. I.e., a bit of text that will not be evaluated by R. Comments let us write notes to explain what a particular piece of code does. Comments are doubly useful. They can help others understand your code. They can also help you remember what the code does after some time away. Copy the preceding code into your R script and run it. If all went well, R should now show the first six rows of the dataset in the console. 0.8.2 str() We frequently want to know something about the types of variables in a dataset. For example, if you want to run an analysis of variance (ANOVA), the independent variable(s) should be categorical. In R, these variables would be represented by a special type of variable called a factor. Before running our analysis, we should check if are data satisfy this requirement. We can use the str() function to get some information about the structure of an R object (str is an abbreviation of structure). To run the str() function, you only need to provide one argument: the name of the dataset. ## Inspect the structure of the dataset str(LifeSat) ## &#39;data.frame&#39;: 98 obs. of 8 variables: ## $ LifSat : num 13 18 19 24 24 24 30 33 33 33 ... ## $ age : num 75 75 72 72 70 73 72 72 68 73 ... ## $ educ : num 6 5 5 6 5 6 6 5 7 6 ... ## $ gender : num 2 2 2 2 1 2 1 2 1 1 ... ## ..- attr(*, &quot;value.labels&quot;)= Named chr [1:2] &quot;2&quot; &quot;1&quot; ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;male&quot; &quot;female&quot; ## $ female : num 0 0 0 0 1 0 1 0 1 1 ... ## $ ChildSup: num 4 6 6 6 6 8 8 7 4 8 ... ## $ SpouSup : num 2 5 5 4 5 6 4 6 2 8 ... ## $ SES : num 3 1 1 1 1 1 1 2 2 2 ... ## ..- attr(*, &quot;value.labels&quot;)= Named chr [1:3] &quot;3&quot; &quot;2&quot; &quot;1&quot; ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;high SES&quot; &quot;middle SES&quot; &quot;low SES&quot; ## - attr(*, &quot;variable.labels&quot;)= Named chr [1:8] &quot;&quot; &quot;&quot; &quot;Years of education&quot; &quot;&quot; ... ## ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;LifSat&quot; &quot;age&quot; &quot;educ&quot; &quot;gender&quot; ... ## - attr(*, &quot;codepage&quot;)= int 1252 According to the output printed to the console, the LifeSat dataset is a data frame consisting of 98 observations of 8 variables. I.e., our dataset has 98 rows and 8 columns. We also get information about the contents of each column. For the fist column, we see $ LifSat : num 13 18 19 24 24 24 30 33 33 33 ... The LifSat variable is numeric (abbreviated as num). 13 is the value in the first row, 18 is the value in the second row, 19 is the value in the third row, and so on. 0.8.3 summary() We can get the descriptive statistics of the variables in a dataset with the summary() function. Again, we only need to provide one argument: the name of the dataset. summary(LifeSat) ## LifSat age educ gender ## Min. : 13.00 Min. :64.00 Min. : 4.000 Min. :1.000 ## 1st Qu.: 44.25 1st Qu.:68.25 1st Qu.: 6.000 1st Qu.:1.000 ## Median : 58.00 Median :70.00 Median : 6.000 Median :1.000 ## Mean : 57.86 Mean :70.26 Mean : 6.541 Mean :1.449 ## 3rd Qu.: 69.75 3rd Qu.:72.00 3rd Qu.: 7.000 3rd Qu.:2.000 ## Max. :100.00 Max. :75.00 Max. :12.000 Max. :2.000 ## female ChildSup SpouSup SES ## Min. :0.000 Min. : 3.000 Min. : 2.000 Min. :1.000 ## 1st Qu.:0.000 1st Qu.: 6.000 1st Qu.: 5.000 1st Qu.:1.000 ## Median :1.000 Median : 7.000 Median : 6.000 Median :2.000 ## Mean :0.551 Mean : 6.857 Mean : 6.061 Mean :1.918 ## 3rd Qu.:1.000 3rd Qu.: 8.000 3rd Qu.: 8.000 3rd Qu.:2.000 ## Max. :1.000 Max. :10.000 Max. :10.000 Max. :3.000 Run the above code and check the output in the console. You should see descriptive statistics for each variable in the dataset. E.g., for LifSat, the minimum value is 13, the median is 58, and the mean is 57.86. "],["tutorialPlotting.html", "0.9 Plotting data", " 0.9 Plotting data It is almost always a good idea to visualize your data before you dive into a full statistical analysis. For example, you may like to know something about the nature of the relationship between two specific variables, the distribution of some set of values, etc. In this section, we will create three basic plots of our data. When we create these plots, the figures will appear in the plots tab in the lower right quadrant of the RStudio window. 0.9.1 R packages and ggplot2 We will use the ggplot2 package for plotting. This package is not part of the Base R installation. So, you must install ggplot2 yourself (you should have already done so in Section 0.2.3). You only need to install an R package once. R packages are just small software programs, so they must be installed just like any other piece of software. After installing, the package is accessible on your computer but not yet available for use in your current R session. You must first load the package. You need to load the package every time you want to use it in a new R session. After loading, the package contents (i.e., functions, help files, datasets) are initialized in memory and ready to use in your current R session. Loading a package is more-or-less equivalent to ‘opening’ a software program by clicking on its desktop icon. Run the following code to load the ggplot2 package. library(ggplot2) 0.9.2 Histogram We can use a histogram to visualize how the values of a continuous variable are distributed. ## Use the &#39;LifeSat&#39; data to create the plot ## Use the &#39;LifeSat$LifSat&#39; variable to define the X-axis ggplot(data = LifeSat, aes(x = LifSat)) + geom_histogram() # Create a histogram from the data/variable defined above 0.9.3 Boxplot A boxplot provides another useful visualization of a continuous variable’s distribution. We can also use boxplots to detect outliers. ## Same data/variable setup as above: ggplot(data = LifeSat, aes(x = LifSat)) + geom_boxplot() # Create a boxplot 0.9.4 Scatterplot A scatterplot provides a visual representation of the relationship between two variables. Since we are now plotting two variables, we need to define a y variable in addition to thex variable specified in the previous examples. ## Add the &#39;age&#39; variable on the Y-axis: ggplot(data = LifeSat, aes(x = LifSat, y = age)) + geom_point() # Create a scatterplot "],["manipulating-data.html", "0.10 Manipulating Data", " 0.10 Manipulating Data 0.10.1 Data types Recall the output of the str() function. One piece of information contained therein is the type of data stored in each column of our dataset. There a different abbreviations signifying different types of data. Abbreviation Type Description num Numeric All values are numbers (e.g., 1.02) chr Character All values are words log Logical Boolean flags: TRUE or FALSE factor Factor A special type of object with labels to represent the levels of a categorical variable 0.10.2 Factors Factors are a special type of data object that R uses to represent categorical variables. A factor is stored internally as a vector of integers where each group is represented by a different number. The groups also get descriptive labels. R knows that a factor is not numeric and will treat any factor as a nominal grouping variable for anlaysis. In the output from the str() function in 0.8.2, we see that the gender variable is stored as a numeric variable. You can confirm by running either of the following commands. is.numeric(LifeSat$gender) ## [1] TRUE class(LifeSat$gender) ## [1] &quot;numeric&quot; The gender varible is a binary grouping variable, so it should be stored as a factor. To convert gender to a factor, we can use the factor() function. ## Convert &#39;gender&#39; to a factor: LifeSat$gender &lt;- factor(LifeSat$gender) ## Check the results: is.numeric(LifeSat$gender) ## [1] FALSE class(LifeSat$gender) ## [1] &quot;factor&quot; str(LifeSat$gender) ## Factor w/ 2 levels &quot;1&quot;,&quot;2&quot;: 2 2 2 2 1 2 1 2 1 1 ... We now see that gender has been converted to a factor with the levels “1” and “2”. We don’t have to settle for meaningless numeric labels, though. We can assign meaningful value labels by providing an appropriate input for the labels argument. ## Create a factor with meaningful labels: LifeSat$gender &lt;- factor(LifeSat$gender, labels = c(&quot;Female&quot;, &quot;Male&quot;)) ## Check the results: str(LifeSat$gender) ## Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 2 2 2 1 2 1 2 1 1 ... 0.10.3 Subsetting: Extracting one variable When working with data frames, we can extract a single variable (i.e., column) from the data using the dollar sign operator, $. As shown in the example below. # Extract the &#39;LifSat&#39; variable from &#39;LifeSat&#39;: LifeSat$LifSat ## [1] 13 18 19 24 24 24 30 33 33 33 33 33 35 35 37 37 41 41 41 ## [20] 43 43 43 44 44 44 45 47 48 48 48 50 51 51 52 53 53 53 53 ## [39] 54 55 55 56 56 56 57 58 58 58 58 58 58 59 59 60 61 61 63 ## [58] 63 63 65 66 67 67 67 67 68 68 68 69 69 69 69 69 70 70 70 ## [77] 71 72 74 74 76 77 77 78 78 79 79 79 81 81 82 83 85 86 87 ## [96] 91 99 100 In the above code, we ask R to extract the column named LifSat from the data frame named LifeSat. The result will be returned as another special type of object: a vector. 0.10.4 Subsetting: Extracting rows and columns We can also extract rectangular subsets of a data frame using the following convention: my_data_frame[row_numbers, column_numbers]. ## Extract the first four rows of the first two columns: LifeSat[1:4, 1:2] By leaving either rows or columns empty, we get all rows or columns: ## Extract all rows of first two columns: LifeSat[ , 1:2] ## Extract all columns of first two rows: LifeSat[1:2, ] We can refer to the columns by name, too: LifeSat[1:2, c(&quot;age&quot;, &quot;educ&quot;)] 0.10.5 Subsetting based on logical conditions We can also select rows or columns that satisfy logical conditions. In the following code, we select only the rows for which LifeSat$age is greater than 70. LifeSat[LifeSat$age &gt; 70, ] This approach works for any valid logical expression that will flag rows (or columns). Below, we select only the males and save the subset as a new object, LifeSat_male. LifeSat_male &lt;- LifeSat[LifeSat$gender == &quot;Male&quot;, ] str(LifeSat_male) ## &#39;data.frame&#39;: 44 obs. of 8 variables: ## $ LifSat : num 13 18 19 24 24 33 33 35 35 37 ... ## $ age : num 75 75 72 72 73 72 72 68 71 68 ... ## $ educ : num 6 5 5 6 6 5 12 6 4 7 ... ## $ gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ female : num 0 0 0 0 0 0 0 0 0 0 ... ## $ ChildSup: num 4 6 6 6 8 7 5 6 7 4 ... ## $ SpouSup : num 2 5 5 4 6 6 6 6 7 4 ... ## $ SES : num 3 1 1 1 1 2 2 3 1 2 ... ## - attr(*, &quot;variable.labels&quot;)= Named chr [1:8] &quot;&quot; &quot;&quot; &quot;Years of education&quot; &quot;&quot; ... ## ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;LifSat&quot; &quot;age&quot; &quot;educ&quot; &quot;gender&quot; ... ## - attr(*, &quot;codepage&quot;)= int 1252 0.10.6 Changing cell values We can easily overwrite values in a dataset using the same type of subsetting operations shown above in combination with the assignment operator, &lt;-. The following code will extract the 5th entry in the LifSat variable. LifeSat[5, &quot;LifSat&quot;] ## [1] 24 Actually, it’s not really accurate to say that the above code “extracts” any value. The above command creates a new temporary object containing only the relevant value and prints the contents of that object to the console. All the subsetting examples above (other than the selection of all males) have done something similar. Rather than thinking about the [] or $ selection operators as ways of extracting pieces of a data object, it’s more appropriate to think about these operators as selecting, highlight, activating, nominating (or some other such concept) the referenced elements. Once the elements are so selected, we can also overwrite their original values. We only need to assign new values to the subset. To demonstrate, let’s overwrite the value selected above with 10. ## Overwrite the 5th &#39;LifSat&#39; value: LifeSat[5, &quot;LifSat&quot;] &lt;- 10 ## Check the result: LifeSat[5, &quot;LifSat&quot;] ## [1] 10 "],["getting-help.html", "0.11 Getting Help", " 0.11 Getting Help As you start to apply the techniques described in this guide, you will soon have questions that the guide does not answer. This section describes a few tips on how to get help answering these questions. Every function in R has documentation (i.e., a help file). To see this file in RStudio, select the name of the function in your script, and press F1, or run the command ? followed by the name of the function (e.g., ?aov). The second option works outside of RStudio, too. If you get stuck, start with Google. Typically, adding “R” to a search is enough to return relevant results (e.g., “exploratory factor analysis R”). Google is particularly useful for error messages. If you get an error message that you don’t understand, try googling it. Someone else has almost certainly been confused by the same message in the past, and there will be help somewhere on the web. If the error message isn’t in English, run Sys.setenv(LANGUAGE = \"en\") and re-run the code that produced the error (you’re more likely to find help for English error messages.) If Google doesn’t help, try stackoverflow. Before posting your question, spend some time searching the site for an existing answer (active contributors really hate it when you ask a question that has already been answered on the site). Including [R] in your search string restricts your search to questions and answers that use R. Lastly, if you find errors (or typos!) in this guide’s text or R syntax, feel free to contact me (Kyle Lang). "],["introduction.html", "1 Introduction", " 1 Introduction Homework before the lecture Complete the preparatory material: Read over the Course Information chapter Work through the Software Setup chapter Work through the R Tutorial chapter Watch the Lecture Recording for this week. Complete the Reading for this week, and answer the associated reading questions. Complete the Formative Assessment for this week. Lecture content We start with a brief introduction to the course, the course goals and rules, and the general idea of statistical modeling. We will introduce the type of models that we will consider in this course. We will shortly discuss several related concepts: Model simplicity/complexity Model fit Graphical representations of model parameters Interpretations of model parameters Homework before the practical Complete the At-Home Exercises. Practical content During the practical you will work on the In-Class Exercises. "],["lecture.html", "1.1 Lecture", " 1.1 Lecture Download slides The first lecture will be used to introduce the concept of fitting models to data and explain some important concepts and notation that will be used during this course. "],["reading.html", "1.2 Reading", " 1.2 Reading Reference Smaldino, P. E. (2017). Models are stupid, and we need more of them. Computational Social Psychology, 311–331. SKIP PAGES 322 - 327 Questions What are the differences between a “verbal model” and a “formal model”? As explained in the paragraph “A Brief Note on Statistical Models”, formal models are not the same as statistical models. Still, we can learn a lot from Smaldino’s approach. Write down three insights from this paper that you would like to apply to your statistical modeling during this course. Answers Q1: A verbal model can be non-specific about many things The level of analysis The definitions of its parts The relationships within the modeled system Verbal models can appear powerful and useful partly because they employ strategic ambiguity. Because verbal models are so vague, they can apparently explain many different phenomena, even contradictory ones (e.g., the Cubist chicken). A formal model, on the other hand, seeks to rigorously define its level of analysis, parts (variables), and relationships between these parts. Q2: You should define the level of analysis for your model You should clearly operationalize the parts (variables) and explicitly define the hypothesized relationships (causal, correlational, definitional) between these parts The act of formally defining our models is productive. We can test a well-defined model using data. We can discuss such a model among experts with relative consensus about what the model means, and we can improve it. These things are harder to do with vague verbal models. Every model imposes some violence upon reality. I.e., every model is wrong (but some are useful). Because a model is a simplification, by necessity, we leave out many factors. This simplification is a feature, not a bug. Models abstract away the uninteresting details (i.e., noise) of a phenomenon. Even models that are too simple to be interesting represent useful building-blocks to expand upon when creating more complex, interesting models. Data can be used to validate a model and to refine the model into a closer approximation of reality "],["formative-assessment.html", "1.3 Formative Assessment", " 1.3 Formative Assessment A formative assessment helps you judge your progress in the course and helps you address any blind spots in your understanding of the material. This assessment does not contribute to your course grade. If you get a question wrong, you will receive a hint on how to improve your understanding of the material. Ideally, you should complete this assessment after you have watched the lecture recording, but before the lecture meeting. This way, if you discover gaps in your understanding, we can discuss any topics that need more attention during the lecture meeting. Question 1: What kind of model is depicted above? A path model Multiple regression ANOVA with dummies A measurement model Question 2: The error term (\\(\\epsilon_i\\)) in regression equations reflects how much the observed scores of individuals differ from their predicted scores. FALSE TRUE Question 3: In OLS regression, a model is fit to the individual participant data. By contrast, regression in structural equation modeling fits a model to the observed covariance matrix. TRUE FALSE Question 4: A model with more degrees of freedom is… more simple more complex Question 5: In the equation \\(Y_i = a + bX_i + e_i\\), what are the ‘model parameters’? \\(a\\) and \\(bX_i\\) \\(Y_i, X_i\\) and \\(e_i\\) \\(Y\\) and \\(X\\) \\(a\\) and \\(b\\) Question 6: What are degrees of freedom? The number of unique pieces of information The number of unique pieces of information minus number of parameters Number of participants minus number of parameters The number of parameters Question 7: A model with more degrees of freedom will… fit the data better fit the data worse Question 8: A psychologist administers a test intended to measure intelligence. Participants complete different puzzles and answer different questions. From a measurement theory point of view, what kind of variable is intelligence in this context? An observed variable A dependent variable A measurement variable A latent variable Question 9: Multiple regression and ANCOVA are statistically equivalent. TRUE FALSE "],["at-home-exercises.html", "1.4 At-Home Exercises", " 1.4 At-Home Exercises Load the LifeSat.sav data. library(foreign) LifeSat &lt;- read.spss(&quot;LifeSat.sav&quot;, to.data.frame = TRUE) 1.4.1 Make a table of descriptive statistics for the variables: LifSat, educ, ChildSup, SpouSup, and age. What is the average age in the sample? What is the ange (youngest and oldest child)? Hint: Use the tidySEM::descriptives() function.` Click for explanation The package tidySEM contains the descriptives() function for computing descriptive statistics. The describe() function in the psych package is a good alternative. library(tidySEM) descriptives(LifeSat[ , c(&quot;LifSat&quot;, &quot;educ&quot;, &quot;ChildSup&quot;, &quot;SpouSup&quot;, &quot;age&quot;)]) 1.4.2 Run a simple linear regression with LifSat as the dependent variable and educ as the independent variable. Hints: The lm() function (short for linear model) does linear regression. The summary() function provides relevant summary statistics for the model. It can be helpful to store the results of your analysis in an object. Click for explanation results &lt;- lm(LifSat ~ educ, data = LifeSat) summary(results) ## ## Call: ## lm(formula = LifSat ~ educ, data = LifeSat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -44.158 -11.678 2.212 12.541 43.212 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.418 8.003 4.301 4.09e-05 *** ## educ 3.562 1.192 2.988 0.00356 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.93 on 96 degrees of freedom ## Multiple R-squared: 0.08511, Adjusted R-squared: 0.07558 ## F-statistic: 8.931 on 1 and 96 DF, p-value: 0.00356 1.4.3 Repeat the analysis from 1.4.2 with age as the independent variable. Click for explanation results &lt;- lm(LifSat ~ age, data = LifeSat) summary(results) ## ## Call: ## lm(formula = LifSat ~ age, data = LifeSat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -48.230 -14.025 3.321 13.745 40.770 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 199.6492 53.1261 3.758 0.000294 *** ## age -2.0203 0.7557 -2.673 0.008829 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 18.08 on 96 degrees of freedom ## Multiple R-squared: 0.06928, Adjusted R-squared: 0.05959 ## F-statistic: 7.146 on 1 and 96 DF, p-value: 0.008829 1.4.4 Repeat the analysis from 1.4.2 and 1.4.3 with ChildSup as the independent variable. Click for explanation results &lt;- lm(LifSat ~ ChildSup, data = LifeSat) summary(results) ## ## Call: ## lm(formula = LifSat ~ ChildSup, data = LifeSat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -45.131 -11.875 0.862 12.595 44.869 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.052 8.487 4.366 3.2e-05 *** ## ChildSup 3.013 1.208 2.494 0.0144 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 18.17 on 96 degrees of freedom ## Multiple R-squared: 0.06083, Adjusted R-squared: 0.05105 ## F-statistic: 6.218 on 1 and 96 DF, p-value: 0.01436 1.4.5 Run a multiple linear regression with LifSat as the dependent variable and educ, age, and ChildSup as the independent variables. Hint: You can use the + sign to add multiple variables to the RHS of your model formula. Click for explanation results &lt;- lm(LifSat ~ educ + age + ChildSup, data = LifeSat) summary(results) ## ## Call: ## lm(formula = LifSat ~ educ + age + ChildSup, data = LifeSat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -41.525 -12.472 2.601 11.235 42.108 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 132.5942 54.2325 2.445 0.0164 * ## educ 2.9123 1.1640 2.502 0.0141 * ## age -1.5766 0.7316 -2.155 0.0337 * ## ChildSup 2.4554 1.1564 2.123 0.0364 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.23 on 94 degrees of freedom ## Multiple R-squared: 0.1732, Adjusted R-squared: 0.1468 ## F-statistic: 6.565 on 3 and 94 DF, p-value: 0.0004452 1.4.6 Compare the results from 1.4.5 with those from 1.4.2, 1.4.3, and 1.4.4. What do you notice when you compare the estimated slopes for each of the three predictors in the multiple regression model with the corresponding estimates from the simple regression models? "],["in-class-exercises.html", "1.5 In-Class Exercises", " 1.5 In-Class Exercises During this practical, you will work through some exercises about ANOVA, ANCOVA, and regression. Note that ANOVA and ANCOVA are special cases of regression. Or, more accurately, ANOVA, ANCOVA, and regression are all different flavors of the general linear model. If you need to refresh your knowledge on ANOVA, ANCOVA, or regression consider the resources listed in the Background knowledge section. 1.5.1 Part 1: Data Exploration 1.5.1.1 Open the file Sesam.sav: ## Load the &#39;foreign&#39; library for reading SPSS files: library(foreign) ## Load the &#39;Sesam.sav&#39; data into an object called &#39;sesam&#39;: sesam &lt;- read.spss(&quot;Sesam.sav&quot;, to.data.frame = TRUE, use.value.labels = FALSE) This file is part of a larger dataset that evaluates the impact of the first year of the Sesame Street television series. Sesame Street is mainly concerned with teaching preschool related skills to children in the 3–5 year age range. The following variables will be used in this exercise: age: measured in months prelet: knowledge of letters before watching Sesame Street (range 0–58) prenumb: knowledge of numbers before watching Sesame Street (range 0–54) prerelat: knowledge of relations before watching Sesame Street (range 0–17) peabody: vocabulary maturity before watching Sesame Street (range 20–120) postnumb: knowledge of numbers after a year of Sesame Street (range 0–54) Note: Unless otherwise noted, the following questions refer to the sesam data and the above variables. 1.5.1.2 What is the measurement level of each variable? Hint: The output of the str() function should be helpful here. Click for explanation ## Examine the data structure: str(sesam) ## &#39;data.frame&#39;: 240 obs. of 8 variables: ## $ id : num 1 2 3 4 5 6 7 8 9 10 ... ## $ age : num 66 67 56 49 69 54 47 51 69 53 ... ## $ prelet : num 23 26 14 11 47 26 12 48 44 38 ... ## $ prenumb : num 40 39 9 14 51 33 13 52 42 31 ... ## $ prerelat: num 14 16 9 9 17 14 11 15 15 10 ... ## $ peabody : num 62 80 32 27 71 32 28 38 49 32 ... ## $ postnumb: num 44 39 40 19 54 39 44 51 48 52 ... ## $ gain : num 4 0 31 5 3 6 31 -1 6 21 ... ## - attr(*, &quot;codepage&quot;)= int 65001 All variables are numeric. 1.5.1.3 What is the average age in the sample? What is the age range (youngest and oldest child)? Hint: Use tidySEM::descriptives() Click for explanation As in the take home exercises, you can use the descriptives() function from the tidySEM package to describe the data: library(tidySEM) descriptives(sesam) 1.5.1.4 What is the average gain in knowledge of numbers? What is the standard deviation of this gain? Hints: You will need to compute the gain and save the change score as a new object. You can then use the base-R functions mean() and sd() to do the calculations. Click for explanation Create a new variable that represents the difference between pre- and post-test scores on knowledge of numbers: sesam$ndif &lt;- sesam$postnumb - sesam$prenumb Compute the mean and SD of the change score: mean(sesam$ndif) ## [1] 9.158333 sd(sesam$ndif) ## [1] 9.682401 1.5.1.5 Create an appropriate visualization of the gain scores you computed in 1.5.1.4. Justify your choice of visualization. Hint: Some applicable visualizations are explained in 0.9. Click for explanation library(ggplot2) ## Create an empty baseline plot object: p &lt;- ggplot(sesam, aes(x = ndif)) ## Add some appropriate geoms: p + geom_histogram() p + geom_density() p + geom_boxplot() 1.5.1.6 Create a visualization that provides information about the bivariate relationship between two of the variables. Justify your choice of visualization. Hint: Again, Section 0.9 may provide some useful insights. Click for explanation ## Create a scatterplot of the pre- and post-test number knowledge ggplot(sesam, aes(x = prenumb, y = postnumb)) + geom_point() 1.5.2 Part 2: Regression Analysis 1.5.2.1 Are there significant, bivariate associations between postnumb and the following variables? age prelet prenumb prerelat peabody Use Pearson correlations to answer this question. You do not need to check the assumptions here (though you would in real life). Hint: The base-R cor.test() function and the corr.test() function from the psych package will both conduct hypothesis tests for a correlation coefficients (the base-R cor() function only computes the coefficients). Click for explanation library(psych) ## Test the correlations using psych::corr.test(): corr.test( sesam[ , c(&quot;postnumb&quot;, &quot;age&quot;, &quot;prelet&quot;, &quot;prenumb&quot;, &quot;prerelat&quot;, &quot;peabody&quot;)] ) ## Call:corr.test(x = sesam[, c(&quot;postnumb&quot;, &quot;age&quot;, &quot;prelet&quot;, &quot;prenumb&quot;, ## &quot;prerelat&quot;, &quot;peabody&quot;)]) ## Correlation matrix ## postnumb age prelet prenumb prerelat peabody ## postnumb 1.00 0.34 0.50 0.68 0.54 0.52 ## age 0.34 1.00 0.33 0.43 0.44 0.29 ## prelet 0.50 0.33 1.00 0.72 0.47 0.40 ## prenumb 0.68 0.43 0.72 1.00 0.72 0.61 ## prerelat 0.54 0.44 0.47 0.72 1.00 0.56 ## peabody 0.52 0.29 0.40 0.61 0.56 1.00 ## Sample Size ## [1] 240 ## Probability values (Entries above the diagonal are adjusted for multiple tests.) ## postnumb age prelet prenumb prerelat peabody ## postnumb 0 0 0 0 0 0 ## age 0 0 0 0 0 0 ## prelet 0 0 0 0 0 0 ## prenumb 0 0 0 0 0 0 ## prerelat 0 0 0 0 0 0 ## peabody 0 0 0 0 0 0 ## ## To see confidence intervals of the correlations, print with the short=FALSE option ## OR ## ## Test the correlations using multiple cor.test() calls: cor.test(sesam$postnumb, sesam$age) ## ## Pearson&#39;s product-moment correlation ## ## data: sesam$postnumb and sesam$age ## t = 5.5972, df = 238, p-value = 5.979e-08 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.2241066 0.4483253 ## sample estimates: ## cor ## 0.3410578 cor.test(sesam$postnumb, sesam$prelet) ## ## Pearson&#39;s product-moment correlation ## ## data: sesam$postnumb and sesam$prelet ## t = 8.9986, df = 238, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.4029239 0.5926632 ## sample estimates: ## cor ## 0.5038464 cor.test(sesam$postnumb, sesam$prenumb) ## ## Pearson&#39;s product-moment correlation ## ## data: sesam$postnumb and sesam$prenumb ## t = 14.133, df = 238, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.6002172 0.7389277 ## sample estimates: ## cor ## 0.6755051 cor.test(sesam$postnumb, sesam$prerelat) ## ## Pearson&#39;s product-moment correlation ## ## data: sesam$postnumb and sesam$prerelat ## t = 9.9857, df = 238, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.4475469 0.6268773 ## sample estimates: ## cor ## 0.5433818 cor.test(sesam$postnumb, sesam$peabody) ## ## Pearson&#39;s product-moment correlation ## ## data: sesam$postnumb and sesam$peabody ## t = 9.395, df = 238, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.4212427 0.6067923 ## sample estimates: ## cor ## 0.520128 1.5.2.2 Do age and prenumb explain a significant proportion of the variance in postnumb? What statistic did you use to justify your conclusion? Interpret the model fit. Hints: The lm() function (short for linear model) estimates linear regression models. The summary() function provides relevant summary statistics for the model. Click for explanation results &lt;- lm(postnumb ~ age + prenumb, data = sesam) summary(results) ## ## Call: ## lm(formula = postnumb ~ age + prenumb, data = sesam) ## ## Residuals: ## Min 1Q Median 3Q Max ## -38.130 -6.456 -0.456 5.435 22.568 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.4242 5.1854 1.432 0.154 ## age 0.1225 0.1084 1.131 0.259 ## prenumb 0.7809 0.0637 12.259 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.486 on 237 degrees of freedom ## Multiple R-squared: 0.4592, Adjusted R-squared: 0.4547 ## F-statistic: 100.6 on 2 and 237 DF, p-value: &lt; 2.2e-16 Yes, age and prenumb explain a significant amount of variability in postnumb (\\(R^2 = 0.459\\), \\(F[2, 237] = 100.629\\), \\(p &lt; 0.001\\)). We use the F statistic for the overall test of model fit to support this conclusion. The variables age and prenumb together explain 45.9% of the variability in postnumb. 1.5.2.3 Write the null and alternative hypotheses for tested in 1.5.2.2. Click for explanation Since we are testing for explained variance, our hypotheses concern the \\(R^2\\). \\[ \\begin{align*} H_0: R^2 = 0\\\\ H_1: R^2 &gt; 0 \\end{align*} \\] Note that this is a directional hypotheses because the \\(R^2\\) cannot be negative. 1.5.2.4 Consider the path model below. How many regression coefficients are estimated in this model? How many variances are estimated? How many covariances are estimated? How many degrees of freedom does this model have? Hint: As you learned in Lecture 1, \\(df = N_{obs} – N_{par}\\). 1.5.2.5 Consider a multiple regression analysis with three continuous independent variables: scores on tests of language, history, and logic, and one continuous dependent variable: score on a math test. We want to know if scores on the language, history, and logic tests can predict the math test score. Sketch a path model that you could use to answer this question How many regression parameters are there? How many variances could you estimate? How many covariances could you estimate? How many degrees of freedom does this model have? 1.5.3 Part 3: ANOVA 1.5.3.1 Load the Drivers.sav data. # Read the data into a data frame named &#39;drivers&#39;: drivers &lt;- read.spss(&quot;Drivers.sav&quot;, to.data.frame = TRUE) In this section, we will use ANOVA to evaluate the following research question: Does talking on the phone interfere with people’s driving skills? These data come from an experiment. The condition variable represents the three experimental conditions: Hand-held phone Hands-free phone Control (no phone) We will use condition as the IV in our ANOVA models. The DV, RT, represents the participant’s reaction time (in milliseconds) during a driving simulation. ANOVA vs. Linear Regression As you may know, the mathematical model underlying ANOVA is just a linear regression model with nominal IVs. So, in terms of the underlying statistical models, there is no difference between ANOVA and regression; the differences lie in the focus of the analysis. ANOVA is really a type of statistical test wherein we are testing hypotheses about the effects of some set of nominal grouping factors on some continuous outcome. When doing an ANOVA, we usually don’t interact directly with the parameter estimates from the underlying model. Regression is a type of statistical model (i.e., a way to represent a univariate distribution with a conditional mean and fixed variance). When we do a regression analysis, we primarily focus on the estimated parameters of the underling linear model. When doing ANOVA in R, we estimate the model exactly as we would for linear regression; we simply summarize the results differently. 1.5.3.2 Perform the ANOVA to test the above research question. Hint: After estimating the model with lm(), you can use the anova() function to compute the sums-of-squares and significance tests for each factor in your model. Click for explanation ## Estimate the underlying model: results &lt;- lm(RT ~ condition, data = drivers) ## Summarize the model as a regression analysis: summary(results) ## ## Call: ## lm(formula = RT ~ condition, data = drivers) ## ## Residuals: ## Min 1Q Median 3Q Max ## -317.50 -71.25 2.98 89.55 243.45 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 654.50 29.08 22.506 &lt;2e-16 *** ## conditionhands-free -36.95 41.13 -0.898 0.3727 ## conditioncontrol -100.75 41.13 -2.450 0.0174 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 130.1 on 57 degrees of freedom ## Multiple R-squared: 0.09729, Adjusted R-squared: 0.06562 ## F-statistic: 3.072 on 2 and 57 DF, p-value: 0.05408 ## Summarize the model as an ANOVA: anova(results) Of course the results of any analysis are only valid if the assumptions of the analysis/model are satisfied. In particular, we should probably check at least three conditions: There are no overly influential cases The residual variance is homogonous across groups The residuals are normally distributed 1.5.3.3 Check for influential cases. Hint: You can use the cooks.distance() function to compute Cook’s Distance statistic for each observation. Observations with Cook’s D values substantially larger than, and qualitatively distinct from, the rest of the data may be overly influential. You can evaluate the relative sizes of the distances by making an index plot of the estimated distance statistics. Click for explanation ## Compute the Cook&#39;s distances: d &lt;- cooks.distance(results) ## Plot the distances: plot(d) In the figure above, we’re looking for any distances that clearly “stand out from the crowd”. None of the distances in the above figure look notable. We do not see evidence of influential observations. 1.5.3.4 Check the normality of the residuals. Hints: One of the best ways to check the normality of residuals is with a Normal QQplot. We can easily create a Normal QQ-Plot of the residuals by plotting the results object from 1.5.3.2. Click for explanation plot(results, 2) In the QQ-Plot created above, we want to see all of the points follow the diagonal, dashed line. Perfectly normal residuals will fall exactly along this line Deviations away from the line indicate devations from normality. The residuals in this figure look quite good. We only see very minor deviations from the idealized line. The residuals appear to be more-or-less normally distributed. 1.5.3.5 Check the homogeneity of the residual variances. Hints: A scale-location plot is one of the best ways to check the homogeneity of variances assumption. Plot an estimate of the residual variance against the predicted values Any trend indicates differences in residual variance between groups Plotting the results object from 1.5.3.2 will also produce a scale-location plot. Click for explanation plot(results, 3) The red line in this figure is a loess line which represents the trend of the plotted data. If this loess line is flat, there is no evidence of differences in the residual variances between groups. Trends in this line indicate violations of the homogeneity assumption. In this case, the line is pretty much flat and we have little-to-no evidence of heterogeneous residual variances. 1.5.3.6 Summarize your conclusions regarding the assumption. Are the assumptions satisfied? Can we trust the model results? Click for explanation There are no observations that stand out as particularly influential. Furthermore, we have no evidence of heterogeneous residual variances or substantial violations of normality for the residuals. Hence, the assumptions appear to be satisfied, and we can trust the conclusions of our analysis. 1.5.3.7 Use your results to answer the research question. Click for explanation anova(results) The effect of condition on RT was nonsignificant (\\(F[2, 57] = 3.07\\), \\(p = 0.054\\)). Therefore, based on these results, we do not have evidence for an effect of mobile phone usage on driving performance. 1.5.4 Part 4: ANCOVA We will now conduct an ANCOVA to assess the following research question. Are there differences in reaction times between the phone conditions after controlling for age? As with ANOVA, the statistical model underlying an ANCOVA is simply a linear regression model. The model for an ANCOVA, however, includes at least one continuous covariate in addition to the categorical IVs. Consequently, when we conduct ANCOVA in R, we again use the lm() function to estimate the model. 1.5.4.1 Estimate the ANCOVA model needed to test the above research question. Click for explanation ## Estimate the model: results &lt;- lm(RT ~ condition + age, data = drivers) ## Conduct the statistical tests: anova(results) ANCOVA and Interactions As you can see above, when we run an ANCOVA, we’re just estimating a multiple linear regression model. We call the analysis ANCOVA because we are interested in a specific hypothesis. Our substantive interest lies in the categorical IVs. The continuous covariates are uninteresting, nuisance variables that we are controlling for to get better estimates of the interesting treatment effects. One implication of this hypothesis is the absence of any interaction between the covariates and grouping factors. If the covariate moderates the treatment effect, the covariate has a direct impact on the substantively interesting group differences. Such a covariate is not a covariate; it’s a substantively integral feature of the model. If we want to report our analysis as an ANCOVA, we need to show that the covariate effects are equivalent in each group. In other words, there is no interaction between the grouping factors and the covariates. 1.5.4.2 Test for an interaction between age and condition in the model from 1.5.4.1. What is your conclusion? Can we report our analysis as an ANCOVA? Hint: You can include an interaction term in an R formulas ‘multiplying’ the two variable names using the * operator. Click for explanation ## Add the interaction between &#39;age&#39; and &#39;condition&#39; to the model: results_int &lt;- lm(RT ~ condition * age, data = drivers) ## Conduct the hypothesis tests. anova(results_int) The interaction between condition and age is not significant (\\(F[2, 54] = 1.07\\), \\(p = 0.349\\)). Therefore, the covariate effects should be more-or-less equivalent in each group, and we can report our analysis as an ANCOVA. 1.5.4.3 Answer the research from above question. If you did find a significant, partial effect of condition, do a post hoc comparison of pairwise mean differences to see which groups showed significantly different reaction times, after controlling for age. Use Tukey’s HSD correction for all pairwise comparisons to control the Type I error rate. Hints: You can conduct the appropriate post hoc test with the TukeyHSD() function. The TukeyHSD() function only works on models estimated using the aov() function (which is the same as lm() but summarizes the results in ANOVA style). To satisfy TukeyHSD(), you can either rerun your model with aov() or convert your results object to the correct format via aov(results). Click for explanation ## Check the hypothesis tests for the ANCOVA from above: anova(results) ## Run the post hoc comparisons: TukeyHSD(aov(results)) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = results) ## ## $condition ## diff lwr upr p adj ## hands-free-hand-held -36.95 -118.5708 44.67082 0.5242511 ## control-hand-held -100.75 -182.3708 -19.12918 0.0119407 ## control-hands-free -63.80 -145.4208 17.82082 0.1533777 After controlling for age, phone usage significantly affects driving performance (\\(F[2, 56] = 4.52\\), \\(p = 0.015\\)). Specifically, the hand-held condition has a significant higher reaction time than the control condition. 1.5.5 Part 5: Back to Regression As we saw above, we can use ANOVA or ANCOVA to test hypotheses about differences between groups in some continuous outcome. AN(C)OVA is a type of statistical test, though, not a type of model. The statistical model underlying an AN(C)OVA is just a linear regression model. Since this class is about statistical modeling, it won’t do us much good to keep thinking in terms of statistical tests; we’re better off approaching these problems from a modeling perspective. We can just as easily make the inferences from above by working directly within a regression modeling framework. In this section, we will explore linear regression models with categorical predictors. 1.5.5.1 Load the Sesam2.sav data. # Read the data into an object called &#39;sesam2&#39;: sesam2 &lt;- read.spss(&quot;Sesam2.sav&quot;, to.data.frame = TRUE) 1.5.5.2 VIEWCAT is a nominal grouping variable, but it is represented as a numeric variable in the sesam2 data. Convert VIEWCAT into a factor. Make sure that VIEWCAT = 1 is the reference group. Hints: You can identify the reference group with the levels() or contrasts() functions. The reference group is the group labelled with the first level printed by levels(). When you run contrasts(), you will see a pattern matrix that defines a certain dummy coding scheme. The reference group is the group that has zeros in each column of this matrix. If you need to change the reference group, you can use the relevel() function. Click for explanation ## Convert &#39;VIEWCAT&#39; to a factor: sesam2$VIEWCAT &lt;- factor(sesam2$VIEWCAT) ## Check the reference group: levels(sesam2$VIEWCAT) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; contrasts(sesam2$VIEWCAT) ## 2 3 4 ## 1 0 0 0 ## 2 1 0 0 ## 3 0 1 0 ## 4 0 0 1 1.5.5.3 Estimate a multiple regression model wherein VIEWCAT predicts POSTNUMB. Summarize the model. Interpret the estimates. Click for explanation results &lt;- lm(POSTNUMB ~ VIEWCAT, data = sesam2) summary(results) ## ## Call: ## lm(formula = POSTNUMB ~ VIEWCAT, data = sesam2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -25.474 -7.942 0.240 8.526 25.240 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 18.760 2.316 8.102 8.95e-14 *** ## VIEWCAT2 9.331 2.900 3.218 0.00154 ** ## VIEWCAT3 14.714 2.777 5.298 3.49e-07 *** ## VIEWCAT4 18.032 2.809 6.419 1.24e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.58 on 175 degrees of freedom ## Multiple R-squared: 0.2102, Adjusted R-squared: 0.1967 ## F-statistic: 15.53 on 3 and 175 DF, p-value: 5.337e-09 1.5.5.4 Use ggplot() to make a scatterplot with AGE on the x-axis and POSTNUMB on the y-axis. Color the points according to the their VIEWCAT level. Save the plot object to a variable in your environment. Hint: You can map color to the levels of a variable on your dataset by assigning the variable names to the color argument of the aes() function in ggplot(). Click for explanation library(ggplot2) ## Add aes(..., color = VIEWCAT) to get different colors for each group: p &lt;- ggplot(sesam2, aes(x = AGE, y = POSTNUMB, color = VIEWCAT)) + geom_point() # Add points for scatterplot ## Print the plot stored as &#39;p&#39;: p We assigned the global color aesthetic to the VIEWCAT variable, so the points are colored based on their group. 1.5.5.5 Add linear regression lines for each group to the above scatterplot. Hints: You can add regression lines with ggplot2::geom_smooth() To get linear regression lines, set the argument method = \"lm\" To omit error envelopes, set the argument se = FALSE Click for explanation ## Add OLS best-fit lines: p + geom_smooth(method = &quot;lm&quot;, se = FALSE) The global color aesthetic assignment from above carries through to any additional plot elements that we add, including the regression lines. So, we also get a separate regression line for each VIEWCAT group. 1.5.5.6 How would you interpret the pattern of regression lines above? Click for explanation All the lines show a positive slope, so post-test number recognition appears to increase along with increasing age. The lines are not parallel, though. So, VIEWCAT may be moderating the effect of AGE on POSTNUMB. Moderated Regression Based on the figure we just created, we may want to test for moderation in our regression model. To do so, we need to add an interaction between AGE and VIEWCAT. The VIEWCAT factor is represented by 3 in our model, though. So, when we interact AGE and VIEWCAT, we will create 3 interaction terms. To test the overall moderating influence of VIEWCAT, we need to conduct a multiparameter hypothesis test of all 3 interaction terms. One way that we can go about implementing such a test is through a hierarchical regression analysis entailing three steps: Estimate the additive model wherein we regress POSTNUMB onto AGE and VIEWCAT without any interaction. Estimate the moderated model by adding the interaction between AGE and VIEWCAT into the additive model. Conduct a \\(\\Delta R^2\\) test to compare the fit of the two models. 1.5.5.7 Conduct the hierarchical regression analysis described above. Does VIEWCAT significantly moderate the effect of AGE on POSTNUMB? Provide statistical justification for your conclusion. Click for explanation ## Estimate the additive model a view the results: results_add &lt;- lm(POSTNUMB ~ VIEWCAT + AGE, data = sesam2) summary(results_add) ## ## Call: ## lm(formula = POSTNUMB ~ VIEWCAT + AGE, data = sesam2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -23.680 -8.003 -0.070 8.464 22.635 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -10.1056 6.5091 -1.553 0.12235 ## VIEWCAT2 9.1453 2.7390 3.339 0.00103 ** ## VIEWCAT3 13.8602 2.6294 5.271 3.98e-07 *** ## VIEWCAT4 16.9215 2.6636 6.353 1.79e-09 *** ## AGE 0.5750 0.1221 4.708 5.08e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.94 on 174 degrees of freedom ## Multiple R-squared: 0.2995, Adjusted R-squared: 0.2834 ## F-statistic: 18.6 on 4 and 174 DF, p-value: 9.642e-13 ## Estimate the moderated model and view the results: results_mod &lt;- lm(POSTNUMB ~ VIEWCAT * AGE, data = sesam2) summary(results_mod) ## ## Call: ## lm(formula = POSTNUMB ~ VIEWCAT * AGE, data = sesam2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -23.8371 -8.2387 0.6158 8.7988 22.5611 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -18.7211 15.5883 -1.201 0.2314 ## VIEWCAT2 9.9741 20.6227 0.484 0.6293 ## VIEWCAT3 23.5825 19.3591 1.218 0.2248 ## VIEWCAT4 34.3969 19.3600 1.777 0.0774 . ## AGE 0.7466 0.3074 2.429 0.0162 * ## VIEWCAT2:AGE -0.0175 0.4060 -0.043 0.9657 ## VIEWCAT3:AGE -0.1930 0.3782 -0.510 0.6104 ## VIEWCAT4:AGE -0.3416 0.3770 -0.906 0.3663 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.99 on 171 degrees of freedom ## Multiple R-squared: 0.3046, Adjusted R-squared: 0.2762 ## F-statistic: 10.7 on 7 and 171 DF, p-value: 3.79e-11 ## Test for moderation: anova(results_add, results_mod) VIEWCAT does not significantly moderate the effect of AGE on POSTNUMB (\\(F[3, 171] = 0.422\\), \\(p = 0.738\\)). 1.5.5.8 Write the regression equations for the additive and moderated models from 1.5.5.7. Click for explanation Additive Model \\[ \\begin{align*} Y_{postnumb} = \\beta_0 + \\beta_1 X_{view2} + \\beta_2 X_{view3} + \\beta_3 X_{view4} + \\beta_4 X_{age} + \\varepsilon \\end{align*} \\] Moderated Model \\[ \\begin{align*} Y_{postnumb} = \\beta_0 + &amp;\\beta_1 X_{view2} + \\beta_2 X_{view3} + \\beta_3 X_{view4} + \\beta_4 X_{age} +\\\\ &amp;\\beta_5 X_{view2} X_{age} + \\beta_6 X_{view3} X_{age} + \\beta_7 X_{view4} X_{age} + \\varepsilon \\end{align*} \\] 1.5.5.9 Write the null and alternative hypotheses for the test of moderation you conducted in 1.5.5.7. Click for explanation \\[ \\begin{align*} H_0: \\Delta R^2 = 0\\\\ H_1: \\Delta R^2 &gt; 0 \\end{align*} \\] 1.5.5.10 Write the regression equation for each of the four VIEWCAT groups. Click for explanation VIEWCAT 1: \\[ Y_{postnumb} = \\beta_0 + \\beta_4 X_{age} + \\varepsilon \\] VIEWCAT 2: \\[ Y_{postnumb} = \\beta_0 + \\beta_1 X_{view2} + \\beta_4 X_{age} + \\beta_5 X_{view2} X_{age} + \\varepsilon \\] VIEWCAT 3: \\[ Y_{postnumb} = \\beta_0 + \\beta_2 X_{view3} + \\beta_4 X_{age} + \\beta_6 X_{view3} X_{age} + \\varepsilon \\] VIEWCAT 4: \\[ Y_{postnumb} = \\beta_0 + \\beta_3 X_{view4} + \\beta_4 X_{age} + \\beta_7 X_{view4} X_{age} + \\varepsilon \\] 1.5.5.11 Run the anova() function on only the moderated regression model from 1.5.5.7. Compare the result to the anova() output from 1.5.5.7. What do you notice when comparing the two anova() outputs? Does this comparison offer any insight into the relation between regression and AN(C)OVA? Click for explanation anova(results_mod) anova(results_add, results_mod) When we run the anova() function on only the moderated regression model, we get ANCOVA-style tests like those we used in 1.5.4.2. The marginal test for the interaction between AGE and VIEWCAT in this ANCOVA-style result is identical to the model comparison test from 1.5.5.7. This comparison shows that the significance tests for marginal effects in an AN(C)OVA model can be equivalent to certain model comparison tests in a regression model. Indeed, although you cannot know this from only the examples explored here, any significance test in an AN(C)OVA can be reformulated as a comparison between two appropriately nested linear regression models. End of In-Class Exercises 1 "],["efa.html", "2 EFA", " 2 EFA Homework before the lecture Watch the Lecture Recording for this week. Complete the Reading for this week, and answer the associated reading questions. Complete the Formative Assessment for this week. Lecture content This lecture constitutes a general introduction to latent variables and scaling procedures. We will discuss several different aspects of exploratory factor analysis (EFA). Most notably: The differences between Principal Component Analyses (PCA) and Factor Analysis Model estimation and factor extraction methods Factor rotations You will have to make decisions regarding each of these aspects when conducting a factor analysis. We will also discuss reliability and factor scores as means of evaluating the properties of a scale. Homework before the practical Complete the At-Home Exercises. Practical content During the practical you will work on the In-Class Exercises. "],["lecture-1.html", "2.1 Lecture", " 2.1 Lecture Download slides How do you know if you have measured the putative hypothetical construct that you intend to measure? The methods introduced in this lecture (namely, latent variables, factor analysis, and reliability analysis) can shed empirical light on this issue. In the social and behavioral sciences we’re often forced to measure key concepts indirectly. For example, we have no way of directly quantifying a person’s current level of depression, or their innate motivation, or their risk-aversion, or any of the other myriad psychological features that comprise the human mental state. In truth, we cannot really measure these hypothetical constructs at all, we must estimate latent representations thereof (though, psychometricians still use the language of physical measurement to describe this process). Furthermore, we can rarely estimate an adequate representation with only a single observed variable (e.g., question on a survey, score on a test, reading from a sensor). We generally need several observed variables to reliably represent a single hypothetical construct. For example, we cannot accurately determine someone’s IQ or socio-economic status based on their response to a single question; we need several questions that each tap into slightly different aspects of IQ or SES. Given multiple items measuring the same construct, we can use the methods discussed in this lecture (i.e., factor analysis and reliability analysis) to evaluate the quality of our measurement (i.e., how well we have estimated the underlying hypothetical construct). If we do well enough in this estimation task, we will be able to combine these estimated latent variables with the path analysis methods discussed in two weeks to produce the full structural equation models that we will cover at the end of this course. "],["reading-1.html", "2.2 Reading", " 2.2 Reading This week, you will read two papers. Reference 1 Preacher, K. J., &amp; MacCullum, R. C. (2003). Repairing Tom Swift’s electric factor analysis machine, Understanding Statistics 2(1) 13–43. Questions 1 What is a latent variable? Give an example of a latent variable. What is factor analysis, and what can you investigate using this method? In the introduction, Preacher and Maccallum describe a “little jiffy” method of doing factor analysis. Briefly describe this little jiffy—or bad practice—method. Briefly explain the key differences between Principal Component Analyses (PCA) and Exploratory Factor Analyses (EFA). What is the purpose of factor rotation? Answers 1 A latent variable is a variable (i.e. something that differs between research subjects or changes within the same subject over time) that we cannot observe directly. Rather, we infer the value of a latent variable by combining observed variables that each tap into different aspects of the latent variable. Almost all psychological, sociological, or cultural constructs are latent variables. Some common examples include IQ, personality, political or racial attitudes, mathematical or artistic ability, etc. (Exploratory) factor analysis is a statistical method for estimating the latent measurement structure that generates a set of measured variables. In other words, when doing EFA, we’re trying to estimate how many latent variables underlie a set of observed data and which observed items “belong to” which latent variable. The “little jiffy” method consists of the following steps: Run a PCA to extract factors Apply no subsequent rotation or the varimax rotation Retain all components with eigenvalues greater than 1.0 This approach results from running “factor analysis” with the SPSS defaults. In most situations, however, this setup won’t give a meaningful answer to the substantive research questions we’re investigating when doing EFA. The main difference between PCA and EFA comes down to how the observed variance is factored between the latent variables and the unique (residual) factors. PCA directly explains the observed variances. So, if you extract enough components, the latent variables will explain 100% of the observed variance. EFA allows each observed item to retain its own unique factor to quantify the item variance not explained by the latent variables. Unless you constrain the model, the variance explained by the latent factors should not reach 100% of the observed variance. Factor rotation entails applying linear transformations to “rotate” the factor axes (i.e., the geometric representations of the latent variables) through the data space to produce a so-called “simple structure” in which each observed item loads onto only one factor (to the extent that this is possible). There are two flavors of rotations: Orthogonal rotation: All latent factors are uncorrelated (i.e. the factor axes describe a 90 degree angle) Oblique rotation: The latent factors are allowed to correlate Reference 2 Kestilä, E. (2006). Is there demand for radical right populism in the Finnish electorate? Scandinavian Political Studies 29(3), 169–191. Questions 2 What is the research question that the author tries to answer? Briefly describe the characteristics of the Radical Right Parties (RRP) in Europe. What are the two main explanations of support for RRP upon which this paper focuses? Does the empirical part of the paper reflect the theoretical framework well? Why or why not? According to the author, is Finland very different from other European countries on the main dependent variables? What is the author’s conclusion (i.e., how does the author answer the research question)? Answers 2 Why is there no Radical Right Party in Finland? To explore this question, the author considers how similar the attitudes of the Finnish electorate about trust in politics and immigration are to other Western-European countries. Pages 175–177 provide the relevant information. Briefly: RRPs emphasize anti-immigrant attitudes and connect immigration to feelings of insecurity, spurring debates about crime. RRPs are anti-elitist, protest parties that often push for lower taxes while protecting the welfare state. RRPs represent the antitheses of green parties. RRPs represent the ‘losers’ of modernization and emphasize traditional moral values, social norms, the role of the family, and the importance of national culture. The anti-immigration issue and protest voting Of course, this conclusion is debatable, but I don’t think the empirical and theoretical aspects of the paper align very well. The way the key constructs are operationalized (i.e., via orthogonally rotated component scores) is not a very good way to get sensible estimates of the true hypothetical constructs. Furthermore, the idea of defining the outcome for the final regression analysis as the naive sum of the component scores casts considerable doubt on the validity of that analysis (in terms of the DV accurately representing its putative construct). No, Finland is very similar to other (Nordic) European countries. Anti-immigrant and anti-politics attitudes cannot explain why there is no RRP in Finland. The author finds reasons for the absence of such a link in history and the fact that politicians from mainstream political parties have caught the populist voters. "],["formative-assessment-1.html", "2.3 Formative Assessment", " 2.3 Formative Assessment A formative assessment helps you judge your progress in the course and helps you address any blind spots in your understanding of the material. This assessment does not contribute to your course grade. If you get a question wrong, you will receive a hint on how to improve your understanding of the material. Ideally, you should complete this assessment after you have watched the lecture recording, but before the lecture meeting. This way, if you discover gaps in your understanding, we can discuss any topics that need more attention during the lecture meeting. Question 1: When conducting an exploratory factor analysis, cross-loadings are assumed to be zero. TRUE FALSE Question 2: Order these methods for determining the number of factors from least subjective to most subjective: 1) Kaiser’s criterion, 2) Scree plot, 3) Parallel analysis. 3, 1, 2 2, 1, 3 1, 2, 3 3, 2, 1 Question 3: EFA models the covariances among items; PCA describes the total variance of items. FALSE TRUE Question 4: What kind of model is displayed above? Principal Components Analysis An Item Response Theory Analysis Confirmatory Factor Analysis Exploratory Factor Analysis Question 5: Varimax is a form of orthogonal rotation in factor analysis TRUE FALSE Question 6: Which of these is not a valid method for choosing the correct number of factors? Ability to predict a criterion scale. Kaiser’s criterion Parallel analysis Scree plot Theory Question 7: Promax rotation in factor analysis is a specific form of orthogonal rotation. FALSE TRUE Question 8: Factor analysis is a statistical technique aimed at data reduction. TRUE FALSE Question 9: What is the maximum number of latent variables when conducting Exploratory Factor Analysis with 5 items? No limit; you can keep adding latent variables, but they will explain ever decreasing amounts of variance. Two; you need 3 indicators for one latent variable, but the model is still identified due to the covariance between latent variables. Only one; you need at least 3 indicators per latent variable. Five; the same as the number of indicators. Question 10: With PCA, you must indicate the number of factors to extract a-priori. TRUE FALSE "],["at-home-exercises-1.html", "2.4 At-Home Exercises", " 2.4 At-Home Exercises In these exercises, you will attempt to replicate some of the analyses from the second reading for this week: Kestilä, E. (2006). Is there demand for radical right populism in the Finnish electorate? Scandinavian Political Studies 29(3), 169–191. The data for this practical were collected during the first round of the European Social Survey (ESS). The ESS is a repeated cross-sectional survey administered in 32 European countries. The first wave was collected in 2002, and two new waves have been collected each year since. You can find more info and access the data at https://www.europeansocialsurvey.org. The data we will analyze for this practical are contained in the file named ESSround1-a.sav. This file contains data for all respondents, but only includes those variables that you will need to complete the following exercises. 2.4.1 Load the ESSround1-a.sav dataset into R. Inspect the data after loading to make sure everything went well. ## Load the &#39;foreign&#39; package: library(foreign) ## Read the &#39;ESSround1-a.sav&#39; ess into a data frame called &#39;ess&#39;: ess &lt;- read.spss(&quot;ESSround1-a.sav&quot;, to.data.frame = TRUE) ## Inspect the result: dim(ess) summary(ess) str(ess) When you inspect the data, you may notice that almost all variables are represented as factors. If so, good eyes! We’ll come back to this issue a little later. Click here for a description of the variables. Variable Description name Title of dataset essround ESS round edition Edition proddate Production date cntry Country idno Respondent’s identification number trstlgl Trust in the legal system trstplc Trust in the police trstun Trust in the United Nations trstep Trust in the European Parliament trstprl Trust in country’s parliament stfhlth State of health services in country nowadays stfedu State of education in country nowadays stfeco How satisfied with present state of economy in country stfgov How satisfied with the national government stfdem How satisfied with the way democracy works in country pltinvt Politicians interested in votes rather than peoples opinions pltcare Politicians in general care what people like respondent think trstplt Trust in politicians imsmetn Allow many/few immigrants of same race/ethnic group as majority imdfetn Allow many/few immigrants of different race/ethnic group from majority eimrcnt Allow many/few immigrants from richer countries in Europe eimpcnt Allow many/few immigrants from poorer countries in Europe imrcntr Allow many/few immigrants from richer countries outside Europe impcntr Allow many/few immigrants from poorer countries outside Europe qfimchr Qualification for immigration: christian background qfimwht Qualification for immigration: be white imwgdwn Average wages/salaries generally brought down by immigrants imhecop Immigrants harm economic prospects of the poor more than the rich imtcjob Immigrants take jobs away in country or create new jobs imbleco Taxes and services: immigrants take out more than they put in or less imbgeco Immigration bad or good for country’s economy imueclt Country’s cultural life undermined or enriched by immigrants imwbcnt Immigrants make country worse or better place to live imwbcrm Immigrants make country’s crime problems worse or better imrsprc Richer countries should be responsible for accepting people from poorer countries pplstrd Better for a country if almost everyone share customs and traditions vrtrlg Better for a country if a variety of different religions shrrfg Country has more than its fair share of people applying refugee status rfgawrk People applying refugee status allowed to work while cases considered gvrfgap Government should be generous judging applications for refugee status rfgfrpc Most refugee applicants not in real fear of persecution own countries rfggvfn Financial support to refugee applicants while cases considered rfgbfml Granted refugees should be entitled to bring close family members gndr Gender yrbrn Year of birth edulvl Highest level of education eduyrs Years of full-time education completed polintr How interested in politics lrscale Placement on left right scale The ess dataset contains much more information than Kestilä (2006) used. Kestilä only analyzed data from the following ten countries: Austria Belgium Denmark Finland France Germany Italy Netherlands Norway Sweden So, our first task is to subset the data to only the relevant population. As explained in 0.10.5, we can select rows from a dataset based on logical conditions. In this case, we want to select only rows from the 10 countries listed above. 2.4.2 Subset the data to include only the 10 countries analyzed by Kestilä (2006). Inspect the subsetted data to check that everything went well. Hint: Use the %in% operator to create a logical vector that indicates which elements of the cntry variable are in the set of target counties. Click for explanation ## Create a character vector naming the target countries: targets &lt;- c(&quot;Austria&quot;, &quot;Belgium&quot;, &quot;Denmark&quot;, &quot;Finland&quot;, &quot;France&quot;, &quot;Germany&quot;, &quot;Italy&quot;, &quot;Netherlands&quot;, &quot;Norway&quot;, &quot;Sweden&quot;) ## Select only those rows that come from a target country: ess &lt;- ess[ess$cntry %in% targets, ] ## Inspect the result: dim(ess) ## [1] 19690 50 table(ess$cntry) ## ## Austria Belgium Switzerland Czech Republic Germany ## 2257 1899 0 0 2919 ## Denmark Spain Finland France United Kingdom ## 1506 0 2000 1503 0 ## Greece Hungary Ireland Israel Italy ## 0 0 0 0 1207 ## Luxembourg Netherlands Norway Poland Portugal ## 0 2364 2036 0 0 ## Sweden Slovenia ## 1999 0 Before we can analyze the data, we need to screen them for problems. At this point, we won’t get into the nitty-gritty of outlier analysis and data cleaning. We must, however, ensure that the variables we want to analyze are formatted correctly (e.g., interval and ratio variables should be numeric, nominal variables should be factors). 2.4.3 Screen the data to see if the variables are formatted correctly. Click for explanation library(tidySEM) ## Check the data structure: str(ess) ## &#39;data.frame&#39;: 19690 obs. of 50 variables: ## $ name : chr &quot;ESS1e06_1&quot; &quot;ESS1e06_1&quot; &quot;ESS1e06_1&quot; &quot;ESS1e06_1&quot; ... ## $ essround: num 1 1 1 1 1 1 1 1 1 1 ... ## $ edition : chr &quot;6.1&quot; &quot;6.1&quot; &quot;6.1&quot; &quot;6.1&quot; ... ## $ proddate: chr &quot;03.10.2008&quot; &quot;03.10.2008&quot; &quot;03.10.2008&quot; &quot;03.10.2008&quot; ... ## $ cntry : Factor w/ 22 levels &quot;Austria&quot;,&quot;Belgium&quot;,..: 1 18 1 1 18 1 2 18 1 18 ... ## $ idno : num 1 1 2 3 3 4 4 4 6 6 ... ## $ trstlgl : Factor w/ 11 levels &quot;No trust at all&quot;,..: 11 7 9 5 9 11 10 8 8 8 ... ## $ trstplc : Factor w/ 11 levels &quot;No trust at all&quot;,..: 11 9 6 9 9 10 9 10 5 10 ... ## $ trstun : Factor w/ 11 levels &quot;No trust at all&quot;,..: 10 9 7 NA 6 9 NA 8 6 8 ... ## $ trstep : Factor w/ 11 levels &quot;No trust at all&quot;,..: NA 4 1 8 4 8 1 4 5 7 ... ## $ trstprl : Factor w/ 11 levels &quot;No trust at all&quot;,..: 10 8 1 7 9 9 11 3 7 9 ... ## $ stfhlth : Factor w/ 11 levels &quot;Extremely bad&quot;,..: 11 5 1 8 7 9 NA 7 4 6 ... ## $ stfedu : Factor w/ 11 levels &quot;Extremely bad&quot;,..: 9 8 8 6 9 8 NA 8 7 8 ... ## $ stfeco : Factor w/ 11 levels &quot;Extremely dissatisfied&quot;,..: 8 7 1 8 9 7 NA 10 9 10 ... ## $ stfgov : Factor w/ 11 levels &quot;Extremely dissatisfied&quot;,..: 8 8 1 8 7 4 NA 6 6 8 ... ## $ stfdem : Factor w/ 11 levels &quot;Extremely dissatisfied&quot;,..: 9 6 6 6 8 8 NA 8 8 10 ... ## $ pltinvt : Factor w/ 5 levels &quot;Nearly all just interested in votes&quot;,..: 1 3 1 1 4 1 1 3 2 3 ... ## $ pltcare : Factor w/ 5 levels &quot;Hardly any politicians care&quot;,..: 1 4 1 1 4 3 2 5 2 3 ... ## $ trstplt : Factor w/ 11 levels &quot;No trust at all&quot;,..: 1 6 1 3 6 5 9 3 5 7 ... ## $ imsmetn : Factor w/ 4 levels &quot;Allow many to come and live here&quot;,..: 4 3 2 3 2 1 NA 2 NA 1 ... ## $ imdfetn : Factor w/ 4 levels &quot;Allow many to come and live here&quot;,..: 3 3 2 3 2 2 NA 2 NA 1 ... ## $ eimrcnt : Factor w/ 4 levels &quot;Allow many to come and live here&quot;,..: 4 2 2 2 3 1 NA 2 NA 1 ... ## $ eimpcnt : Factor w/ 4 levels &quot;Allow many to come and live here&quot;,..: 3 2 2 2 2 2 NA 2 NA 1 ... ## $ imrcntr : Factor w/ 4 levels &quot;Allow many to come and live here&quot;,..: 3 3 2 2 2 1 NA 2 NA 2 ... ## $ impcntr : Factor w/ 4 levels &quot;Allow many to come and live here&quot;,..: 3 2 2 3 2 1 NA 2 NA 2 ... ## $ qfimchr : Factor w/ 14 levels &quot;Extremely unimportant&quot;,..: 5 3 1 7 3 1 14 1 2 3 ... ## $ qfimwht : Factor w/ 14 levels &quot;Extremely unimportant&quot;,..: 2 1 1 1 1 1 14 1 1 2 ... ## $ imwgdwn : Factor w/ 5 levels &quot;Agree strongly&quot;,..: 3 4 2 2 3 3 NA 4 NA 4 ... ## $ imhecop : Factor w/ 5 levels &quot;Agree strongly&quot;,..: 2 2 1 4 3 2 NA 3 NA 2 ... ## $ imtcjob : Factor w/ 11 levels &quot;Take jobs away&quot;,..: 8 6 7 6 8 11 NA 9 NA 5 ... ## $ imbleco : Factor w/ 11 levels &quot;Generally take out more&quot;,..: 10 5 3 NA 4 11 NA 10 NA 7 ... ## $ imbgeco : Factor w/ 11 levels &quot;Bad for the economy&quot;,..: 5 4 11 8 6 11 NA 9 NA 6 ... ## $ imueclt : Factor w/ 11 levels &quot;Cultural life undermined&quot;,..: 10 5 11 6 5 11 NA 10 NA 4 ... ## $ imwbcnt : Factor w/ 11 levels &quot;Worse place to live&quot;,..: 8 4 6 6 6 11 NA 9 NA 6 ... ## $ imwbcrm : Factor w/ 11 levels &quot;Crime problems made worse&quot;,..: 4 4 6 3 4 6 NA 6 NA 4 ... ## $ imrsprc : Factor w/ 5 levels &quot;Agree strongly&quot;,..: 2 2 1 4 1 2 NA 1 1 3 ... ## $ pplstrd : Factor w/ 5 levels &quot;Agree strongly&quot;,..: 2 4 2 2 3 4 NA 4 4 2 ... ## $ vrtrlg : Factor w/ 5 levels &quot;Agree strongly&quot;,..: 3 5 3 2 4 1 NA 4 2 3 ... ## $ shrrfg : Factor w/ 5 levels &quot;Agree strongly&quot;,..: 3 2 1 1 3 3 NA 3 4 3 ... ## $ rfgawrk : Factor w/ 5 levels &quot;Agree strongly&quot;,..: 2 2 1 2 2 2 NA 2 1 2 ... ## $ gvrfgap : Factor w/ 5 levels &quot;Agree strongly&quot;,..: 4 3 2 4 2 2 NA 3 2 4 ... ## $ rfgfrpc : Factor w/ 5 levels &quot;Agree strongly&quot;,..: 4 3 2 4 4 4 NA 4 3 4 ... ## $ rfggvfn : Factor w/ 5 levels &quot;Agree strongly&quot;,..: 2 3 2 4 3 2 NA 2 2 2 ... ## $ rfgbfml : Factor w/ 5 levels &quot;Agree strongly&quot;,..: 2 3 1 2 2 1 NA 4 2 3 ... ## $ gndr : Factor w/ 2 levels &quot;Male&quot;,&quot;Female&quot;: 1 2 1 2 2 1 NA 2 2 1 ... ## $ yrbrn : Factor w/ 88 levels &quot;1893&quot;,&quot;1900&quot;,..: 48 77 52 39 63 58 NA 72 61 70 ... ## $ edulvl : Factor w/ 7 levels &quot;Not completed primary education&quot;,..: NA 4 NA NA 4 NA NA 7 NA 6 ... ## $ eduyrs : Factor w/ 35 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 12 17 15 10 13 19 NA 18 16 18 ... ## $ polintr : Factor w/ 4 levels &quot;Very interested&quot;,..: 3 3 1 2 3 2 1 4 3 3 ... ## $ lrscale : Factor w/ 11 levels &quot;Left&quot;,&quot;1&quot;,&quot;2&quot;,..: 7 8 7 6 9 6 NA 9 6 8 ... ## - attr(*, &quot;variable.labels&quot;)= Named chr [1:50] &quot;Title of dataset&quot; &quot;ESS round&quot; &quot;Edition&quot; &quot;Production date&quot; ... ## ..- attr(*, &quot;names&quot;)= chr [1:50] &quot;name&quot; &quot;essround&quot; &quot;edition&quot; &quot;proddate&quot; ... ## - attr(*, &quot;codepage&quot;)= int 65001 ## Compute descriptive statistics: descriptives(ess) ## Check the levels of one of the spurious factors: levels(ess$trstlgl) ## [1] &quot;No trust at all&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; ## [5] &quot;4 &quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; ## [9] &quot;8&quot; &quot;9&quot; &quot;Complete trust&quot; As noted above, one thing you might notice when inspecting the ess data is that nearly all variables are stored as factors. 2.4.4 Correct any formatting issues that you discovered above. Click for explanation In keeping with conventions, we will treat ordinal Likert-type rating scales with five or more levels as continuous. Consequently, we need to convert these variables from factors to numeric vectors. We can convert a factor to a numeric variable using the as.numeric() function, but this function only operates on one variable at a time. To replicate the Kestilä (2006) analysis, we need to convert 37 variables. Thankfully, we can use apply functions to broadcast univariate operation across each column (or row) of a dataset. We are specifically interested in the lapply() function. The lapply() function applies some function to each element in a list (e.g., each column in a data frame). The variables we care about live in columns 7 to 44 of the ess dataset. In the following code, we apply the as.numeric() function to these columns and overwrite their original factor versions with the converted numeric data. ess[7:44] &lt;- lapply(ess[7:44], as.numeric) In addition to screening with summary statistics, we can also visualize the variables’ distributions. You have already created a few such visualizations for single variables. Now, we will use a few tricks to efficiently plot each of our target variables. The first step in this process will be to convert the interesting part of our data from “wide format” (one column per variable) into “long format” (one column of variable names, one column of data values). The pivot_longer() function from the tidyr package provides a convenient way to execute this conversion. 2.4.5 Use tidyr::pivot_longer() to create a long-formatted data frame from the target variables in ess (i.e., columns 7 to 44). Click for explanation ## Load the tidyr package: library(tidyr) ## Convert the target variables into a long-formatted data frame: ess_plot &lt;- pivot_longer(ess[7:44], colnames(ess)[7:44]) The next step in the process will be to plot the variables using ggplot(). To create one plot for each variable, we will use the facet_wrap() function to facet the plots on the ess_plot$name column (i.e., create a separate conditional plot for each unique value in ess_plot$name). 2.4.6 Use ggplot() with an appropriate geom (e.g., geom_histogram(), geom_density(), geom_boxplot()) and facet_wrap() to visualize each of the target variables. Hint: To implement the faceting, simply add facet_wrap(~ name, scales = \"free_x\") to the end of your ggplot() call. Click for explanation library(ggplot2) ggplot(ess_plot, aes(x = value)) + geom_histogram() + # Create a histogram facet_wrap(~ name, scales = &quot;free_x&quot;) # Facet on &#39;name&#39; Notice that the variables are actually discrete (i.e., each variable takes only a few integer values). However, most variables look relatively normal despite being categorical. So, it’s probably fine to analyze these variables as continuous. 2.4.7 Check the descriptives for the target variables again. Do you see any remaining issues? Click for explanation descriptives(ess[7:44]) No. Everything looks pretty much fine. Now, we’re ready to run the analyses and see if we can replicate the Kestilä (2006) results. 2.4.8 Run two principal component analyses (PCA): one for trust in politics, one for attitudes towards immigration. Use the principal() function from the psych package. Use exactly the same specifications as Kestilä (2006) concerning the estimation method, rotation, number of components extracted, etc. Hints: Remember that you can view the help file for psych::principal() by running ?psych::principal or, if the psych package already loaded, simply running ?principal. When you print the output from psych::principal(), you can use the cut option to hide any factor loadings smaller than a given threshold. You could consider hiding any loadings smaller than those reported by Kestilä (2006) to make the output easier to interpret. Click for explanation Trust in politics Kestilä extracted three components with VARIMAX rotation. ## Load the psych package: library(psych) ## Run the PCA: pca_trust &lt;- principal(ess[7:19], nfactors = 3, rotate = &quot;varimax&quot;) ## Print the results: print(pca_trust, cut = 0.3, digits = 3) ## Principal Components Analysis ## Call: principal(r = ess[7:19], nfactors = 3, rotate = &quot;varimax&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## RC3 RC2 RC1 h2 u2 com ## trstlgl 0.779 0.669 0.331 1.21 ## trstplc 0.761 0.633 0.367 1.18 ## trstun 0.675 0.556 0.444 1.44 ## trstep 0.651 0.332 0.549 0.451 1.57 ## trstprl 0.569 0.489 0.650 0.350 2.49 ## stfhlth 0.745 0.567 0.433 1.04 ## stfedu 0.750 0.603 0.397 1.14 ## stfeco 0.711 0.300 0.616 0.384 1.44 ## stfgov 0.634 0.377 0.587 0.413 1.88 ## stfdem 0.369 0.568 0.325 0.564 0.436 2.38 ## pltinvt 0.817 0.695 0.305 1.08 ## pltcare 0.811 0.695 0.305 1.11 ## trstplt 0.510 0.611 0.716 0.284 2.40 ## ## RC3 RC2 RC1 ## SS loadings 2.942 2.668 2.490 ## Proportion Var 0.226 0.205 0.192 ## Cumulative Var 0.226 0.432 0.623 ## Proportion Explained 0.363 0.329 0.307 ## Cumulative Proportion 0.363 0.693 1.000 ## ## Mean item complexity = 1.6 ## Test of the hypothesis that 3 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.07 ## with the empirical chi square 15240.94 with prob &lt; 0 ## ## Fit based upon off diagonal values = 0.967 Attitudes toward imigration Kestilä extracted five components with VARIMAX rotation. pca_att &lt;- principal(ess[20:44], nfactors = 5, rotate = &quot;varimax&quot;) print(pca_att, cut = 0.3, digits = 3) ## Principal Components Analysis ## Call: principal(r = ess[20:44], nfactors = 5, rotate = &quot;varimax&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## RC2 RC1 RC5 RC3 RC4 h2 u2 com ## imsmetn 0.796 0.725 0.275 1.30 ## imdfetn 0.776 0.794 0.206 1.69 ## eimrcnt 0.827 0.715 0.285 1.09 ## eimpcnt 0.801 0.789 0.211 1.49 ## imrcntr 0.835 0.747 0.253 1.15 ## impcntr 0.777 0.782 0.218 1.63 ## qfimchr 0.819 0.699 0.301 1.09 ## qfimwht 0.764 0.654 0.346 1.25 ## imwgdwn 0.806 0.712 0.288 1.19 ## imhecop 0.747 0.669 0.331 1.42 ## imtcjob 0.565 0.335 0.480 0.520 1.99 ## imbleco 0.702 0.553 0.447 1.25 ## imbgeco 0.697 0.603 0.397 1.52 ## imueclt 0.569 -0.336 0.544 0.456 2.42 ## imwbcnt 0.673 0.633 0.367 1.87 ## imwbcrm 0.657 0.480 0.520 1.23 ## imrsprc 0.614 0.441 0.559 1.35 ## pplstrd 0.329 -0.539 0.460 0.540 2.17 ## vrtrlg -0.349 0.460 0.415 0.585 2.75 ## shrrfg 0.368 -0.352 0.417 0.583 4.11 ## rfgawrk 0.615 0.397 0.603 1.10 ## gvrfgap 0.691 0.558 0.442 1.35 ## rfgfrpc -0.388 0.327 0.673 3.31 ## rfggvfn 0.584 0.416 0.584 1.46 ## rfgbfml 0.596 0.460 0.540 1.61 ## ## RC2 RC1 RC5 RC3 RC4 ## SS loadings 4.377 3.404 2.776 2.191 1.722 ## Proportion Var 0.175 0.136 0.111 0.088 0.069 ## Cumulative Var 0.175 0.311 0.422 0.510 0.579 ## Proportion Explained 0.303 0.235 0.192 0.151 0.119 ## Cumulative Proportion 0.303 0.538 0.730 0.881 1.000 ## ## Mean item complexity = 1.7 ## Test of the hypothesis that 5 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.05 ## with the empirical chi square 29520.06 with prob &lt; 0 ## ## Fit based upon off diagonal values = 0.976 Feature engineering (i.e., creating new variables by combining and/or transforming existing variables) is one of the most common applications of PCA. PCA is a dimension reduction technique that distills the most salient information from a set of variables into a (smaller) set of component scores. Hence, PCA can be a good way of creating aggregate items (analogous to weighted scale scores) when the data are not collected with validated scales. Principal component scores are automatically generated when we run the PCA. If we want to use these scores in subsequent analyses (e.g., as predictors in a regression model), we usually add them to our dataset as additional columns. 2.4.9 Add the component scores produced by the analyses you ran above to the ess data frame. Give each component score an informative name, based on your interpretation of the factor loading matrix I.e., What hypothetical construct do you think each component represents given the items that load onto it? Hints: You can use the data.frame() function to join multiple objects into a single data frame. You can use the colnames() function to assign column names to a matrix or data frame. Click for explanation 1. Extract the component scores The object produced by psych::principal() is simply list, and the component scores are already stored therein. So, to extract the component scores, we simply use the $ operator to extract them. ## Save the component scores in stand-alone matrices: trust_scores &lt;- pca_trust$scores att_scores &lt;- pca_att$scores ## Inspect the result: head(trust_scores) ## RC3 RC2 RC1 ## 1 NA NA NA ## 4 0.09755193 -0.01552183 0.994954 ## 7 0.23069626 -1.53162604 -2.022642 ## 14 NA NA NA ## 17 -0.21112678 0.84370377 1.200007 ## 20 1.86596955 0.31083233 -1.062603 summary(trust_scores) ## RC3 RC2 RC1 ## Min. :-4.035 Min. :-3.706 Min. :-3.139 ## 1st Qu.:-0.527 1st Qu.:-0.652 1st Qu.:-0.649 ## Median : 0.155 Median : 0.094 Median : 0.092 ## Mean : 0.055 Mean : 0.015 Mean : 0.049 ## 3rd Qu.: 0.727 3rd Qu.: 0.742 3rd Qu.: 0.742 ## Max. : 3.302 Max. : 3.452 Max. : 3.539 ## NA&#39;s :4912 NA&#39;s :4912 NA&#39;s :4912 head(att_scores) ## RC2 RC1 RC5 RC3 RC4 ## 1 1.9920289 1.3140238 -0.8305392 -0.06329775 -0.08837693 ## 4 0.1708174 -1.2167781 -0.4974957 -0.23766146 0.67364069 ## 7 -0.3580985 0.3236336 -1.5094405 -0.53052720 -2.20637993 ## 14 NA NA NA NA NA ## 17 -0.1136716 -0.7869911 -1.4664715 -0.07112144 0.41078167 ## 20 -0.9188606 2.8264230 -0.3477484 -0.73788338 -1.32089442 summary(att_scores) ## RC2 RC1 RC5 RC3 ## Min. :-3.650 Min. :-3.909 Min. :-3.817 Min. :-2.730 ## 1st Qu.:-0.611 1st Qu.:-0.599 1st Qu.:-0.653 1st Qu.:-0.748 ## Median :-0.080 Median : 0.053 Median :-0.004 Median :-0.152 ## Mean :-0.007 Mean : 0.001 Mean : 0.023 Mean :-0.026 ## 3rd Qu.: 0.686 3rd Qu.: 0.644 3rd Qu.: 0.653 3rd Qu.: 0.620 ## Max. : 3.743 Max. : 4.495 Max. : 4.114 Max. : 3.869 ## NA&#39;s :5400 NA&#39;s :5400 NA&#39;s :5400 NA&#39;s :5400 ## RC4 ## Min. :-3.767 ## 1st Qu.:-0.682 ## Median : 0.045 ## Mean : 0.004 ## 3rd Qu.: 0.716 ## Max. : 3.248 ## NA&#39;s :5400 2. Name the component scores ## Check names (note the order): colnames(trust_scores) ## [1] &quot;RC3&quot; &quot;RC2&quot; &quot;RC1&quot; colnames(att_scores) ## [1] &quot;RC2&quot; &quot;RC1&quot; &quot;RC5&quot; &quot;RC3&quot; &quot;RC4&quot; ## Give informative names: colnames(trust_scores) &lt;- c(&quot;Trust_Institutions&quot;, &quot;Satisfaction&quot;, &quot; Trust_Politicians&quot;) colnames(att_scores) &lt;- c(&quot;Quantity&quot;, &quot;Effects&quot;, &quot;Refugees&quot;, &quot;Diversity&quot;, &quot;Economic&quot;) 3. Add the component scores to the dataset # Add the component scores to the &#39;ess&#39; data: ess &lt;- data.frame(ess, trust_scores, att_scores) 2.4.10 Were you able to replicate the results of Kestilä (2006)? Click for explanation Yes, more-or-less. Although the exact estimates differ somewhat, the general pattern of factor loadings in Kestilä (2006) matches what we found here. "],["in-class-exercises-1.html", "2.5 In-Class Exercises", " 2.5 In-Class Exercises In these exercises, we will continue with our re-analysis/replication of the Kestilä (2006) results. Rather than attempting a direct replication, we will now redo the analysis using exploratory factor analysis (EFA). 2.5.1 Load the ESSround1-b.csv dataset. These are the same data that you analyzed for the At-Home Exercises, but the processing/recoding that you should have done for those exercises has already been implemented. ess &lt;- read.csv(&quot;ESSround1-b.csv&quot;) 2.5.2 Kestilä (2006) claimed that running a PCA is a good way to test if the questions in the ESS measure attitudes towards immigration and trust in politics. Based on what you’ve learned from the readings and lectures, do you agree with this position? Click for information Hopefully not. PCA is not a method for estimating latent measurement structure; PCA is a dimension reduction technique that tries to summarize a set of data with a smaller set of component scores. If we really want to estimate the factor structure underlying a set of observed variables, we should use EFA. 2.5.3 Suppose you had to construct the trust in politics and attitude towards immigration scales described by Kestilä (2006) based on the theory and background information presented in that article. What type of analysis would you choose? What key factors would influence your decision? Click for information We are trying to estimate meaningful latent factors, so EFA would be an appropriate method. The theory presented by Kestilä (2006) did not hypothesize a particular number of factors, so we would need to use appropriate techniques to estimate the best number. In particular, combining information from: Scree plots Parallel analysis Substantive interpretability of the (rotated) factor loadings Since the factors are almost certainly correlated, we should apply an oblique rotation. We will now rerun the two PCAs that you conducted for the At-Home Exercises using EFA. We will estimate the EFA models using the psych::fa() function, but we need to know how many factors to extract. We could simply estimate a range of solutions and compare the results. We can restrict the range of plausible solutions and save some time by first checking/plotting the eigenvalues and running parallel analysis. 2.5.4 Estimate the number of latent factors underlying the Trust items based on the eigenvalues, the scree plot, and parallel analysis. How many factors are suggested by each method? Hint: You can create a scree plot by supplying the vector of eigenvalues to the qplot() function from the ggplot2 package and applying the geom_path() geometry. Click for explanation First, we’ll run a trivial EFA using the psych::fa() function to get eigenvalues. We don’t care about the factors yet, so we can extract a single factor. We also don’t care about interpretable solutions, so we don’t need rotation. ## Load the psych package: library(psych) ## Run a trivial EFA on the &#39;trust&#39; items efa_trust0 &lt;- fa(ess[7:19], nfactors = 1, rotate = &quot;none&quot;) We can check the eigenvalues to see what proportion of the observed variance is accounted for by each additional factor we may extract. round(efa_trust0$values, digits = 3) ## [1] 4.940 0.733 0.492 0.173 0.064 0.014 -0.066 -0.091 -0.195 -0.210 ## [11] -0.289 -0.307 -0.318 Since only one eigenvalue is greater than one, the so-called “Kaiser Criterion” would suggest extracting a single factor. The Kaiser Criterion is not a valid way to select the number of factors in EFA. So, we don’t want to rely on this information alone. We can still use the eigenvalues to help us with factor enumeration, though. One way to do so is by plotting the eigenvalues in a scree plot. We can easily create a scree plot by passing the eigenvalues to the qplot() function from the ggplot2 package and add apply the geom_path() geometry: library(ggplot2) qplot(y = efa_trust0$values) + geom_path() + ylab(&quot;Eigenvalues&quot;) + xlab(&quot;Factors&quot;) Although the scree plot provides useful information, we need to interpret that information subjectively, and the conclusions are sometimes ambiguous. In this case, the plot seems to suggest either one or three components, depending on where we consider the “elbow” to lie. As recommended in the lecture, we can also use “parallel analysis” (Horn, 1965) to provide more quantified information about the number of factors. We’ll use the psych::fa.parallel() function to implement parallel analysis. Parallel analysis relies on randomly simulated/permuted data, so we should set a seed to make sure our results are reproducible. We can set the fa = \"fa\" option to get only the results for EFA. ## Set the random number seed: set.seed(235711) ## Run the parallel analysis: pa_trust &lt;- fa.parallel(ess[7:19], fa = &quot;fa&quot;) ## Parallel analysis suggests that the number of factors = 6 and the number of components = NA The results of the parallel analysis suggest 6 factors. If you’ve been paying close attention, you may have noticed that we need to compute the eigenvalues from the original data to run parallel analysis. Hence, we don’t actually need to run a separate EFA to estimate the eigenvalues. ## View the eigenvalues estimated during the parallel analysis: pa_trust$fa.values ## [1] 4.93985031 0.73335474 0.49174655 0.17294854 0.06372444 0.01363969 ## [7] -0.06605432 -0.09109183 -0.19535125 -0.20991379 -0.28855561 -0.30662420 ## [13] -0.31782296 ## Compare to the version from the EFA: pa_trust$fa.values - efa_trust0$values ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 ## Recreate the scree plot from above: qplot(y = pa_trust$fa.values) + geom_path() + ylab(&quot;Eigenvalues&quot;) + xlab(&quot;Factors&quot;) The different criteria disagree on how many factors we should extract, but we have narrowed the range. Based on the scree plot and parallel analysis, we should consider solutions for 3 to 6 factors. We need to examine the factor loadings to see which solution makes the most substantive sense. 2.5.5 Do the same analysis for the attitudes toward immigration items. Click for explanation This time, we’ll start by running the parallel analysis and get the eigenvalues from the object returned by psych::fa.parallel(). ## Set the seed: set.seed(235711) ## Run parallel analysis on the &#39;attitudes&#39; items: pa_att &lt;- fa.parallel(ess[20:44]) ## Parallel analysis suggests that the number of factors = 7 and the number of components = 5 ## Check the eigenvalues: round(pa_att$fa.values, digits = 3) ## [1] 7.841 1.474 0.748 0.534 0.327 0.152 0.123 0.025 -0.033 -0.064 ## [11] -0.081 -0.098 -0.121 -0.137 -0.145 -0.179 -0.196 -0.204 -0.214 -0.222 ## [21] -0.246 -0.253 -0.331 -0.425 -0.432 ## Create a scree plot: qplot(y = pa_att$fa.values) + geom_path() + ylab(&quot;Eigenvalues&quot;) + xlab(&quot;Factors&quot;) For the attitudes toward immigration analysis, the results are even more ambiguous than they were for the trust items. The Kaiser Criterion suggests 2 factors The scree plot is hopelessly ambiguous At least 3 factors? No more than 9 factors? Parallel analysis suggests 7 factors Based on the scree plot and parallel analysis, it seems reasonable to consider solutions for 3 to 7 factors. Again, we need to check the substantive interpretation to choose the most reasonable solution. To evaluate the substantive interpretability of the different solutions, we need to estimate the full EFA models for each candidate number of factors. We then compare the factor loadings across solutions to see which set of loading define the most reasonable set of latent variables. 2.5.6 For the trust items, estimate the EFA models for each plausible number of components that you identified above. Use the psych::fa() function to estimate the models. You will need to specify a few key options. The data (including only the variables you want to analyze) The number of factors that you want to extract The rotation method The estimation method The method of estimating factor scores Hint: You can save yourself a lot of typing/copy-pasting (and the attendant chances of errors) by using a for() loop to iterate through numbers of factors. Click for explanation ## Define an empty list to hold all of our fitted EFA objects: efa_trust &lt;- list() ## Loop through the interesting numbers of factors and estimate an EFA for each: for(i in 3:6) efa_trust[[i - 2]] &lt;- fa(ess[7:19], nfactors = i, # Number of factors = Loop index rotate = &quot;promax&quot;, # Oblique rotation scores = &quot;Bartlett&quot;) # Estimate factor scores with WLS 2.5.7 Repeat the above analysis for the attitudes items. Click for explanation efa_att &lt;- list() for(i in 3:7) efa_att[[i - 2]] &lt;- fa(ess[20:44], nfactors = i, rotate = &quot;promax&quot;, scores = &quot;Bartlett&quot;) 2.5.8 Compare the factor loading matrices from the models estimated from the Trust items, and select the best solution. Hints: The factor loadings are stored in the loadings slot of the object returned by psych::fa(). Looping can also be useful here. Click for explanation for(x in efa_trust) print(x$loadings) ## ## Loadings: ## MR3 MR1 MR2 ## pltcare -0.103 0.815 ## pltinvt -0.100 0.806 ## trstprl 0.405 0.381 ## trstlgl 0.843 -0.103 ## trstplc 0.776 -0.217 ## trstplt 0.279 0.571 ## trstep 0.503 0.243 ## trstun 0.543 0.199 ## stfeco -0.121 0.156 0.703 ## stfgov 0.246 0.594 ## stfdem 0.180 0.166 0.462 ## stfedu -0.168 0.701 ## stfhlth -0.134 0.664 ## ## MR3 MR1 MR2 ## SS loadings 2.178 2.101 2.023 ## Proportion Var 0.168 0.162 0.156 ## Cumulative Var 0.168 0.329 0.485 ## ## Loadings: ## MR2 MR1 MR4 MR3 ## pltcare -0.106 0.873 ## pltinvt 0.843 ## trstprl 0.118 0.471 0.194 0.106 ## trstlgl 0.785 ## trstplc 0.715 ## trstplt 0.101 0.491 0.335 ## trstep 0.982 -0.171 ## trstun 0.670 0.113 ## stfeco 0.726 ## stfgov 0.629 0.200 -0.157 ## stfdem 0.485 0.129 0.110 ## stfedu 0.712 -0.108 -0.106 0.108 ## stfhlth 0.672 -0.101 ## ## MR2 MR1 MR4 MR3 ## SS loadings 2.161 1.974 1.706 1.210 ## Proportion Var 0.166 0.152 0.131 0.093 ## Cumulative Var 0.166 0.318 0.449 0.542 ## ## Loadings: ## MR1 MR5 MR4 MR3 MR2 ## pltcare 0.872 ## pltinvt 0.871 ## trstprl 0.360 0.142 0.248 0.201 -0.104 ## trstlgl 0.940 ## trstplc 0.808 ## trstplt 0.294 0.316 0.308 ## trstep -0.140 -0.105 1.123 -0.157 ## trstun 0.515 0.163 ## stfeco 0.701 -0.120 0.181 ## stfgov 0.992 -0.109 -0.129 ## stfdem 0.526 0.158 ## stfedu 0.746 ## stfhlth 0.578 ## ## MR1 MR5 MR4 MR3 MR2 ## SS loadings 2.016 1.668 1.714 1.681 0.956 ## Proportion Var 0.155 0.128 0.132 0.129 0.074 ## Cumulative Var 0.155 0.283 0.415 0.545 0.618 ## ## Loadings: ## MR6 MR3 MR5 MR4 MR1 MR2 ## pltcare 0.840 ## pltinvt 0.899 ## trstprl 1.068 ## trstlgl -0.117 0.742 0.160 ## trstplc 0.955 -0.141 ## trstplt 0.156 0.296 0.172 0.303 ## trstep -0.129 1.175 ## trstun 0.227 0.390 ## stfeco 0.588 0.216 ## stfgov 1.041 -0.101 ## stfdem 0.423 0.151 0.119 ## stfedu 0.688 ## stfhlth 0.743 ## ## MR6 MR3 MR5 MR4 MR1 MR2 ## SS loadings 1.667 1.629 1.573 1.579 1.310 1.103 ## Proportion Var 0.128 0.125 0.121 0.121 0.101 0.085 ## Cumulative Var 0.128 0.254 0.375 0.496 0.597 0.682 Note: Any factor loadings with magnitude lower than 0.1 are suppressed in above output. The factor loadings matrix indicates how strongly each latent factor (columns) associates with the observed items (rows). We can interpret these factor loadings in the same way that we would interpret regression coefficients (indeed, a factor analytic model can be viewed as a multivariate regression model wherein the latent factors are the predictors and the observed items are the outcomes). A higher factor loading indicates a stronger association between the item and factor linked by that loading. Items with high factor loadings are “good” indicators of the respective factors. Items with only very low loadings do not provide much information about any factor. You may want to exclude such items from your analysis. Note that the size of the factor loadings depends on the number of factors. So, you should only consider excluding an observed item after you have chosen the number of latent factors. When we print the loading matrix, we see additional information printed below the factor loadings. Proportion Var: What proportion of the items’ variance is explained by each of the factors. Cumulative Var: How much variance the factors explain, in total. If you estimated as many factors as items, then the Cumulative Var for the final factor would be 1.00 (i.e., 100%). 2.5.9 Compare the factor loading matrices from the models estimated from the Attitudes items, and select the best solution. Click for explanation for(x in efa_att) print(x$loadings) ## ## Loadings: ## MR1 MR2 MR3 ## imsmetn 0.801 ## imdfetn 0.756 0.111 ## eimrcnt 0.837 ## eimpcnt 0.813 ## imrcntr 0.854 ## impcntr 0.771 ## qfimchr 0.233 0.852 ## qfimwht 0.135 0.712 ## imwgdwn 0.308 -0.163 ## imhecop 0.380 -0.143 ## imtcjob 0.615 ## imbleco 0.691 ## imbgeco 0.681 ## imueclt 0.558 -0.209 ## imwbcnt 0.731 ## imwbcrm 0.634 ## imrsprc -0.487 -0.119 ## pplstrd 0.259 -0.413 ## vrtrlg -0.261 0.274 ## shrrfg 0.529 -0.105 ## rfgawrk -0.368 ## gvrfgap -0.615 -0.151 ## rfgfrpc 0.449 ## rfggvfn -0.478 ## rfgbfml -0.541 ## ## MR1 MR2 MR3 ## SS loadings 4.811 3.943 1.667 ## Proportion Var 0.192 0.158 0.067 ## Cumulative Var 0.192 0.350 0.417 ## ## Loadings: ## MR2 MR1 MR4 MR3 ## imsmetn 0.788 ## imdfetn 0.734 0.144 0.115 ## eimrcnt 0.854 -0.146 ## eimpcnt 0.788 0.161 ## imrcntr 0.861 ## impcntr 0.744 0.191 ## qfimchr -0.141 0.859 ## qfimwht 0.735 ## imwgdwn 0.601 0.237 ## imhecop 0.656 0.210 ## imtcjob 0.671 0.141 ## imbleco 0.607 -0.157 0.156 ## imbgeco 0.635 -0.123 ## imueclt 0.368 -0.241 -0.187 ## imwbcnt 0.536 -0.262 ## imwbcrm 0.430 -0.257 ## imrsprc 0.605 ## pplstrd 0.219 -0.396 ## vrtrlg 0.220 0.295 ## shrrfg 0.300 -0.277 ## rfgawrk 0.447 ## gvrfgap 0.765 ## rfgfrpc 0.228 -0.263 ## rfggvfn 0.483 ## rfgbfml 0.635 ## ## MR2 MR1 MR4 MR3 ## SS loadings 3.831 2.867 2.460 1.671 ## Proportion Var 0.153 0.115 0.098 0.067 ## Cumulative Var 0.153 0.268 0.366 0.433 ## ## Loadings: ## MR2 MR1 MR5 MR3 MR4 ## imsmetn 0.794 ## imdfetn 0.733 0.156 0.117 ## eimrcnt 0.905 -0.139 -0.229 ## eimpcnt 0.781 0.110 0.197 ## imrcntr 0.908 -0.117 -0.179 ## impcntr 0.734 0.123 0.231 ## qfimchr 0.114 -0.156 0.864 ## qfimwht 0.165 0.735 ## imwgdwn 0.717 ## imhecop 0.695 ## imtcjob 0.532 0.129 0.207 ## imbleco 0.691 0.142 ## imbgeco 0.793 ## imueclt 0.560 -0.211 ## imwbcnt 0.711 ## imwbcrm 0.557 -0.110 ## imrsprc 0.610 ## pplstrd 0.224 -0.406 ## vrtrlg -0.222 0.102 0.308 0.103 ## shrrfg 0.228 -0.266 0.121 ## rfgawrk 0.450 ## gvrfgap 0.778 ## rfgfrpc -0.317 0.165 ## rfggvfn 0.503 ## rfgbfml -0.110 0.562 ## ## MR2 MR1 MR5 MR3 MR4 ## SS loadings 3.974 2.799 2.193 1.693 1.131 ## Proportion Var 0.159 0.112 0.088 0.068 0.045 ## Cumulative Var 0.159 0.271 0.359 0.426 0.472 ## ## Loadings: ## MR2 MR1 MR6 MR3 MR5 MR4 ## imsmetn 0.696 0.175 ## imdfetn 0.834 ## eimrcnt 0.236 0.868 ## eimpcnt 0.943 ## imrcntr 0.447 0.526 ## impcntr 0.956 ## qfimchr 0.140 -0.120 0.858 ## qfimwht 0.174 0.723 ## imwgdwn 0.723 ## imhecop 0.678 ## imtcjob 0.551 0.127 0.202 ## imbleco 0.753 0.150 ## imbgeco 0.813 ## imueclt 0.567 -0.210 ## imwbcnt 0.744 ## imwbcrm 0.601 ## imrsprc 0.158 0.514 -0.103 ## pplstrd 0.222 -0.402 ## vrtrlg -0.224 0.305 0.101 ## shrrfg 0.217 -0.281 -0.102 0.125 ## rfgawrk 0.499 ## gvrfgap 0.786 ## rfgfrpc 0.100 -0.279 0.156 ## rfggvfn 0.530 ## rfgbfml 0.610 ## ## MR2 MR1 MR6 MR3 MR5 MR4 ## SS loadings 3.289 2.992 1.988 1.649 1.091 1.102 ## Proportion Var 0.132 0.120 0.080 0.066 0.044 0.044 ## Cumulative Var 0.132 0.251 0.331 0.397 0.440 0.484 ## ## Loadings: ## MR2 MR1 MR6 MR7 MR5 MR4 MR3 ## imsmetn 0.715 0.170 ## imdfetn 0.854 ## eimrcnt 0.255 0.852 ## eimpcnt 0.965 ## imrcntr 0.467 0.516 ## impcntr 0.978 ## qfimchr -0.231 0.625 ## qfimwht 0.107 0.718 ## imwgdwn 0.735 ## imhecop 0.715 ## imtcjob 0.573 -0.163 0.198 ## imbleco 0.709 ## imbgeco 0.852 -0.108 ## imueclt 0.435 0.288 -0.110 ## imwbcnt 0.576 0.235 ## imwbcrm 0.423 0.225 0.122 ## imrsprc 0.160 0.480 -0.100 ## pplstrd 0.648 -0.109 ## vrtrlg -0.527 ## shrrfg -0.212 0.324 0.149 ## rfgawrk 0.517 ## gvrfgap 0.744 ## rfgfrpc -0.213 0.237 0.182 ## rfggvfn 0.542 ## rfgbfml 0.599 ## ## MR2 MR1 MR6 MR7 MR5 MR4 MR3 ## SS loadings 3.462 2.275 1.830 1.175 1.052 1.184 0.991 ## Proportion Var 0.138 0.091 0.073 0.047 0.042 0.047 0.040 ## Cumulative Var 0.138 0.229 0.303 0.350 0.392 0.439 0.479 It is very possible that you selected a different numbers of factors than Kestilä (2006). We need to keep these exercises consistent, though. So, the remaining questions will all assume you have extract three factors from the Trust items and five factors from the Attitudes items, to parallel the Kestilä (2006) results. ## Select the three-factor solution for &#39;trust&#39;: efa_trust &lt;- efa_trust[[1]] ## Select the five-factor solution for &#39;attitudes&#39;: efa_att &lt;- efa_att[[3]] 2.5.10 Give the factor scores meaningful names, and add the scores to the ess dataset as new columns. Hint: If you’re not sure of what do to, check 2.4.9. Click for explanation ## Rename the factor scores: colnames(efa_trust$scores) &lt;- c(&quot;trust_pol&quot;, &quot;satisfy&quot;, &quot;trust_inst&quot;) colnames(efa_att$scores) &lt;- c(&quot;effects&quot;, &quot;allowance&quot;, &quot;refugees&quot;, &quot;ethnic&quot;, &quot;europe&quot;) ## Add factor scores to the dataset as new columns: ess &lt;- data.frame(ess, efa_trust$scores, efa_att$scores) Kestilä (2006) used the component scores to descriptively evaluate country-level differences in Attitudes toward Immigration and Political Trust. So, now it’s time to replicate those analyses. 2.5.11 Repeat the Kestilä (2006) between-country comparison using the factor scores you created in 2.5.10 and an appropriate statistical test. Click for explanation Here, we’ll only demonstrate a possible approach to analyzing one of the Trust dimensions. We can use an ANOVA to test whether the countries differ in average levels of Trust in Institutions (as quantified by the relevant factor score). out &lt;- aov(trust_inst ~ cntry, data = ess) summary(out) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## cntry 8 4730 591.2 637.3 &lt;2e-16 *** ## Residuals 14769 13700 0.9 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 3409 observations deleted due to missingness According to the omnibus F-test, average levels of Trust in Institutions significantly differ between countries, but this test cannot tell us between which countries the differences lie. One way to test for differences between the individual countries would be a post hoc test of all pairwise comparisons. Since we’ll be doing 45 tests, we need to apply a correction for repeated testing. Below, we use Tukey’s HSD correction. TukeyHSD(out) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = trust_inst ~ cntry, data = ess) ## ## $cntry ## diff lwr upr p adj ## Belgium-Austria 0.20168084 0.09589531 0.307466370 0.0000001 ## Denmark-Austria 0.76351677 0.65075137 0.876282159 0.0000000 ## Finland-Austria 0.64445653 0.54454924 0.744363816 0.0000000 ## Germany-Austria -1.06714607 -1.16042396 -0.973868169 0.0000000 ## Italy-Austria -0.71544868 -0.83369744 -0.597199916 0.0000000 ## Netherlands-Austria -0.21332373 -0.31147689 -0.115170569 0.0000000 ## Norway-Austria -0.02389303 -0.12537152 0.077585455 0.9983715 ## Sweden-Austria -0.20104524 -0.30601176 -0.096078730 0.0000001 ## Denmark-Belgium 0.56183592 0.44430483 0.679367018 0.0000000 ## Finland-Belgium 0.44277568 0.33751890 0.548032469 0.0000000 ## Germany-Belgium -1.26882691 -1.36781322 -1.169840595 0.0000000 ## Italy-Belgium -0.91712952 -1.03993136 -0.794327672 0.0000000 ## Netherlands-Belgium -0.41500457 -0.51859785 -0.311411294 0.0000000 ## Norway-Belgium -0.22557387 -0.33232315 -0.118824600 0.0000000 ## Sweden-Belgium -0.40272609 -0.51279649 -0.292655685 0.0000000 ## Finland-Denmark -0.11906024 -0.23132977 -0.006790713 0.0279561 ## Germany-Denmark -1.83066283 -1.93707592 -1.724249740 0.0000000 ## Italy-Denmark -1.47896544 -1.60782874 -1.350102145 0.0000000 ## Netherlands-Denmark -0.97684049 -1.08755193 -0.866129055 0.0000000 ## Norway-Denmark -0.78740980 -0.90107977 -0.673739822 0.0000000 ## Sweden-Denmark -0.96456201 -1.08135648 -0.847767536 0.0000000 ## Germany-Finland -1.71160259 -1.80428041 -1.618924768 0.0000000 ## Italy-Finland -1.35990520 -1.47768118 -1.242129219 0.0000000 ## Netherlands-Finland -0.85778025 -0.95536333 -0.760197183 0.0000000 ## Norway-Finland -0.66834956 -0.76927674 -0.567422375 0.0000000 ## Sweden-Finland -0.84550177 -0.94993539 -0.741068145 0.0000000 ## Italy-Germany 0.35169739 0.23949007 0.463904704 0.0000000 ## Netherlands-Germany 0.85382234 0.76303822 0.944606449 0.0000000 ## Norway-Germany 1.04325303 0.94888357 1.137622497 0.0000000 ## Sweden-Germany 0.86610082 0.76799027 0.964211378 0.0000000 ## Netherlands-Italy 0.50212495 0.38583325 0.618416642 0.0000000 ## Norway-Italy 0.69155564 0.57244394 0.810667352 0.0000000 ## Sweden-Italy 0.51440343 0.39230640 0.636500461 0.0000000 ## Norway-Netherlands 0.18943070 0.09023961 0.288621788 0.0000001 ## Sweden-Netherlands 0.01227848 -0.09047831 0.115035276 0.9999902 ## Sweden-Norway -0.17715221 -0.28308992 -0.071214500 0.0000077 The second part of the Kestilä (2006) analysis was to evaluate how socio-demographic characteristics affected attitudes towards immigrants and trust in politics among the Finnish electorate. Before we can replicate this part of the analysis, we need to subset the data to only the Finnish cases. 2.5.12 Create a new data frame that contains only the Finnish cases from ess. Hint: You can use logical indexing based on the cntry variable. Click for explanation ess_finland &lt;- ess[ess$cntry == &quot;Finland&quot;, ] We still have one more step before we can estimate any models. We must prepare our variables for analysis. Our dependent variables will be the factor scores generated above. So, we do not need to apply any further processing. We have not yet used any of the independent variables, though. So, we should inspect those variables to see if they require any processing. In the ess data, the relevant variables have the following names: gndr yrbrn eduyrs polintr lrscale 2.5.13 Inspect the independent variables listed above. Click for explanation library(tidySEM) descriptives(ess_finland[c(&quot;gndr&quot;, &quot;yrbrn&quot;, &quot;eduyrs&quot;, &quot;polintr&quot;, &quot;lrscale&quot;)]) It looks like we still need some recoding. 1. Age The data contain the participants’ years of birth instead of their age, but Kestilä analyzed age. Fortunately, we know that the data were collected in 2002, so we can simply subtract each participant’s value of yrbrn from the 2002 to compute their age. ess_finland$age &lt;- 2002 - ess_finland$yrbrn 2. Political Interest &amp; Political Orientation The variables polintr and lrscale are represented as character vectors. If we analyze these variables as they are, R will convert the character vectors to factors and make dummy codes for each distinct value; we definitely don’t want that. We need to convert polintr and lrscale to numeric vectors, but we cannot naively apply the as.numeric() function, because R doesn’t know how to convert a word into a number (or, at least, not how we want the operation to be done). There are many ways that we could go about this conversion, but the recode() function from the dplyr package is particularly useful for arbitrary recoding tasks such as we have here. Kestilä (2006) dichotomized polintr by combining the lowest two and highest two categories. So, we don’t actually want to convert the polint variable into a numeric, Likert-type variable. We want polint to be a binary factor. The recode_factor() function from dplyr() will automatically convert our result into a factor. ## Load the dplyr package: library(dplyr) ## Recode the four character values into two factor levels: ess_finland$polintr_bin &lt;- recode_factor(ess_finland$polintr, &quot;Not at all interested&quot; = &quot;Low Interest&quot;, &quot;Hardly interested&quot; = &quot;Low Interest&quot;, &quot;Quite interested&quot; = &quot;High Interest&quot;, &quot;Very interested&quot; = &quot;High Interest&quot;) ## Check the conversion: table(old = ess_finland$polintr, new = ess_finland$polintr_bin) ## new ## old Low Interest High Interest ## Hardly interested 842 0 ## Not at all interested 228 0 ## Quite interested 0 785 ## Very interested 0 144 For political orientation, only the two extreme values of the scale are labeled as text. So, we only need to recode those two levels. Since we’re only replacing two of the 11 values, we’ll need to provide a value for the .default argument in dplyr::recode(). ## Recode the extreme levels: tmp &lt;- recode(ess_finland$lrscale, &quot;Left&quot; = 0, &quot;Right&quot; = 10, .default = as.numeric(ess_finland$lrscale) ) ## Check the conversion: table(old = ess_finland$lrscale, new = tmp) ## new ## old 0 1 2 3 4 5 6 7 8 9 10 ## 1 0 25 0 0 0 0 0 0 0 0 0 ## 2 0 0 62 0 0 0 0 0 0 0 0 ## 3 0 0 0 148 0 0 0 0 0 0 0 ## 4 0 0 0 0 183 0 0 0 0 0 0 ## 5 0 0 0 0 0 599 0 0 0 0 0 ## 6 0 0 0 0 0 0 199 0 0 0 0 ## 7 0 0 0 0 0 0 0 296 0 0 0 ## 8 0 0 0 0 0 0 0 0 217 0 0 ## 9 0 0 0 0 0 0 0 0 0 79 0 ## Left 24 0 0 0 0 0 0 0 0 0 0 ## Right 0 0 0 0 0 0 0 0 0 0 59 ## Overwrite the old variable: ess_finland$lrscale &lt;- tmp 3. Sex Although gndr would be automatically converted to a factor by lm(), it’s better to be explicit about our intentions and create the factor ourselves. ess_finland$sex &lt;- factor(ess_finland$gndr) Now, we’re finally ready to replicate the regression analysis from Kestilä (2006). Creating a single aggregate score by summing the individual component scores is a pretty silly thing to do, though. So, we won’t reproduce that aspect of the analysis. 2.5.14 Run a series of multiple linear regression analyses with the factor scores you created in 2.5.10 as the dependent variables and the same predictors used by Kestilä (2006). Do your results agree with those reported by Kestilä (2006)? Click for explanation ## Predicting &#39;Trust in Institutions&#39;: out_trust_inst &lt;- lm(trust_inst ~ sex + age + eduyrs + polintr_bin + lrscale, data = ess_finland) summary(out_trust_inst) ## ## Call: ## lm(formula = trust_inst ~ sex + age + eduyrs + polintr_bin + ## lrscale, data = ess_finland) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6185 -0.4519 0.1278 0.5827 2.8276 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.450351 0.113407 3.971 7.45e-05 *** ## sexMale 0.122805 0.040248 3.051 0.00231 ** ## age -0.005010 0.001230 -4.073 4.85e-05 *** ## eduyrs -0.003403 0.005692 -0.598 0.55002 ## polintr_binHigh Interest 0.071832 0.041387 1.736 0.08281 . ## lrscale 0.085176 0.010008 8.511 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8306 on 1734 degrees of freedom ## (260 observations deleted due to missingness) ## Multiple R-squared: 0.05488, Adjusted R-squared: 0.05215 ## F-statistic: 20.14 on 5 and 1734 DF, p-value: &lt; 2.2e-16 ## Predicting &#39;Trust in Politicians&#39;: out_trust_pol &lt;- lm(trust_pol ~ sex + age + eduyrs + polintr_bin + lrscale, data = ess_finland) summary(out_trust_pol) ## ## Call: ## lm(formula = trust_pol ~ sex + age + eduyrs + polintr_bin + lrscale, ## data = ess_finland) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.0051 -0.5076 0.1363 0.6615 2.5792 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.094787 0.127932 -0.741 0.458845 ## sexMale -0.001633 0.045404 -0.036 0.971316 ## age -0.002739 0.001388 -1.974 0.048569 * ## eduyrs 0.024486 0.006421 3.813 0.000142 *** ## polintr_binHigh Interest 0.154389 0.046688 3.307 0.000963 *** ## lrscale 0.058146 0.011290 5.150 2.9e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9369 on 1734 degrees of freedom ## (260 observations deleted due to missingness) ## Multiple R-squared: 0.04013, Adjusted R-squared: 0.03736 ## F-statistic: 14.5 on 5 and 1734 DF, p-value: 6.103e-14 ## Predicting &#39;Attitudes toward Refugees&#39;: out_refugees &lt;- lm(refugees ~ sex + age + eduyrs + polintr_bin + lrscale, data = ess_finland) summary(out_refugees) ## ## Call: ## lm(formula = refugees ~ sex + age + eduyrs + polintr_bin + lrscale, ## data = ess_finland) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8921 -0.7112 -0.0618 0.6912 4.1660 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.6771470 0.1489234 -4.547 5.83e-06 *** ## sexMale 0.4853926 0.0524999 9.246 &lt; 2e-16 *** ## age -0.0004773 0.0016248 -0.294 0.768968 ## eduyrs -0.0254016 0.0075576 -3.361 0.000794 *** ## polintr_binHigh Interest -0.2148018 0.0541602 -3.966 7.61e-05 *** ## lrscale 0.0940049 0.0131295 7.160 1.20e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.074 on 1699 degrees of freedom ## (295 observations deleted due to missingness) ## Multiple R-squared: 0.09368, Adjusted R-squared: 0.09102 ## F-statistic: 35.12 on 5 and 1699 DF, p-value: &lt; 2.2e-16 That does it for our replication of the Kesilä (2006) analyses, but we still have one more topic to consider in this practical. One of the most common applications of EFA is scale development. Given a pool of items without a known factor structure, we try to estimate the underlying latent factors that define the (sub)scales represented by our items. In such applications, we use the factor loading matrix for our optimal solution to make “bright-line” assignments of items to putative factors according to the simple structure represented by the estimated factor loading matrix. In other words, we disregard small factor loadings and assign observed items to only the single latent factor upon which they load most strongly. We then hypothesize that those items are true indicators of that latent factor. We can use confirmatory factor analysis (which you will learn about next week) to test rigorously this hypothesis, but we can already get started by estimating the internal consistency (a type of reliability) of the hypothesized subscales. 2.5.15 Estimate the internal consistency of the three Trust subscales and five Attitudes subscales implied by your EFA solutions from above. Use Cronbach’s Alpha to quantify internal consistency. Use the alpha() function from the psych() package to conduct the analysis. Run your analysis on the full ess dataset, not the Finnish subset. Are the subscales implied by your EFA reliable, in the sense of good internal consistency? Note that \\(\\alpha &gt; 0.7\\) is generally considered acceptable, and \\(\\alpha &gt; 0.8\\) is usually considered good. Click for explanation Let’s estimate the reliability of the Satisfaction subscale from the Trust analysis. According to our EFA, the Satisfaction subscale should be indicated by the following five variables: stfeco stfgov stfdem stfedu stfhlth To estimate the internal consistency of this subscale, we simply provide a data frame containing only the subscale data to the alpha() function. ## Define a vector of subscale item names: (targets &lt;- paste0(&quot;stf&quot;, c(&quot;eco&quot;, &quot;gov&quot;, &quot;dem&quot;, &quot;edu&quot;, &quot;hlth&quot;))) ## [1] &quot;stfeco&quot; &quot;stfgov&quot; &quot;stfdem&quot; &quot;stfedu&quot; &quot;stfhlth&quot; ## Run the reliability analysis on the subscale data: (out &lt;- psych::alpha(ess[targets])) ## ## Reliability analysis ## Call: psych::alpha(x = ess[targets]) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.79 0.79 0.77 0.44 3.9 0.0024 6.4 1.7 0.41 ## ## 95% confidence boundaries ## lower alpha upper ## Feldt 0.79 0.79 0.8 ## Duhachek 0.79 0.79 0.8 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## stfeco 0.74 0.74 0.70 0.42 2.9 0.0032 0.0067 0.39 ## stfgov 0.74 0.74 0.69 0.42 2.9 0.0031 0.0031 0.41 ## stfdem 0.75 0.75 0.71 0.43 3.0 0.0030 0.0074 0.41 ## stfedu 0.76 0.76 0.72 0.45 3.2 0.0029 0.0103 0.43 ## stfhlth 0.78 0.78 0.73 0.47 3.5 0.0027 0.0063 0.45 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## stfeco 17728 0.78 0.77 0.70 0.62 6.1 2.3 ## stfgov 17621 0.77 0.77 0.70 0.61 5.5 2.3 ## stfdem 17624 0.75 0.75 0.66 0.58 6.8 2.2 ## stfedu 17380 0.73 0.73 0.62 0.55 7.0 2.3 ## stfhlth 17981 0.70 0.69 0.57 0.50 6.8 2.3 The raw_alpha value is the estimate of Cronbach’s Alpha. In this case \\(\\alpha = 0.795\\), so the subscale is pretty reliable. The table labeled “Reliability if an item is dropped” shows what Cronbach’s Alpha would be if each item were excluded from the scale. If this value is notably higher than the raw_alpha value, it could indicate a bad item. Note that reliability is only one aspect of scale quality, though. So, you shouldn’t throw out items just because they perform poorly in reliability analysis. End of In-Class Exercises "],["cfa.html", "3 CFA", " 3 CFA Homework before the lecture Watch the Lecture Recording for this week. Complete the Reading for this week, and answer the associated reading questions. Complete the Formative Assessment for this week. Lecture content In this lecture, we will introduce confirmatory factor analysis (CFA) and discuss how it differs from EFA. Furthermore, we will revisit the idea of model fit and introduce into the R-package lavaan. Homework before the practical Complete the At-Home Exercises. Practical content During the practical you will work on the In-Class Exercises. "],["lecture-2.html", "3.1 Lecture", " 3.1 Lecture Download slides Often, we work with scales that have a validated or hypothesized factor structure. In the former case, the scale structure has been validated through previous psychometric studies. In the latter case, we may have conducted an EFA to estimate the factor structure on prior data, or theory/intuition may suggest a plausible structure. Regardless of how we come to expect a given factor structure, such situations represent confirmatory modeling problems, because we are attempting to empirically confirm an a priori expectation. Hence, exploratory methods like EFA are not appropriate, and we should employ confirmatory modeling techniques. This week we consider one such technique: confirmatory factor analysis (CFA). As the name suggests, CFA is related to the EFA methods we discussed last week in that both methods are flavors of factor analysis. However, the two methods address fundamentally different research questions. Rather than attempting to estimate an unknown factor structure (as in EFA), we now want to compare a hypothesized measurement model (i.e., factor structure) to observed data in order to evaluate the model’s plausibility. "],["reading-2.html", "3.2 Reading", " 3.2 Reading Reference Byrne, B. (2005). Factor analytic models: Viewing the structure of an assessment instrument from three perspectives, Journal of Personality Assessment, 85(1), 17–32. Questions What are the main differences between exploratory factor analysis (EFA) and confirmatory factor analysis (CFA)? In which circumstances should a researcher use EFA, and in which should they use CFA? What are the five main limitations of EFA that CFA overcomes? In which circumstances can a second order CFA model be useful? Consider the following four techniques: PCA, EFA, CFA, second order CFA. For each of the following three research situations, which of the above techniques would you use and why? A researcher has developed a new questionnaire that should measure personality and wants to know how many factors underlie the items in their new measure. A researcher is modeling data collected with a seven-item scale that has been used since the 1960s to measure authoritarianism. A researcher has recorded highest completed level of education, years of education, and highest level of education attempted for all respondents in a survey. The researcher wants to include some operationalization of the concept of ‘education’ in their model but is unsure of which observed variable to use. Answers “EFA is primarily a data-driven approach, whereas CFA is theoretically grounded.” (p. 17) “EFA is most appropriately used when links between the observed variables and their underlying factors are unknown or uncertain…One logical application of EFA would be in the development of a new assessment measure…In contrast, CFA is appropriately used when the researcher has some knowledge of the underlying latent variable structure. Based on theory…he or she postulates relations between the observed measures and the underlying factors a priori, and then tests this hypothesized structure statistically.” (p. 17) From page 18: In EFA, all common factors are either uncorrelated (orthogonal) or correlated (oblique). In CFA, arbitrary latent covariance structures can be specified by the researcher In EFA, all observed variables are influenced by all latent variables. In CFA, each observed variable is explained by only the latent variables (usually only one) specified by the researcher. In EFA, the unique factors are uncorrelated. In CFA, theoretically interesting residual correlations can be estimated between unique factors. In EFA, we very limited options when it comes to testing model fit. In CFA, testing model fit is a fundamental part of the analysis that is facilitated by a wide array of statistics and tools. In EFA, you cannot test whether factor structures and/or loadings are the same across groups. In CFA we can easily do so via multiple group modeling and measurement invariance testing. When the model contains multiple latent factors (ideally, more than two, for model identification) which can be explained by some common latent construct. Although several different approaches could be justified, the following seem like the most sensible: EFA: The factor structure is unknown and the researcher’s goal is to estimate this unknown factor structure. CFA: The factor structure is well-established. The researcher can use CFA quantify a latent authoritarianism variable for their model. CFA: The researcher doesn’t need to choose one variable from their three options. They can define a latent variable using all three items as indicators, and use that latent variable in subsequent analysis. "],["formative-assessment-2.html", "3.3 Formative Assessment", " 3.3 Formative Assessment A formative assessment helps you judge your progress in the course and helps you address any blind spots in your understanding of the material. This assessment does not contribute to your course grade. If you get a question wrong, you will receive a hint on how to improve your understanding of the material. Ideally, you should complete this assessment after you have watched the lecture recording, but before the lecture meeting. This way, if you discover gaps in your understanding, we can discuss any topics that need more attention during the lecture meeting. Question 1: A researcher has developed a new questionnaire that should measure depression, and wants to know how many factors there are. Which technique would you use? EFA PCA CFA Question 2: In CFA, some parameters must be constrained in order to make the model identified. TRUE FALSE Question 3: PCA and EFA both assume that indicator variables do not have measurement error FALSE TRUE Question 4: Which of these methods is valid for choosing the correct number of factors in CFA? Parallel analysis Scree plot Ability to predict a criterion scale. Kaiser’s criterion Theory Question 5: The minimum number of items for a CFA is… 2 3 5 1 Question 6: Generally, more factor loadings are estimated in a EFA model than in a CFA model FALSE TRUE Question 7: Rotation can be applied to the loadings of both EFA and CFA. FALSE TRUE "],["at-home-exercises-2.html", "3.4 At-Home Exercises", " 3.4 At-Home Exercises Last week, you used PCA and EFA to replicate the results of Kestilä (2006). In this week’s In-Class Exercises, you will revisit the Kestilä (2006) analysis one more time, now using confirmatory factor analysis (CFA). In this course, we will use the R-package lavaan to fit CFA models (as well as the path models and structural equation models you’ll learn about in the coming weeks). Before you can jump into fitting a CFA to the Kestilä (2006) data, however, you need to learn the basics of working with lavaan. To prepare for the In-Class Exercises, work your way through this introduction to lavaan (part of which originates from the official lavaan tutorial). 3.4.1 To get started with lavaan, complete the following two chapters from the official lavaan tutorial: Installing lavaan Lavaan syntax You only need to read this one Now, we’ll demonstrate the basic lavaan workflow by estimating a linear regression model using observed variables. We will analyze data from Hamilton (1990) which quantify several attributes of 21 states in the US. We will use three of these variables in this tutorial: SAT: Average SAT score Income: Per capita income expressed in $1000 units Education: Median education for residents 25 years of age or older The data are available in either CSV or XLSX format via the Hamilton.csv or Hamilton.xlsx files, respectively. 3.4.2 Load the Hamilton (1990) data. Hints: Use the read.csv() function to load the data from CSV. Use the readxl::read_excel() function to load the data from XLSX. Both files contain the same data; you only need to load one version. Click for explanation ## Load the readxl library: library(readxl) ## Read the first sheet from &#39;Hamilton.xlsx&#39; into a data frame called &#39;hamilton&#39; hamilton &lt;- read_excel(&quot;Hamilton.xlsx&quot;, sheet = 1) ## OR ## ## Read the data from &#39;Hamilton.csv&#39; into a data frame called &#39;hamilton&#39; hamilton &lt;- read.csv(&quot;Hamilton.csv&quot;) The following path diagram shows the model we will fit to these data. This diagram represents a linear regression wherein Education and Income predict SAT. Furthermore, Education and Income are modeled as correlated random variables. As with any sensible model, Education and Income will not perfectly explain SAT (if this claim doesn’t make sense, think back to our discussions of model complexity and the strengths of parsimonious formal models in Week 1). Consequently, the variable Other represents the residual of SAT (i.e., all aspects of SAT that are not, linearly, related to Education or Income). Although not related to the mechanics of using lavaan, you may find the following equivalencies interesting. The regression model we’re considering here is actually a simple example of a path model such as those we’ll start with in Week 5 of this course. The Other factor shown in the diagram is equivalent to the residual term in a linear regression model. The Other factor is also equivalent to the unique factor or uniqueness for an observed indicator in an EFA or CFA model. 3.4.3 Based on what you learning from the lavaan tutorial, write out the lavaan model syntax for the model represented by the path diagram above. Write the syntax as a character string. Save this string as an object in your environment. Click for explanation mod &lt;- &#39; SAT ~ Income + Education Income ~~ Education &#39; This syntax specifies two regression slopes and one covariance. However, our specification also implies three variances which lavaan will automatically estimate. The residual variance of SAT The variance of Income The variance of Education Note that in the path diagram, all three variables have arrows pointing toward them. Hence, all three variables are random, which means we must model their variances. If we had not correlated Income and Education, we could have modeled them as fixed predictors and not estimated their variance. SAT is the DV in our model, so it must be random, and we must estimate its (residual) variance. If we wanted to fully explicate all paths in terms of lavaan syntax, we could do so as follows. mod &lt;- &#39; SAT ~ Income + Education Income ~~ Education SAT ~~ SAT Income ~~ Income Education ~~ Education &#39; In lavaan, we fit structural models (i.e., models with directional, causal relations) using the sem() function. We will use the sem() function to fit the model you specified in 3.4.3 to the Hamilton data. 3.4.4 Consult the documentation for lavaan::sem() to learn what you must do to estimate your model. Click for explanation ?lavaan::sem 3.4.5 Use the sem() function to fit the model you defined in 3.4.3 to the Hamilton data. Use the default settings, for now. Save the result to an object in your environment. Click for explanation ## Load the lavaan package: library(lavaan) ## Fit the model, and store the result in an object called &#39;fit&#39;: fit &lt;- sem(mod, data = hamilton) You can safely ignore the warning about the variances’ scales. 3.4.6 Summarize the fitted model by running summary() on the results object from 3.4.5. Do either of the predictors have a significant effect on SAT? Click for explanation summary(fit) ## lavaan 0.6-12 ended normally after 19 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 21 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## SAT ~ ## Income 2.156 3.050 0.707 0.480 ## Education 136.022 29.819 4.562 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## Income ~~ ## Education 0.127 0.064 2.000 0.046 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .SAT 382.736 118.115 3.240 0.001 ## Income 2.562 0.791 3.240 0.001 ## Education 0.027 0.008 3.240 0.001 Yes, Education significantly affects SAT ( \\(\\beta = 136.02\\), \\(z = 4.56\\), \\(p &lt; 0.001\\) ), but Income does not ( \\(\\beta = 2.16\\), \\(z = 0.707\\), \\(p = 0.48\\) ). We can also request additional results. For example, by specifying the option rsquare = TRUE in the summary() function, we can see the estimated squared multiple correlations for each dependent variable. 3.4.7 Summarize the model again, and add the rsquare = TRUE argument. What proportion of the variance in SAT is explained by Income and Education? Click for explanation summary(fit, rsquare = TRUE) ## lavaan 0.6-12 ended normally after 19 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 21 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## SAT ~ ## Income 2.156 3.050 0.707 0.480 ## Education 136.022 29.819 4.562 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## Income ~~ ## Education 0.127 0.064 2.000 0.046 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .SAT 382.736 118.115 3.240 0.001 ## Income 2.562 0.791 3.240 0.001 ## Education 0.027 0.008 3.240 0.001 ## ## R-Square: ## Estimate ## SAT 0.603 Income and Education explain 60.34% of the variance in SAT. The semPlot package can automatically generate a path diagram for some types of fitted SEMs (e.g., path models, CFA models). For the purposes of visualizing our results, we will use the semPlot::semPaths() function. 3.4.8 Review the documentation for semPlot::semPaths(). Click for explanation ?semPlot::semPaths 3.4.9 Use the semPaths() function to create a basic path diagram of your fitted model. Click for explanation ## Load the `semPlot` package: library(semPlot) ## Create a basic path diagram: semPaths(fit) We can improve this basic diagram in several ways. For example, we can add the values of the estimates for the paths. We can also rotate the diagram to match the orientation of our theoretical path diagram from above. semPaths(fit, whatLabels = &quot;est&quot;, rotation = 2) End of At-Home Exercises "],["in-class-exercises-2.html", "3.5 In-Class Exercises", " 3.5 In-Class Exercises This week, we will wrap up our re-analysis of the Kestilä (2006) results. During this practical, you will conduct a CFA of the Trust in Politics items and compare the results to those obtained from your previous EFA- and PCA-based replications of Kestilä (2006). 3.5.1 Load the ESS data. The relevant data are contained in the ess_round1.rds file. This file is in R Data Set (RDS) format. The dataset is already stored as a data frame with the processing and cleaning that you should have done for previous practicals completed. Check the documentation for the readRDS() function to see how you can load data stored in an RDS file. Click for explanation ess &lt;- readRDS(&quot;ess_round1.rds&quot;) Although you may have settled on any number of EFA solutions during the Week 2 In-Class Exercises, we are going to base the following CFA on a three-factor model of Trust in Politics similar to the original PCA results from Kastelä (2006). Note: Unless otherwise specified, all following questions refer to the Trust in Politics items. We will not consider the Attitudes toward Immigration items in these exercises. 3.5.2 Define the lavaan model syntax for the CFA implied by the three-factor EFA solution you found in the Week 2 In-Class Exercises. Covary the three latent factors. Do not specify any mean structure. As in 3.4.3, save this model syntax as an object in your environment. Click for explanation We don’t have to specify the latent covariances in the model syntax, we can tell lavaan to estimate all latent covariances when we fit the model. mod_3f &lt;- &#39; politicians =~ pltcare + pltinvt + trstplt satisfaction =~ stfeco + stfgov + stfdem + stfedu + stfhlth institutions =~ trstlgl + trstplc + trstun + trstprl &#39; 3.5.3 Estimate the CFA model you defined above, and summarize the results. Use the lavaan::cfa() function to estimate the model. Use the default settings for the cfa() function. Request the model fit statistics with the summary by supplying the fit.measures = TRUE argument to summary(). Request the standardized parameter estimates with the summary by supplying the standardized = TRUE argument to summary(). Check the results, and answer the following questions: Does the model fit the data well? How are the latent variances and covariances specified when using the default settings? How is the model identified when using the default settings? Click for explanation ## Load the lavaan package: library(lavaan) ## Estimate the CFA model: fit_3f &lt;- cfa(mod_3f, data = ess) ## Summarize the fitted model: summary(fit_3f, fit.measures = TRUE, standardized = TRUE) ## lavaan 0.6-12 ended normally after 45 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 27 ## ## Used Total ## Number of observations 15448 18187 ## ## Model Test User Model: ## ## Test statistic 9188.922 ## Degrees of freedom 51 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 75675.049 ## Degrees of freedom 66 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.879 ## Tucker-Lewis Index (TLI) 0.844 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -357923.209 ## Loglikelihood unrestricted model (H1) -353328.748 ## ## Akaike (AIC) 715900.419 ## Bayesian (BIC) 716106.840 ## Sample-size adjusted Bayesian (BIC) 716021.036 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.108 ## 90 Percent confidence interval - lower 0.106 ## 90 Percent confidence interval - upper 0.110 ## P-value RMSEA &lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.058 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## politicians =~ ## pltcare 1.000 0.671 0.639 ## pltinvt 0.981 0.015 65.449 0.000 0.658 0.624 ## trstplt 2.848 0.035 80.855 0.000 1.911 0.876 ## satisfaction =~ ## stfeco 1.000 1.659 0.712 ## stfgov 1.039 0.013 81.895 0.000 1.724 0.752 ## stfdem 0.979 0.012 80.257 0.000 1.624 0.733 ## stfedu 0.780 0.012 64.344 0.000 1.294 0.576 ## stfhlth 0.706 0.012 58.239 0.000 1.171 0.519 ## institutions =~ ## trstlgl 1.000 1.640 0.688 ## trstplc 0.777 0.012 64.538 0.000 1.275 0.582 ## trstun 0.861 0.013 66.949 0.000 1.411 0.605 ## trstprl 1.123 0.013 85.721 0.000 1.842 0.812 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## politicians ~~ ## satisfaction 0.793 0.016 48.672 0.000 0.712 0.712 ## institutions 0.950 0.018 51.888 0.000 0.863 0.863 ## satisfaction ~~ ## institutions 2.046 0.040 51.736 0.000 0.752 0.752 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .pltcare 0.651 0.008 77.811 0.000 0.651 0.591 ## .pltinvt 0.680 0.009 78.658 0.000 0.680 0.611 ## .trstplt 1.103 0.029 38.414 0.000 1.103 0.232 ## .stfeco 2.671 0.038 70.723 0.000 2.671 0.493 ## .stfgov 2.287 0.035 66.172 0.000 2.287 0.435 ## .stfdem 2.266 0.033 68.444 0.000 2.266 0.462 ## .stfedu 3.378 0.042 79.725 0.000 3.378 0.668 ## .stfhlth 3.721 0.045 81.846 0.000 3.721 0.731 ## .trstlgl 2.997 0.040 74.548 0.000 2.997 0.527 ## .trstplc 3.178 0.040 80.410 0.000 3.178 0.662 ## .trstun 3.443 0.043 79.405 0.000 3.443 0.633 ## .trstprl 1.746 0.030 57.842 0.000 1.746 0.340 ## politicians 0.450 0.011 41.572 0.000 1.000 1.000 ## satisfaction 2.751 0.058 47.286 0.000 1.000 1.000 ## institutions 2.690 0.059 45.613 0.000 1.000 1.000 No, the model does not seem to fit the data well. The SRMR looks good, but one good looking fit statistic is not enough. The RMSEA, TLI, and CFI are all in the “unacceptable” range. The \\(\\chi^2\\) is highly significant, but we don’t care. The cfa() function is just a wrapper for the lavaan() function with several options set at the defaults you would want for a standard CFA. By default: All latent variances and covariances are freely estimated (due to the argument auto.cov.lv.x = TRUE) The model is identified by fixing the first factor loading of each factor to 1 (due to the argument auto.fix.first = TRUE) To see a full list of the (many) options you can specify to tweak the behavior of lavaan estimation functions run ?lavOptions. Now, we will consider a couple of alternative factor structures for the Trust in Politics CFA. First, we will go extremely simple by estimating a one-factor model wherein all Trust items are explained by a single latent variable. 3.5.4 Define the lavaan model syntax for a one-factor model of the Trust items. Save this syntax as an object in your environment. Click for explanation mod_1f &lt;- &#39; political_trust =~ pltcare + pltinvt + trstprl + trstplt + stfeco + stfgov + stfdem + stfedu + stfhlth + trstlgl + trstplc + trstun + trstep &#39; 3.5.5 Estimate the one-factor model, and summarize the results. Does this model appear to fit better or worse than the three-factor model? Notes: We can also conduct a formal significance test for the difference between two \\(\\chi^2\\) values, but we won’t introduce those ideas until Week 5. You can use the lavaan::fitMeasures() function to extract only the model fit information from a fitted lavaan object. Click for explanation ## Estimate the one factor model: fit_1f &lt;- cfa(mod_1f, data = ess) ## Summarize the results: summary(fit_1f, fit.measures = TRUE) ## lavaan 0.6-12 ended normally after 38 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 26 ## ## Used Total ## Number of observations 14778 18187 ## ## Model Test User Model: ## ## Test statistic 17667.304 ## Degrees of freedom 65 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 81699.096 ## Degrees of freedom 78 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.784 ## Tucker-Lewis Index (TLI) 0.741 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -374912.206 ## Loglikelihood unrestricted model (H1) -366078.555 ## ## Akaike (AIC) 749876.413 ## Bayesian (BIC) 750074.036 ## Sample-size adjusted Bayesian (BIC) 749991.410 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.135 ## 90 Percent confidence interval - lower 0.134 ## 90 Percent confidence interval - upper 0.137 ## P-value RMSEA &lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.080 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## political_trust =~ ## pltcare 1.000 ## pltinvt 0.966 0.018 55.075 0.000 ## trstprl 2.986 0.043 70.171 0.000 ## trstplt 2.988 0.042 71.755 0.000 ## stfeco 2.262 0.039 57.544 0.000 ## stfgov 2.489 0.040 62.079 0.000 ## stfdem 2.522 0.039 64.095 0.000 ## stfedu 1.756 0.036 48.642 0.000 ## stfhlth 1.554 0.035 43.930 0.000 ## trstlgl 2.526 0.041 61.195 0.000 ## trstplc 1.956 0.036 54.052 0.000 ## trstun 2.350 0.040 59.017 0.000 ## trstep 2.296 0.038 60.160 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .pltcare 0.743 0.009 81.579 0.000 ## .pltinvt 0.775 0.009 82.043 0.000 ## .trstprl 1.938 0.027 70.877 0.000 ## .trstplt 1.548 0.023 67.052 0.000 ## .stfeco 3.565 0.044 81.289 0.000 ## .stfgov 3.044 0.038 79.326 0.000 ## .stfdem 2.631 0.034 78.072 0.000 ## .stfedu 3.941 0.047 83.419 0.000 ## .stfhlth 4.201 0.050 84.093 0.000 ## .trstlgl 3.370 0.042 79.787 0.000 ## .trstplc 3.410 0.041 82.311 0.000 ## .trstun 3.451 0.043 80.749 0.000 ## .trstep 3.019 0.038 80.272 0.000 ## political_trst 0.360 0.010 36.350 0.000 ## Compare fit statistics: fitMeasures(fit_3f) ## npar fmin chisq df ## 27.000 0.297 9188.922 51.000 ## pvalue baseline.chisq baseline.df baseline.pvalue ## 0.000 75675.049 66.000 0.000 ## cfi tli nnfi rfi ## 0.879 0.844 0.844 0.843 ## nfi pnfi ifi rni ## 0.879 0.679 0.879 0.879 ## logl unrestricted.logl aic bic ## -357923.209 -353328.748 715900.419 716106.840 ## ntotal bic2 rmsea rmsea.ci.lower ## 15448.000 716021.036 0.108 0.106 ## rmsea.ci.upper rmsea.pvalue rmr rmr_nomean ## 0.110 0.000 0.245 0.245 ## srmr srmr_bentler srmr_bentler_nomean crmr ## 0.058 0.058 0.058 0.064 ## crmr_nomean srmr_mplus srmr_mplus_nomean cn_05 ## 0.064 0.058 0.058 116.444 ## cn_01 gfi agfi pgfi ## 131.098 0.905 0.854 0.591 ## mfi ecvi ## 0.744 0.598 fitMeasures(fit_1f) ## npar fmin chisq df ## 26.000 0.598 17667.304 65.000 ## pvalue baseline.chisq baseline.df baseline.pvalue ## 0.000 81699.096 78.000 0.000 ## cfi tli nnfi rfi ## 0.784 0.741 0.741 0.741 ## nfi pnfi ifi rni ## 0.784 0.653 0.784 0.784 ## logl unrestricted.logl aic bic ## -374912.206 -366078.555 749876.413 750074.036 ## ntotal bic2 rmsea rmsea.ci.lower ## 14778.000 749991.410 0.135 0.134 ## rmsea.ci.upper rmsea.pvalue rmr rmr_nomean ## 0.137 0.000 0.364 0.364 ## srmr srmr_bentler srmr_bentler_nomean crmr ## 0.080 0.080 0.080 0.087 ## crmr_nomean srmr_mplus srmr_mplus_nomean cn_05 ## 0.087 0.080 0.080 71.949 ## cn_01 gfi agfi pgfi ## 79.980 0.825 0.756 0.590 ## mfi ecvi ## 0.551 1.199 The one-factor model definitely seems to fit worse than the three-factor model. A second order CFA model is another way of representing the latent structure underlying a set of items. As you read in Byrne (2005), however, the second order CFA is only appropriate in certain circumstances. 3.5.6 Given the CFA results above, would a second order CFA be appropriate for the Trust data? Why or why not? Click for explanation Yes, a second order CFA model is a theoretically appropriate representation of the Trust items. The first order latent variables in the three-factor model are all significantly correlated. The first order latent variables in the three-factor model seem to tap different aspects of some single underlying construct. 3.5.7 Define the lavaan model syntax for a second-order CFA model of the Trust items. Use the three factors defined in 3.5.2 as the first order factors. Click for explanation To define the second order factor, we use the same syntactic conventions that we employ to define a first order factor. The only differences is that the “indicators” of the second order factor (i.e., the variables listed on the RHS of the =~ operator) are previously defined first order latent variables. mod_2nd &lt;- &#39; politicians =~ pltcare + pltinvt + trstplt satisfaction =~ stfeco + stfgov + stfdem + stfedu + stfhlth institutions =~ trstlgl + trstplc + trstun + trstprl trust =~ politicians + satisfaction + institutions &#39; 3.5.8 Estimate the second order CFA model, and summarize the results. Does this model fit better or worse than the three-factor model? Is this model more or less complex than the three-factor model? What information can you use to quantify this difference in complexity? Click for explanation We don’t have to do anything special here. We can estimate and summarize the second order CFA exactly as we did the first order CFA. fit_2nd &lt;- cfa(mod_2nd, data = ess) summary(fit_2nd, fit.measures = TRUE, standardized = TRUE) ## lavaan 0.6-12 ended normally after 45 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 27 ## ## Used Total ## Number of observations 15448 18187 ## ## Model Test User Model: ## ## Test statistic 9188.922 ## Degrees of freedom 51 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 75675.049 ## Degrees of freedom 66 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.879 ## Tucker-Lewis Index (TLI) 0.844 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -357923.209 ## Loglikelihood unrestricted model (H1) -353328.748 ## ## Akaike (AIC) 715900.419 ## Bayesian (BIC) 716106.840 ## Sample-size adjusted Bayesian (BIC) 716021.036 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.108 ## 90 Percent confidence interval - lower 0.106 ## 90 Percent confidence interval - upper 0.110 ## P-value RMSEA &lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.058 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## politicians =~ ## pltcare 1.000 0.671 0.639 ## pltinvt 0.981 0.015 65.449 0.000 0.658 0.624 ## trstplt 2.848 0.035 80.855 0.000 1.911 0.876 ## satisfaction =~ ## stfeco 1.000 1.659 0.712 ## stfgov 1.039 0.013 81.895 0.000 1.724 0.752 ## stfdem 0.979 0.012 80.257 0.000 1.624 0.733 ## stfedu 0.780 0.012 64.344 0.000 1.294 0.576 ## stfhlth 0.706 0.012 58.239 0.000 1.171 0.519 ## institutions =~ ## trstlgl 1.000 1.640 0.688 ## trstplc 0.777 0.012 64.539 0.000 1.275 0.582 ## trstun 0.861 0.013 66.949 0.000 1.411 0.605 ## trstprl 1.123 0.013 85.721 0.000 1.842 0.812 ## trust =~ ## politicians 1.000 0.904 0.904 ## satisfaction 2.153 0.037 57.975 0.000 0.788 0.788 ## institutions 2.580 0.044 59.246 0.000 0.955 0.955 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .pltcare 0.651 0.008 77.811 0.000 0.651 0.591 ## .pltinvt 0.680 0.009 78.658 0.000 0.680 0.611 ## .trstplt 1.103 0.029 38.414 0.000 1.103 0.232 ## .stfeco 2.671 0.038 70.723 0.000 2.671 0.493 ## .stfgov 2.287 0.035 66.172 0.000 2.287 0.435 ## .stfdem 2.266 0.033 68.444 0.000 2.266 0.462 ## .stfedu 3.378 0.042 79.725 0.000 3.378 0.668 ## .stfhlth 3.721 0.045 81.846 0.000 3.721 0.731 ## .trstlgl 2.997 0.040 74.549 0.000 2.997 0.527 ## .trstplc 3.178 0.040 80.410 0.000 3.178 0.662 ## .trstun 3.443 0.043 79.405 0.000 3.443 0.633 ## .trstprl 1.746 0.030 57.842 0.000 1.746 0.340 ## .politicians 0.082 0.004 19.635 0.000 0.182 0.182 ## .satisfaction 1.044 0.029 35.902 0.000 0.379 0.379 ## .institutions 0.238 0.024 10.064 0.000 0.089 0.089 ## trust 0.368 0.010 36.776 0.000 1.000 1.000 ## Compare fit between the first and second order models: fitMeasures(fit_3f) ## npar fmin chisq df ## 27.000 0.297 9188.922 51.000 ## pvalue baseline.chisq baseline.df baseline.pvalue ## 0.000 75675.049 66.000 0.000 ## cfi tli nnfi rfi ## 0.879 0.844 0.844 0.843 ## nfi pnfi ifi rni ## 0.879 0.679 0.879 0.879 ## logl unrestricted.logl aic bic ## -357923.209 -353328.748 715900.419 716106.840 ## ntotal bic2 rmsea rmsea.ci.lower ## 15448.000 716021.036 0.108 0.106 ## rmsea.ci.upper rmsea.pvalue rmr rmr_nomean ## 0.110 0.000 0.245 0.245 ## srmr srmr_bentler srmr_bentler_nomean crmr ## 0.058 0.058 0.058 0.064 ## crmr_nomean srmr_mplus srmr_mplus_nomean cn_05 ## 0.064 0.058 0.058 116.444 ## cn_01 gfi agfi pgfi ## 131.098 0.905 0.854 0.591 ## mfi ecvi ## 0.744 0.598 fitMeasures(fit_2nd) ## npar fmin chisq df ## 27.000 0.297 9188.922 51.000 ## pvalue baseline.chisq baseline.df baseline.pvalue ## 0.000 75675.049 66.000 0.000 ## cfi tli nnfi rfi ## 0.879 0.844 0.844 0.843 ## nfi pnfi ifi rni ## 0.879 0.679 0.879 0.879 ## logl unrestricted.logl aic bic ## -357923.209 -353328.748 715900.419 716106.840 ## ntotal bic2 rmsea rmsea.ci.lower ## 15448.000 716021.036 0.108 0.106 ## rmsea.ci.upper rmsea.pvalue rmr rmr_nomean ## 0.110 0.000 0.245 0.245 ## srmr srmr_bentler srmr_bentler_nomean crmr ## 0.058 0.058 0.058 0.064 ## crmr_nomean srmr_mplus srmr_mplus_nomean cn_05 ## 0.064 0.058 0.058 116.444 ## cn_01 gfi agfi pgfi ## 131.098 0.905 0.854 0.591 ## mfi ecvi ## 0.744 0.598 You should quickly notice something strange about the model fit statistics compared above. If you don’t see it, consider the following: fitMeasures(fit_3f) - fitMeasures(fit_2nd) ## npar fmin chisq df ## 0 0 0 0 ## pvalue baseline.chisq baseline.df baseline.pvalue ## 0 0 0 0 ## cfi tli nnfi rfi ## 0 0 0 0 ## nfi pnfi ifi rni ## 0 0 0 0 ## logl unrestricted.logl aic bic ## 0 0 0 0 ## ntotal bic2 rmsea rmsea.ci.lower ## 0 0 0 0 ## rmsea.ci.upper rmsea.pvalue rmr rmr_nomean ## 0 0 0 0 ## srmr srmr_bentler srmr_bentler_nomean crmr ## 0 0 0 0 ## crmr_nomean srmr_mplus srmr_mplus_nomean cn_05 ## 0 0 0 0 ## cn_01 gfi agfi pgfi ## 0 0 0 0 ## mfi ecvi ## 0 0 The two models produce identical fit statistics! We also see that the degrees of freedom are identical between the two models. Hence, the two models have equal complexity. This result taps into a critical idea in statistical modeling, namely, model equivalency. It turns out the two models we’re comparing here are equivalent in the sense that they are statistically indistinguishable representations of the data. Since this is a very important idea, I want to spend some time discussing it in person. So, spend some time between now and the next lecture session thinking about the implications of this model equivalence. Specifically, consider the following questions: What do we mean when we say that these two models are equivalent? How is it possible for these two models to be equivalent when one contains an additional latent variable? Why are the degrees of freedom equal for these two models? Why are the fit statistics equal for these two models? How can we go about picking the “better” of these two models? We’ll take some time to discuss these ideas in the next lecture session. The above represents the extent to which we will use CFA to replicate the EFA/PCA components of the Kastelä (2006) analysis. We will now use the latent variables defined above to revisit the regression analysis from Kastelä (2006). Note that we will now be specifying regression models involving latent variables, so this is actually your first taste of full structural equation modeling (SEM). Enjoy! Unfortunately, we need to do a little data processing before we can fit the regression model. At the moment, lavaan will not automatically convert a factor variable into dummy codes. So, we need to create explicit dummy codes for the two factors we’ll use as predictors in our regression analysis: sex and political orientation. 3.5.9 Convert the sex and political interest factors into dummy codes. Click for explanation In R, we have several ways of converting a factor into an appropriate set of dummy codes. We could use the dplyr::recode() function as we did last week. We can use the model.matrix() function to define a design matrix based on the inherent contrast attribute of the factor. Missing data will cause problems here. We can us as.numeric() to revert the factor to its underlying numeric representation {Male = 1, Female = 2} and use arithmetic to convert {1, 2} \\(\\rightarrow\\) {0, 1}. When our factor only has two levels, though, the ifelse() function is the simplest way. ## Create a dummy code by broadcasting a logical test on the factor levels: ess$female &lt;- ifelse(ess$sex == &quot;Female&quot;, 1, 0) ## The same for political orientation: ess$hi_pol_interest &lt;- ifelse(ess$polintr_bin == &quot;High Interest&quot;, 1, 0) ## Check the results: with(ess, table(dummy = female, factor = sex)) ## factor ## dummy Female Male ## 0 0 8841 ## 1 9309 0 with(ess, table(dummy = hi_pol_interest, factor = polintr_bin)) ## factor ## dummy Low Interest High Interest ## 0 8099 0 ## 1 0 10042 3.5.10 Finally, subset the data to only Finnish participants. Click for explanation ess_fin &lt;- ess[ess$cntry == &quot;Finland&quot;, ] We are now ready to estimate our latent regression model. Specifically, we want to implement the following regression as an SEM in lavaan. \\[ Y_{trust\\_inst} = \\beta_1 X_{age} + \\beta_2 X_{female} + \\beta_3 X_{edu\\_years} + \\beta_4 X_{hi\\_pol\\_interest} + \\beta_5 X_{lrscale} + \\varepsilon_Y \\] 3.5.11 Define the lavaan model syntax for the regression shown above. Use the definition of the institutions factor from 3.5.2 to define the DV. Hint: You can simply copy the line of syntax that defines the latent factor and add a line to define the latent regression model. Click for explanation mod_sem &lt;- &#39; ## Define the latent DV: institutions =~ trstlgl + trstplc + trstun ## Specify the structural relations: institutions ~ female + age + eduyrs + hi_pol_interest + lrscale &#39; 3.5.12 Estimate the SEM, and summarize the results. Fit the model to the processed Finnish subsample from above. Estimate the model using lavaan::sem(). Use the default settings in the sem() function. Request the standardized parameter estimates with the summary. Request the \\(R^2\\) estimates with the summary by supplying the rsquare = TRUE argument to summary(). Click for explanation ## Fit the SEM: fit_sem &lt;- sem(mod_sem, data = ess_fin) ## Summarize the results: summary(fit_sem, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) ## lavaan 0.6-12 ended normally after 43 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 11 ## ## Used Total ## Number of observations 1835 2000 ## ## Model Test User Model: ## ## Test statistic 101.932 ## Degrees of freedom 10 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 1221.668 ## Degrees of freedom 18 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.924 ## Tucker-Lewis Index (TLI) 0.863 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -10940.642 ## Loglikelihood unrestricted model (H1) -10889.676 ## ## Akaike (AIC) 21903.284 ## Bayesian (BIC) 21963.947 ## Sample-size adjusted Bayesian (BIC) 21929.001 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.071 ## 90 Percent confidence interval - lower 0.059 ## 90 Percent confidence interval - upper 0.084 ## P-value RMSEA &lt;= 0.05 0.003 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.030 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## institutions =~ ## trstlgl 1.000 1.783 0.835 ## trstplc 0.615 0.035 17.401 0.000 1.097 0.640 ## trstun 0.603 0.037 16.355 0.000 1.076 0.528 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## institutions ~ ## female -0.025 0.094 -0.264 0.792 -0.014 -0.007 ## age -0.002 0.003 -0.534 0.594 -0.001 -0.015 ## eduyrs 0.072 0.013 5.405 0.000 0.041 0.157 ## hi_pol_interst 0.178 0.096 1.850 0.064 0.100 0.050 ## lrscale 0.110 0.023 4.727 0.000 0.062 0.125 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .trstlgl 1.379 0.168 8.199 0.000 1.379 0.302 ## .trstplc 1.739 0.084 20.599 0.000 1.739 0.591 ## .trstun 3.000 0.116 25.792 0.000 3.000 0.722 ## .institutions 3.019 0.209 14.451 0.000 0.949 0.949 ## ## R-Square: ## Estimate ## trstlgl 0.698 ## trstplc 0.409 ## trstun 0.278 ## institutions 0.051 3.5.13 Finally, we will rerun the latent regression model from above as a path model with the factor score for Trust in Institutions as the DV. Add the Trust in Institutions factor score that you estimated in 2.5.10 to the ESS data. If you did not save the factors scores last week, you’ll need to rerun the relevant EFA. Don’t forget to subset the data to the Finish participants before fitting the path model. Rerun the above SEM with the EFA-derived Trust in Institutions factor score taking the place of the analagous latent variable as DV. Request the standardized parameter estimates with the summary. Request the \\(R^2\\) estimates with the summary. Click for explanation First, we’ll quickly reproduce the Trust in Institutions factor score that we estimated last week. Note that psych::fa() returns the factor scores in a different order than it did for the Week 2 analyses. ## Load the psych library: library(psych) ## Rerun the three-factor EFA from last week: fit_efa &lt;- fa(ess[7:19], nfactors = 3, rotate = &quot;promax&quot;, scores = &quot;Bartlett&quot;) ## View the factor loadings: print(fit_efa$loadings, cut = 0.3) ## ## Loadings: ## MR3 MR1 MR2 ## pltcare 0.815 ## pltinvt 0.806 ## trstprl 0.405 0.381 ## trstlgl 0.843 ## trstplc 0.776 ## trstplt 0.571 ## trstep 0.503 ## trstun 0.543 ## stfeco 0.703 ## stfgov 0.594 ## stfdem 0.462 ## stfedu 0.701 ## stfhlth 0.664 ## ## MR3 MR1 MR2 ## SS loadings 2.178 2.101 2.023 ## Proportion Var 0.168 0.162 0.156 ## Cumulative Var 0.168 0.329 0.485 ## Reproduce the factor score from last week: ess$trust_inst_efa &lt;- fit_efa$scores[ , 1] ## Subset the data again: ess_fin &lt;- ess[ess$cntry == &quot;Finland&quot;, ] Now, we’ll rerun our regression as a path analysis with the EFA-derived factor score as DV. ## Define the model syntax for the path analysis: mod_pa &lt;- &#39;trust_inst_efa ~ female + age + eduyrs + hi_pol_interest + lrscale&#39; ## Estimate the path model: fit_pa &lt;- sem(mod_pa, data = ess_fin) ## Summarize the results: summary(fit_pa, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) ## lavaan 0.6-12 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Used Total ## Number of observations 1740 2000 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Model Test Baseline Model: ## ## Test statistic 71.266 ## Degrees of freedom 5 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -2352.608 ## Loglikelihood unrestricted model (H1) -2352.608 ## ## Akaike (AIC) 4717.216 ## Bayesian (BIC) 4749.985 ## Sample-size adjusted Bayesian (BIC) 4730.924 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## P-value RMSEA &lt;= 0.05 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## trust_inst_efa ~ ## female 0.002 0.045 0.036 0.971 0.002 0.001 ## age -0.003 0.001 -1.977 0.048 -0.003 -0.051 ## eduyrs 0.024 0.006 3.820 0.000 0.024 0.099 ## hi_pol_interst 0.154 0.047 3.313 0.001 0.154 0.081 ## lrscale 0.058 0.011 5.159 0.000 0.058 0.122 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .trust_inst_efa 0.875 0.030 29.496 0.000 0.875 0.960 ## ## R-Square: ## Estimate ## trust_inst_efa 0.040 3.5.14 Compare the results from the path analysis to the SEM-based results. Does it matter whether we use a latent variable or a factor score to define the DV? Hint: When comparing parameter estimates, use the fully standardized estimates (i.e., the values in the column labeled Std.all). Click for explanation First, we’ll source a script that defines a bunch of convenience functions. One of these functions, partSummary(), will allow us to print only the interesting pieces of the model summary. ## Source a script of convenience function definitions: source(&quot;supportFunctions.R&quot;) ## The partSummary() function requires the dplyr package: library(dplyr) Now, we’ll compare the results. Specifically, we’re interested in differences in the regression coefficients and the \\(R^2\\). ## View the regression estimates from the SEM: partSummary(fit_sem, 8, standardized = TRUE) ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## institutions ~ ## female -0.025 0.094 -0.264 0.792 -0.014 -0.007 ## age -0.002 0.003 -0.534 0.594 -0.001 -0.015 ## eduyrs 0.072 0.013 5.405 0.000 0.041 0.157 ## hi_pol_interst 0.178 0.096 1.850 0.064 0.100 0.050 ## lrscale 0.110 0.023 4.727 0.000 0.062 0.125 ## View the regression estimates from the path analysis: partSummary(fit_pa, 7, standardized = TRUE) ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## trust_inst_efa ~ ## female 0.002 0.045 0.036 0.971 0.002 0.001 ## age -0.003 0.001 -1.977 0.048 -0.003 -0.051 ## eduyrs 0.024 0.006 3.820 0.000 0.024 0.099 ## hi_pol_interst 0.154 0.047 3.313 0.001 0.154 0.081 ## lrscale 0.058 0.011 5.159 0.000 0.058 0.122 ## View the R-squared estimates from the SEM: partSummary(fit_sem, 10, rsquare = TRUE) ## R-Square: ## Estimate ## trstlgl 0.698 ## trstplc 0.409 ## trstun 0.278 ## institutions 0.051 ## View the R-squared estimate from the path analysis: partSummary(fit_pa, 9, rsquare = TRUE) ## R-Square: ## Estimate ## trust_inst_efa 0.040 It certainly looks like the way we define the DV has a meaningful impact. The patterns of significance differ between the two sets of regression slopes, and the \\(R^2\\) is 26.7% larger in the SEM. End of In-Class Exercises "],["mediation.html", "4 Mediation", " 4 Mediation Homework before the lecture Watch the Lecture Recording for this week. Complete the Reading for this week, and answer the associated reading questions. Complete the Formative Assessment for this week. Lecture content In this lecture, we will discuss methods of modeling a particularly useful type of causal process: mediation. We will use this discussion to introduce the ideas path analysis. Homework before the practical Complete the At-Home Exercises. Practical content During the practical you will work on the In-Class Exercises. "],["lecture-3.html", "4.1 Lecture", " 4.1 Lecture Download slides Note that these are more recent versions of the slides than used in the video. Researchers often have theories about possible causal processes linking multiple variables. Mediation is a particularly important example of such a process in which in an input variable, X, influences the outcome, Y, through an intermediary variable, M (the mediator). For instance, psychotherapy (X), may affect thoughts (M), which in turn affects mood (Y). We can investigate mediation via a specific sequence of linear regression equations, but path modeling will make our lives much easier. We can use path models to simultaneously estimate multiple related regression equations. So, mediation analysis is an ideal application of path modeling. In this lecture, we consider both approaches and discuss their relative strengths and weaknesses. "],["reading-3.html", "4.2 Reading", " 4.2 Reading Reference Baron, R. M. &amp; Kenny, D. A. (1986). The moderator-mediator variable distinction in social psychological research: Conceptual, strategic, and statistical Considerations. Journal of Personality and Individual Differences, 51(6), 1173–1182 Note: We will use this reading again next week. This week, we are only considering mediation, so you only need to read the introduction section (p.  1173) and the Nature of Mediator Variables section (pp. 1176–1177). Next week, we will discuss moderation in more detail. So, you will read the remaining sections of this paper as part of next week’s reading assignment. Questions What is mediation? Give an example of mediation. According to the authors, we must satisfy four criteria to infer mediation. What are these criteria? Answers Mediation is a way of describing the relations among three, or more, variables. Specifically, mediation entails a causal process wherein X affects Y indirectly through M. With a hypothesis of mediation, we ask how or why does X affect Y? Amount of study (X) affects exam score (Y) through mastery of the course material (M). Physical exercise (X) affects mood (Y) through neurotransmitter levels (M). Political events (X) affect stock prices (Y) through investor hopes/fears (M). According to the authors, we can only infer mediation after satisfying the following criteria. X significantly predicts Y. X significantly predicts M. M significantly predicts Y. The size of the effect of X on Y decreases after controlling for M. Reference Hayes, A. F. (2009). Beyond Baron and Kenny: Statistical mediation analysis in the new millennium. Communication Monographs, 76(4), 408–420. Questions What is an indirect or mediated effect? What is the difference between the total and direct effect? What is the main problem with the Barron &amp; Kenny “Causal Steps Approach”? What is bootstrapping, and why is it a better way to test mediation than Sobel’s test? Explain how it is possible that “effects that don’t exist can be mediated”. Answers The indirect effect (IE) is the part of the effect of X on Y that is transmitted through M. We compute the IE by multiplying the effect of X on M (a) and the effect of M on Y (b): \\(IE = ab\\). The total effect is the effect of X on Y without accounting for M. The direct effect is the effect of X on Y after controlling for M. The B&amp;K approach infers mediation based on a sequence of significance tests, and this repeated testing lowers power. Moreover, if no significant total effect is found between X and Y, researchers applying B&amp;K logic must infer the absence of any indirect effect, but a nonsignificant total effect does not imply a lack of mediation. Bootstrapping constructs an empirical sampling distribution for a parameter by resampling rows from the original data many (e.g., 1000) times, and conducting the same analysis on each of these samples. The distribution of these parameter estimates represents an empirical sampling distribution for the parameter. This sampling distribution is then used to support inference (e.g., by deriving confidence intervals or standard errors). Sobel’s test assumes the IE is normally distributed, which cannot be true in finite samples. Bootstrapping is a nonparametric technique that directly estimates the sampling distribution of the IE rather than assuming some parametric form. The following passage from the reading (p. 414) explains: That X can exert an indirect effect on Y through M in the absence of an association between X and Y becomes explicable once you consider that a total effect is the sum of many different paths of influence, direct and indirect, not all of which may be a part of the formal model. For example, it could be that two or more indirect paths carry the effect from X through Y, and those paths operate in opposite directions (cf., MacKinnon, Krull, &amp; Lockwood, 2000). Much as a main effect in 2 \\(\\times\\) 2 ANOVA might be nonsignificant if the simple effects are opposite in sign (i.e., a crossover interaction), two or more indirect effects with opposite signs can cancel each other out, producing a total effect and perhaps even a total indirect effect that is not detectably different from zero, in spite of the existence of specific indirect effects that are not zero. "],["formative-assessment-3.html", "4.3 Formative Assessment", " 4.3 Formative Assessment A formative assessment helps you judge your progress in the course and helps you address any blind spots in your understanding of the material. This assessment does not contribute to your course grade. If you get a question wrong, you will receive a hint on how to improve your understanding of the material. Ideally, you should complete this assessment after you have watched the lecture recording, but before the lecture meeting. This way, if you discover gaps in your understanding, we can discuss any topics that need more attention during the lecture meeting. Question 1: Match the name to the definition: 1) total effect, 2) direct effect, 3) indirect effect. 1) The sum of the direct effect and any indirect effects linking X and Y2) The partial bivariate regression coefficient between X and Y3) The product of any regression coefficients between variables mediating the link between X and Y 1) The partial bivariate regression coefficient between X and Y2) The sum of the direct effect and any indirect effects linking X and Y3) The product of any regression coefficients between variables mediating the link between X and Y 1) The sum of the direct effect and any indirect effects linking X and Y2) The product of any regression coefficients between variables mediating the link between X and Y3) The partial bivariate regression coefficient between X and Y Question 2: A model is called “fully mediated” when the direct effect between the independent and dependent variable is non-significant. FALSE TRUE Question 3: When are model A and B nested? If both models use exactly the same covariance matrix If both models have an equal number of DF If both model use the same dependent variables If one model is obtained by constraining some parameters of the other model Question 4: Bootstrapping… …empirically derives the sampling distribution by randomly sampling from the data many times, and calculating the bootstrapped parameters in each sample. …better estimates the population value of parameters by randomly sampling from the data many times, and calculating the bootstrapped parameters in each sample. …empirically derives the sampling distribution by randomly shuffling the data many times, and calculating the bootstrapped parameters in each sample. …better estimates the population value of parameters by randomly sampling from the data many times, and calculating the bootstrapped parameters in each sample. Question 5: In order to for mediation to exist, there must also exist a total effect between X and Y. TRUE FALSE Question 6: In comparing nested models in SEM, one chooses the model with the largest p-value for the Chi-square test of model fit. TRUE FALSE Question 7: A confounder is a special case of a mediator. FALSE TRUE "],["at-home-exercises-3.html", "4.4 At-Home Exercises", " 4.4 At-Home Exercises In this practical, we will analyze the data contained in SelfEsteem.sav. These data comprise 143 observations of the following variables. case: Participant ID number ParAtt: Parental Attachment PeerAtt: Peer Attachment Emp: Empathy ProSoc: Prosocial behavior Aggr: Aggression SelfEst: Self-esteem 4.4.1 Load the SelfEsteem.sav data. Note: Unless otherwise specified, all following analyses apply to these data. Click for explanation library(foreign) seData&lt;- read.spss(&quot;SelfEsteem.sav&quot;, to.data.frame = TRUE) For this analysis, we are interested in the (indirect) effects of parental and peer attachment on self-esteem. Furthermore, we want to evaluate the mediating roles of empathy and social behavior (i.e., prosocial behavior and aggression). Specifically, we have the following hypotheses. Better peer relationships will promote higher self-esteem via a three-step indirect process. Better peer relationships will increase empathy levels. Higher empathy will increase prosocial behavior and decrease aggressive behavior. More prosocial behaviors and less aggressive behavior will both produce higher self-esteem. Better relationships with parents directly increase self-esteem. To evaluate these hypotheses, we will use lavaan to estimate the following multiple mediator model as a path model. 4.4.2 Specify the lavaan model syntax implied by the path diagram shown above. Save the resulting character string as an object in your environment. Click for explanation mod &lt;- &#39; ## Equation for outcome: SelfEst ~ ProSoc + Aggr + Emp + ParAtt + PeerAtt ## Equations for stage 2 mediators: ProSoc ~ PeerAtt + ParAtt + Emp Aggr ~ PeerAtt + ParAtt + Emp ## Equation for stage 1 mediator: Emp ~ ParAtt + PeerAtt ## Covariances: ProSoc ~~ Aggr ParAtt ~~ PeerAtt &#39; 4.4.3 Use the lavaan::sem() function to estimate the model defined in 4.4.2. Use the default settings in sem(). Summarize the fitted model. Do you notice anything special about the model fit? If so, can you explain? Click for explanation library(lavaan) out &lt;- sem(mod, data = seData) summary(out, fit.measures = TRUE) ## lavaan 0.6-12 ended normally after 16 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 21 ## ## Number of observations 143 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Model Test Baseline Model: ## ## Test statistic 174.959 ## Degrees of freedom 15 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1125.399 ## Loglikelihood unrestricted model (H1) -1125.399 ## ## Akaike (AIC) 2292.797 ## Bayesian (BIC) 2355.017 ## Sample-size adjusted Bayesian (BIC) 2288.570 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## P-value RMSEA &lt;= 0.05 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## SelfEst ~ ## ProSoc 0.252 0.096 2.634 0.008 ## Aggr 0.185 0.085 2.172 0.030 ## Emp 0.143 0.098 1.460 0.144 ## ParAtt 0.244 0.078 3.133 0.002 ## PeerAtt 0.051 0.091 0.555 0.579 ## ProSoc ~ ## PeerAtt -0.037 0.080 -0.469 0.639 ## ParAtt 0.193 0.067 2.886 0.004 ## Emp 0.477 0.074 6.411 0.000 ## Aggr ~ ## PeerAtt -0.095 0.090 -1.055 0.291 ## ParAtt -0.034 0.075 -0.454 0.650 ## Emp -0.309 0.084 -3.697 0.000 ## Emp ~ ## ParAtt 0.078 0.075 1.045 0.296 ## PeerAtt 0.306 0.086 3.557 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .ProSoc ~~ ## .Aggr -0.086 0.058 -1.476 0.140 ## ParAtt ~~ ## PeerAtt 0.537 0.103 5.215 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .SelfEst 0.796 0.094 8.456 0.000 ## .ProSoc 0.618 0.073 8.456 0.000 ## .Aggr 0.777 0.092 8.456 0.000 ## .Emp 0.779 0.092 8.456 0.000 ## ParAtt 1.277 0.151 8.456 0.000 ## PeerAtt 0.963 0.114 8.456 0.000 The model fits the data perfectly because the model is saturated (i.e., we have estimated all possible paths). So, the model-implied covariance matrix exactly replicates the observed covariance matrix. We can tell that the model is saturated because the degrees of freedom for the \\(\\chi^2\\) statistic are 0. 4.4.4 Considering the parameter estimates from 4.4.3, what can you say about the hypotheses? Click for explanation Notice that all of the hypotheses stated above are explicitly directional. Hence, when evaluating the significance of the structural paths that speak to these hypotheses, we should use one-tailed tests. We cannot ask lavaan to return one-tailed p-values, but we have no need to do so. We can simply divide the two-tailed p-values in half. The significant direct effect of ParAtt on SelfEst (\\(\\beta = 0.244\\), \\(Z = 3.13\\), \\(p = 0.001\\)) and the lack of a significant direct effect of PeerAtt on SelfEst (\\(\\beta = 0.051\\), \\(Z = 0.555\\), \\(p = 0.29\\)) align with our hypotheses. The remaining patterns of individual estimates also seem to conform with the hypotheses (e.g., all of the individual paths comprising the indirect effects of PeerAtt on SelfEst are significant). We cannot make any firm conclusions until we actually estimate and test the indirect effects, though. Remember that an indirect effect (IE) is the product of multiple regression slopes. Therefore, to estimate an IE, we must define these products in our model syntax. In lavaan, we define the new IE parameters in two steps. Label the relevant regression paths. Use the labels to define new parameters that represent the desired IEs. We can define new parameters in lavaan model syntax via the := operator. The lavaan website contains a tutorial on this procedure: http://lavaan.ugent.be/tutorial/mediation.html 4.4.5 Use the procedure described above to modify the model syntax from 4.4.2 by adding definitions of the two hypothesized IEs from PeerAtt to SelfEst. Click for explanation You can use any labeling scheme that makes sense to you, but I recommend adopting some kind of systematic rule. Here, I will label the individual estimates in terms of the short variable names used in the path diagram above. Notice that I only label the parameters that I will use to define the IEs. You are free to label any parameter that you like, but I choose the to label only the minimally sufficient set to avoid cluttering the code/output. mod &lt;- &#39; ## Equation for outcome: SelfEst ~ y_m21 * ProSoc + y_m22 * Aggr + Emp + ParAtt + PeerAtt ## Equations for stage 2 mediators: ProSoc ~ m21_x2 * PeerAtt + ParAtt + m21_m1 * Emp Aggr ~ m22_x2 * PeerAtt + ParAtt + m22_m1 * Emp ## Equation for stage 1 mediator: Emp ~ ParAtt + m1_x2 * PeerAtt ## Covariances: ProSoc ~~ Aggr ParAtt ~~ PeerAtt ## Indirect effects: ie_pro := m1_x2 * m21_m1 * y_m21 ie_agg := m1_x2 * m22_m1 * y_m22 &#39; 4.4.6 Use lavaan::sem() to estimate the model with the IEs defined. Use the default settings for sem(). Are the hypothesized IEs significant according to the default tests? Click for explanation out &lt;- sem(mod, data = seData) summary(out) ## lavaan 0.6-12 ended normally after 16 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 21 ## ## Number of observations 143 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## SelfEst ~ ## ProSoc (y_21) 0.252 0.096 2.634 0.008 ## Aggr (y_22) 0.185 0.085 2.172 0.030 ## Emp 0.143 0.098 1.460 0.144 ## ParAtt 0.244 0.078 3.133 0.002 ## PerAtt 0.051 0.091 0.555 0.579 ## ProSoc ~ ## PerAtt (m21_2) -0.037 0.080 -0.469 0.639 ## ParAtt 0.193 0.067 2.886 0.004 ## Emp (m21_1) 0.477 0.074 6.411 0.000 ## Aggr ~ ## PerAtt (m22_2) -0.095 0.090 -1.055 0.291 ## ParAtt -0.034 0.075 -0.454 0.650 ## Emp (m22_1) -0.309 0.084 -3.697 0.000 ## Emp ~ ## ParAtt 0.078 0.075 1.045 0.296 ## PerAtt (m1_2) 0.306 0.086 3.557 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .ProSoc ~~ ## .Aggr -0.086 0.058 -1.476 0.140 ## ParAtt ~~ ## PeerAtt 0.537 0.103 5.215 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .SelfEst 0.796 0.094 8.456 0.000 ## .ProSoc 0.618 0.073 8.456 0.000 ## .Aggr 0.777 0.092 8.456 0.000 ## .Emp 0.779 0.092 8.456 0.000 ## ParAtt 1.277 0.151 8.456 0.000 ## PeerAtt 0.963 0.114 8.456 0.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## ie_pro 0.037 0.018 2.010 0.044 ## ie_agg -0.017 0.011 -1.657 0.098 The IE of Peer Attachment on Self Esteem through Empathy and Prosocial Behavior is significant (\\(\\hat{\\textit{IE}} = 0.037\\), \\(Z = 2.01\\), \\(p = 0.022\\)), as is the analogous IE through Aggressive Behavior (\\(\\hat{\\textit{IE}} = -0.017\\), \\(Z = -1.66\\), \\(p = 0.049\\)). The tests we used to evaluate the significance of the IEs in 4.4.6 are flawed because they assume normal sampling distributions for the IEs. However, the IEs are defined as products of multiple, normally distributed, regression slopes. So, the IEs themselves cannot be normally distributed (at least in finite samples), and the results of the normal-theory significance tests may be misleading. To get an accurate test of the IEs, we should use bootstrapping to generate an empirical sampling distribution for each IE. In lavaan, we implement bootstrapping by specifying the se = \"bootstrap\" option in the fitting function (i.e., the cfa() or sem() function) and specifying the number of bootstrap samples via the bootstrap option. Workflow Tip To draw reliable conclusions from bootstrapped results, we need many bootstrap samples (i.e., B &gt; 1000), but we must estimate the full model for each of these samples, so the estimation can take a long time. To avoid too much frustration, you should first estimate the model without bootstrapping to make sure everything is specified correctly. Only after you are certain that your code is correct do you want to run the full bootstrapped version. 4.4.7 Re-estimate the model from 4.4.6 using 1000 bootstrap samples. Other than the se and bootstrap options, use the defaults. Are the hypothesized IEs significant according to the bootstrap-based test statistics? Click for explanation ## Set a seed to get replicable bootstrap samples: set.seed(235711) ## Estimate the model with bootstrapping: out_boot &lt;- sem(mod, data = seData, se = &quot;bootstrap&quot;, bootstrap = 1000) ## Summarize the model: summary(out_boot) ## lavaan 0.6-12 ended normally after 16 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 21 ## ## Number of observations 143 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Bootstrap ## Number of requested bootstrap draws 1000 ## Number of successful bootstrap draws 1000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## SelfEst ~ ## ProSoc (y_21) 0.252 0.098 2.566 0.010 ## Aggr (y_22) 0.185 0.085 2.175 0.030 ## Emp 0.143 0.098 1.465 0.143 ## ParAtt 0.244 0.084 2.925 0.003 ## PerAtt 0.051 0.096 0.526 0.599 ## ProSoc ~ ## PerAtt (m21_2) -0.037 0.078 -0.482 0.630 ## ParAtt 0.193 0.070 2.740 0.006 ## Emp (m21_1) 0.477 0.080 5.995 0.000 ## Aggr ~ ## PerAtt (m22_2) -0.095 0.086 -1.099 0.272 ## ParAtt -0.034 0.078 -0.438 0.662 ## Emp (m22_1) -0.309 0.090 -3.449 0.001 ## Emp ~ ## ParAtt 0.078 0.068 1.147 0.251 ## PerAtt (m1_2) 0.306 0.077 3.968 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .ProSoc ~~ ## .Aggr -0.086 0.056 -1.527 0.127 ## ParAtt ~~ ## PeerAtt 0.537 0.124 4.318 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .SelfEst 0.796 0.083 9.587 0.000 ## .ProSoc 0.618 0.068 9.044 0.000 ## .Aggr 0.777 0.104 7.488 0.000 ## .Emp 0.779 0.091 8.554 0.000 ## ParAtt 1.277 0.200 6.399 0.000 ## PeerAtt 0.963 0.108 8.953 0.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## ie_pro 0.037 0.019 1.931 0.054 ## ie_agg -0.017 0.010 -1.678 0.093 As with the normal-theory tests, both of the hypothesized IEs of Peer Attachment on Self Esteem are significant (Prosocial Behavior: \\(\\hat{\\textit{IE}} = 0.037\\), \\(Z = 1.93\\), \\(p = 0.027\\) ; Aggressive Behavior: \\(\\hat{\\textit{IE}} = -0.017\\), \\(Z = -1.68\\), \\(p = 0.047\\)). When you use the summary() function to summarize the bootstrapped model from 4.4.7, the output will probably look pretty much the same as it did in 4.4.6, but it’s not. The standard errors and test statistics in the bootstrapped summary are derived from empirical sampling distributions, whereas these values are based on an assumed normal sampling distribution in 4.4.6. The standard method of testing IEs with bootstrapping is to compute confidence intervals (CIs) from the empirical sampling distribution of the IEs. In lavaan, we can compute basic (percentile, 95%) CIs by adding the ci = TRUE option to the summary() function. To evaluate our directional hypotheses at an \\(\\alpha = 0.05\\) level, however, we need to compute 90% CIs. We can get more control over the summary statistics (include the CIs) with the parameterEstimates() function. 4.4.8 Check the documentation for lavaan::parameterEstimates(). Click for explanation ?parameterEstimates 4.4.9 Use the parameterEstimates() function to compute bootstrapped CIs for the hypothesized IEs. Compute percentile CIs. Are the IEs significant according to the bootstrapped CIs? Click for explanation parameterEstimates(out_boot, ci = TRUE, level = 0.9) When evaluating a directional hypothesis with a CI, we only consider one of the interval’s boundaries. For a hypothesized positive effect, we check only if the lower boundary is greater than zero. For a hypothesized negative effect, we check if the upper boundary is less than zero. As with the previous tests, the IE of Peer Attachment on Self Esteem through Empathy and Prosocial Behavior is significant (\\(\\hat{\\textit{IE}} = 0.037\\), \\(95\\% ~ CI = [0.009; \\infty]\\)), as is the analogous IE through Aggressive Behavior (\\(\\hat{\\textit{IE}} = -0.017\\), \\(95\\% ~ CI = [-\\infty; -0.003]\\)). 4.4.10 Based on the analyses you’ve conducted here, what do you conclude vis-à-vis the original hypotheses? Click for explanation The hypothesized indirect effects between Peer Attachment and Self Esteem were supported in that the IE through Empathy and Prosocial Behavior as well as the IE through Empathy and Aggressive Behavior were both significant. The hypothesized direct effect of Parent Attachment on Self Esteem was also born out via a significant direct effect in the model. These results may not tell the whole story, though. We have not tested for indirect effects between Parent Attachment and Self Esteem, and we have not evaluated simpler indirect effects between Peer Attachment and Self Esteem (e.g., PeerAtt \\(\\rightarrow\\) Emp \\(\\rightarrow\\) SelfEst). We’ll consider these points during the In-Class Exercises. End of At-Home Exercises "],["in-class-exercises-3.html", "4.5 In-Class Exercises", " 4.5 In-Class Exercises In This practical, we will continue with our mediation analysis of the data provided by the SelfEsteem.sav file. 4.5.1 Load the SelfEsteem.sav data. Click for explanation library(foreign) seData &lt;- read.spss(&quot;SelfEsteem.sav&quot;, to.data.frame = TRUE) To begin this practical, recall the path model that we estimated for the At-Home Exercises. Figure 4.1: Full Model Further, recall the research questions for our analysis. Better peer relationships will promote higher self-esteem via a three-step indirect process. Better peer relationships will increase empathy levels. Higher empathy will increase prosocial behavior and decrease aggressive behavior. More prosocial behaviors and less aggressive behavior will both produce higher self-esteem. Better relationships with parents directly increase self-esteem. Clearly, we have estimated more paths than we need to directly assess the hypotheses. We could still calculate the relevant direct/indirect/total effects from a simpler model. 4.5.2 Draw a path diagram for the simplest model that you could use to define the effects implied by the above hypotheses. For the purposes of testing the final hypothesis, you may treat direct and total effects as synonymous. Keep the covariances between ParAtt and PeerAtt and between ProSoc and Aggr in the model. Click for explanation Figure 4.2: Restricted Model 4.5.3 Define the lavaan model syntax for the path model you diagrammed in 4.5.2. Label any relevant paths Specify any relevant indirect effects as defined parameters. Save the syntax string as an object in your environment. Click for explanation resMod &lt;- &#39; ## Equation for outcome: SelfEst ~ y_m21 * ProSoc + y_m22 * Aggr + ParAtt ## Equations for stage 2 mediators: ProSoc ~ m21_m1 * Emp Aggr ~ m22_m1 * Emp ## Equation for stage 1 mediator: Emp ~ m1_x2 * PeerAtt ## Covariances: ProSoc ~~ Aggr ParAtt ~~ PeerAtt ## Indirect effects: ie_pro := m1_x2 * m21_m1 * y_m21 ie_agg := m1_x2 * m22_m1 * y_m22 &#39; 4.5.4 Estimate the model defined in 4.5.3 using 1000 bootstrap samples. Other than the se and bootstrap options, use the defaults. How does the model fit? Is the hypothesized total/direct effect from ParAtt to SelfEst significant? Are the hypothesized IEs significant according to the one-tailed 95% bootstrap CIs? Click for explanation ## Set a seed to get replicable bootstrap samples: set.seed(235711) ## Estimate the restricted model with bootstrapping: out_res &lt;- sem(resMod, data = seData, se = &quot;bootstrap&quot;, bootstrap = 1000) ## Summarize the model: summary(out_res, fit.measures = TRUE) ## lavaan 0.6-12 ended normally after 15 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 14 ## ## Number of observations 143 ## ## Model Test User Model: ## ## Test statistic 14.561 ## Degrees of freedom 7 ## P-value (Chi-square) 0.042 ## ## Model Test Baseline Model: ## ## Test statistic 174.959 ## Degrees of freedom 15 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.953 ## Tucker-Lewis Index (TLI) 0.899 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1132.679 ## Loglikelihood unrestricted model (H1) -1125.399 ## ## Akaike (AIC) 2293.358 ## Bayesian (BIC) 2334.838 ## Sample-size adjusted Bayesian (BIC) 2290.540 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.087 ## 90 Percent confidence interval - lower 0.016 ## 90 Percent confidence interval - upper 0.150 ## P-value RMSEA &lt;= 0.05 0.146 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.074 ## ## Parameter Estimates: ## ## Standard errors Bootstrap ## Number of requested bootstrap draws 1000 ## Number of successful bootstrap draws 1000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## SelfEst ~ ## ProSoc (y_21) 0.317 0.094 3.379 0.001 ## Aggr (y_22) 0.147 0.083 1.775 0.076 ## ParAtt 0.272 0.077 3.539 0.000 ## ProSoc ~ ## Emp (m21_) 0.520 0.078 6.687 0.000 ## Aggr ~ ## Emp (m22_) -0.354 0.081 -4.370 0.000 ## Emp ~ ## PeerAtt (m1_2) 0.349 0.072 4.885 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .ProSoc ~~ ## .Aggr -0.099 0.058 -1.699 0.089 ## ParAtt ~~ ## PeerAtt 0.537 0.124 4.318 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .SelfEst 0.813 0.085 9.578 0.000 ## .ProSoc 0.657 0.078 8.479 0.000 ## .Aggr 0.789 0.107 7.345 0.000 ## .Emp 0.785 0.095 8.293 0.000 ## ParAtt 1.277 0.200 6.399 0.000 ## PeerAtt 0.963 0.108 8.953 0.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## ie_pro 0.058 0.025 2.278 0.023 ## ie_agg -0.018 0.012 -1.579 0.114 ## Compute CIs: parameterEstimates(out_res, ci = TRUE, level = 0.9) The fit of the model is marginal. The \\(\\chi^2\\) is pretty small, and the CFI looks good, but the TLI is below the threshold of acceptable fit. The RMSEA and SRMR are toward the upper end of the range for “acceptable” fit. Yes, the direct effect of Parent Attachment on Self Esteem is significant (\\(\\beta = 0.272\\), \\(Z = 3.54\\), \\(p &lt; 0.001\\)). The IE of Peer Attachment on Self Esteem through Empathy and Prosocial Behavior is significant (\\(\\hat{\\textit{IE}} = 0.058\\), \\(95\\% ~ CI = [0.022; \\infty]\\)), as is the analogous IE through Aggressive Behavior (\\(\\hat{\\textit{IE}} = -0.018\\), \\(95\\% ~ CI = [-\\infty; -0.002]\\)). 4.5.5 Re-estimate the model from 4.4.7 in the At-Home Exercises. Use B = 1000 bootstrap samples. Other than the se and bootstrap options, use the defaults. Does this model give the same answers (vis-à-vis the hypotheses) as the restricted model from 4.5.4? Click for explanation ## Define the model syntax: fullMod &lt;- &#39; ## Equation for outcome: SelfEst ~ y_m21 * ProSoc + y_m22 * Aggr + Emp + ParAtt + PeerAtt ## Equations for stage 2 mediators: ProSoc ~ m21_x2 * PeerAtt + ParAtt + m21_m1 * Emp Aggr ~ m22_x2 * PeerAtt + ParAtt + m22_m1 * Emp ## Equation for stage 1 mediator: Emp ~ ParAtt + m1_x2 * PeerAtt ## Covariances: ProSoc ~~ Aggr ParAtt ~~ PeerAtt ## Indirect effects: ie_pro := m1_x2 * m21_m1 * y_m21 ie_agg := m1_x2 * m22_m1 * y_m22 &#39; ## Set a seed to get replicable bootstrap samples: set.seed(235711) ## Estimate the model with bootstrapping: out_full &lt;- sem(fullMod, data = seData, se = &quot;bootstrap&quot;, bootstrap = 1000) ## Summarize the model: summary(out_full) ## lavaan 0.6-12 ended normally after 16 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 21 ## ## Number of observations 143 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Bootstrap ## Number of requested bootstrap draws 1000 ## Number of successful bootstrap draws 1000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## SelfEst ~ ## ProSoc (y_21) 0.252 0.098 2.566 0.010 ## Aggr (y_22) 0.185 0.085 2.175 0.030 ## Emp 0.143 0.098 1.465 0.143 ## ParAtt 0.244 0.084 2.925 0.003 ## PerAtt 0.051 0.096 0.526 0.599 ## ProSoc ~ ## PerAtt (m21_2) -0.037 0.078 -0.482 0.630 ## ParAtt 0.193 0.070 2.740 0.006 ## Emp (m21_1) 0.477 0.080 5.995 0.000 ## Aggr ~ ## PerAtt (m22_2) -0.095 0.086 -1.099 0.272 ## ParAtt -0.034 0.078 -0.438 0.662 ## Emp (m22_1) -0.309 0.090 -3.449 0.001 ## Emp ~ ## ParAtt 0.078 0.068 1.147 0.251 ## PerAtt (m1_2) 0.306 0.077 3.968 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .ProSoc ~~ ## .Aggr -0.086 0.056 -1.527 0.127 ## ParAtt ~~ ## PeerAtt 0.537 0.124 4.318 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .SelfEst 0.796 0.083 9.587 0.000 ## .ProSoc 0.618 0.068 9.044 0.000 ## .Aggr 0.777 0.104 7.488 0.000 ## .Emp 0.779 0.091 8.554 0.000 ## ParAtt 1.277 0.200 6.399 0.000 ## PeerAtt 0.963 0.108 8.953 0.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## ie_pro 0.037 0.019 1.931 0.054 ## ie_agg -0.017 0.010 -1.678 0.093 parameterEstimates(out_full, ci = TRUE, level = 0.9) Although the estimates differ somewhat, the inferential conclusions are more-or-less the same whether we estimate the effects with the full or the restricted model. The meaning of these effects are not identical though. We are partialing out the effects of more variables in the full model. If you found that the inferential conclusions were the same in the full and restricted models, considerations of parsimony may tempt you to base your conclusions on the restricted model. However, to do so at this point would be premature. We must first show that the restricted model is an equally good representation of the data. The restricted model is nested within the full model, so we can use a \\(\\Delta \\chi^2\\) test to evaluate the loss of fit. 4.5.6 Conduct a \\(\\Delta \\chi^2\\) test to check if the restricted model still fits sufficiently well. What do you conclude? Do you notice anything about the relation between the \\(\\Delta \\chi^2\\) in this test and the \\(\\chi^2\\) model fit statistic from 4.5.4? Hint: You can use the anova() function to conduct a \\(\\Delta \\chi^2\\) test between two fitted lavaan objects. Click for explanation anova(out_full, out_res) The \\(\\Delta \\chi^2\\) is significant, so we should not prefer the restricted model. Constraining the full model has produced too much loss of fit. The \\(\\Delta \\chi^2\\) (and it’s associated degrees of freedom and p-value) is identical to the model fit statistic from 4.5.4. Of course, we should not find this surprising. Since the full model is saturated, it has \\(\\chi^2 = 0\\). So, our \\(\\Delta \\chi^2\\) statistic was computed by subtracting zero from the \\(\\chi^2\\) for the restricted model. Hence, we didn’t really need to do this test; we already had the answer. Recall the four different flavors of effect that we can define in a multiple mediation model. Total effect Direct effect Specific indirect effect Total indirect effect Take note of the following points. If we have multiple inputs and/or outcomes, we can each pair of X and Y will be linked through a set of these four effects. If any of the multiple mediators are arranged serially in the process, these four flavors of effect can connect an input to a mediator, connect a mediator to an outcome, or connect two mediators. So, we can define many different effects when working with complicated process models. 4.5.7 List all possible effects (i.e., direct, total, [specific/total] indirect) that we can define based on the full model represented in Figure 4.1. Do not include the paths that attach variables that are adjacent in the process (e.g., a and b paths). Click for explanation Three-Step Specific Indirect Effects Y_M21_M1_X1 = Parent Attachment \\(\\rightarrow\\) Empathy \\(\\rightarrow\\) Prosocial Behavior \\(\\rightarrow\\) Self Esteem Y_M22_M1_X1 = Parent Attachment \\(\\rightarrow\\) Empathy \\(\\rightarrow\\) Aggressive Behavior \\(\\rightarrow\\) Self Esteem Y_M21_M1_X2 = Peer Attachment \\(\\rightarrow\\) Empathy \\(\\rightarrow\\) Prosocial Behavior \\(\\rightarrow\\) Self Esteem Y_M22_M1_X2 = Peer Attachment \\(\\rightarrow\\) Empathy \\(\\rightarrow\\) Aggressive Behavior \\(\\rightarrow\\) Self Esteem Two-Step Specific Indirect Effects Y_M1_X1 = Parent Attachment \\(\\rightarrow\\) Empathy \\(\\rightarrow\\) Self Esteem Y_M21_X1 = Parent Attachment \\(\\rightarrow\\) Prosocial Behavior \\(\\rightarrow\\) Self Esteem M21_M1_X1 = Parent Attachment \\(\\rightarrow\\) Empathy \\(\\rightarrow\\) Prosocial Behavior Y_M21_M1 = Empathy \\(\\rightarrow\\) Prosocial Behavior \\(\\rightarrow\\) Self Esteem Y_M22_X1 = Parent Attachment \\(\\rightarrow\\) Aggressive Behavior \\(\\rightarrow\\) Self Esteem M22_M1_X1 = Parent Attachment \\(\\rightarrow\\) Empathy \\(\\rightarrow\\) Aggressive Behavior Y_M22_M1 = Empathy \\(\\rightarrow\\) Aggressive Behavior \\(\\rightarrow\\) Self Esteem Y_M1_X2 = Peer Attachment \\(\\rightarrow\\) Empathy \\(\\rightarrow\\) Self Esteem Y_M21_X2 = Peer Attachment \\(\\rightarrow\\) Prosocial Behavior \\(\\rightarrow\\) Self Esteem M21_M1_X2 = Peer Attachment \\(\\rightarrow\\) Empathy \\(\\rightarrow\\) Prosocial Behavior Y_M21_M1 = Empathy \\(\\rightarrow\\) Prosocial Behavior \\(\\rightarrow\\) Self Esteem Y_M22_X2 = Peer Attachment \\(\\rightarrow\\) Aggressive Behavior \\(\\rightarrow\\) Self Esteem M22_M1_X2 = Peer Attachment \\(\\rightarrow\\) Empathy \\(\\rightarrow\\) Aggressive Behavior Y_M22_M1 = Empathy \\(\\rightarrow\\) Aggressive Behavior \\(\\rightarrow\\) Self Esteem Total Indirect Effects IE_Y_X1 = Y_M21_M1_X1 + Y_M22_M1_X1 + Y_M1_X1 + Y_M21_X1 + Y_M22_X1 IE_Y_X2 = Y_M21_M1_X2 + Y_M22_M1_X2 + Y_M1_X2 + Y_M21_X2 + Y_M22_X2 IE_Y_M1 = Y_M21_M1 + Y_M22_M1 Direct Effects DE_Y_X1 = Parent Attachment \\(\\rightarrow\\) Self Esteem DE_M21_X1 = Parent Attachment \\(\\rightarrow\\) Prosocial Behavior DE_M22_X1 = Parent Attachment \\(\\rightarrow\\) Aggressive Behavior DE_Y_X2 = Peer Attachment \\(\\rightarrow\\) Self Esteem DE_M21_X2 = Peer Attachment \\(\\rightarrow\\) Prosocial Behavior DE_M22_X2 = Peer Attachment \\(\\rightarrow\\) Aggressive Behavior DE_Y_M1 = Empathy \\(\\rightarrow\\) Self Esteem Total Effects TE_Y_X1 = IE_Y_X1 + DE_Y_X1 TE_Y_X2 = IE_Y_X2 + DE_Y_X2 TE_Y_M1 = IE_Y_M1 + DE_Y_M1 TE_M21_X1 = M21_M1_X1 + DE_M21_X1 TE_M21_X2 = M21_M1_X2 + DE_M21_X2 TE_M22_X1 = M22_M1_X1 + DE_M22_X1 TE_M22_X2 = M22_M1_X2 + DE_M22_X2 4.5.8 Modify the model syntax for the full model to define all possible specific and total indirect effects between Parent Attachment and Self Esteem and between Peer Attachment and Self Esteem. Click for explanation mod &lt;- &#39; ## Equation for outcome: SelfEst ~ y_m21 * ProSoc + y_m22 * Aggr + y_m1 * Emp + ParAtt + PeerAtt ## Equations for stage 2 mediators: ProSoc ~ m21_x2 * PeerAtt + m21_x1 * ParAtt + m21_m1 * Emp Aggr ~ m22_x2 * PeerAtt + m22_x1 * ParAtt + m22_m1 * Emp ## Equation for stage 1 mediator: Emp ~ m1_x1 * ParAtt + m1_x2 * PeerAtt ## Covariances: ProSoc ~~ Aggr ParAtt ~~ PeerAtt ## Specific indirect effects: ie_m21_m1_x1 := m1_x1 * m21_m1 * y_m21 ie_m22_m1_x1 := m1_x1 * m22_m1 * y_m22 ie_m21_m1_x2 := m1_x2 * m21_m1 * y_m21 ie_m22_m1_x2 := m1_x2 * m22_m1 * y_m22 ie_m21_x2 := m21_x2 * y_m21 ie_m22_x2 := m22_x2 * y_m22 ie_m21_x1 := m21_x1 * y_m21 ie_m22_x1 := m22_x1 * y_m22 ie_m1_x1 := m1_x1 * y_m1 ie_m1_x2 := m1_x2 * y_m1 ## Total indirect effects: y_x1 := ie_m21_m1_x1 + ie_m22_m1_x1 + ie_m21_x1 + ie_m22_x1 + ie_m1_x1 y_x2 := ie_m21_m1_x2 + ie_m22_m1_x2 + ie_m21_x2 + ie_m22_x2 + ie_m1_x2 &#39; 4.5.9 Use lavaan::sem() to estimate the model from 4.5.8. Use B = 1000 bootstrap samples. Other than the se and bootstrap options, use the defaults. Which specific and total indirect effects are significant according to the bootstrapped CIs? Click for explanation ## Load the dplyr package to help parse the results: library(dplyr) ## Set a seed to get replicable bootstrap samples: set.seed(235711) ## Estimate the model with bootstrapping: out &lt;- sem(mod, data = seData, se = &quot;bootstrap&quot;, bootstrap = 1000) ## Summarize the model: summary(out) ## lavaan 0.6-12 ended normally after 16 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 21 ## ## Number of observations 143 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Bootstrap ## Number of requested bootstrap draws 1000 ## Number of successful bootstrap draws 1000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## SelfEst ~ ## ProSc (y_21) 0.252 0.098 2.566 0.010 ## Aggr (y_22) 0.185 0.085 2.175 0.030 ## Emp (y_m1) 0.143 0.098 1.465 0.143 ## PrAtt 0.244 0.084 2.925 0.003 ## PrAtt 0.051 0.096 0.526 0.599 ## ProSoc ~ ## PrAtt (m21_2) -0.037 0.078 -0.482 0.630 ## PrAtt (m21_x1) 0.193 0.070 2.740 0.006 ## Emp (m21_m1) 0.477 0.080 5.995 0.000 ## Aggr ~ ## PrAtt (m22_2) -0.095 0.086 -1.099 0.272 ## PrAtt (m22_x1) -0.034 0.078 -0.438 0.662 ## Emp (m22_m1) -0.309 0.090 -3.449 0.001 ## Emp ~ ## PrAtt (m1_1) 0.078 0.068 1.147 0.251 ## PrAtt (m1_2) 0.306 0.077 3.968 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .ProSoc ~~ ## .Aggr -0.086 0.056 -1.527 0.127 ## ParAtt ~~ ## PeerAtt 0.537 0.124 4.318 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .SelfEst 0.796 0.083 9.587 0.000 ## .ProSoc 0.618 0.068 9.044 0.000 ## .Aggr 0.777 0.104 7.488 0.000 ## .Emp 0.779 0.091 8.554 0.000 ## ParAtt 1.277 0.200 6.399 0.000 ## PeerAtt 0.963 0.108 8.953 0.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## ie_m21_m1_x1 0.009 0.010 0.907 0.364 ## ie_m22_m1_x1 -0.004 0.005 -0.840 0.401 ## ie_m21_m1_x2 0.037 0.019 1.931 0.054 ## ie_m22_m1_x2 -0.017 0.010 -1.678 0.093 ## ie_m21_x2 -0.009 0.021 -0.451 0.652 ## ie_m22_x2 -0.018 0.020 -0.882 0.378 ## ie_m21_x1 0.049 0.025 1.913 0.056 ## ie_m22_x1 -0.006 0.016 -0.397 0.691 ## ie_m1_x1 0.011 0.014 0.815 0.415 ## ie_m1_x2 0.044 0.033 1.315 0.189 ## y_x1 0.058 0.034 1.706 0.088 ## y_x2 0.036 0.043 0.840 0.401 parameterEstimates(out, ci = TRUE, level = 0.9) %&gt;% filter(op == &quot;:=&quot;) %&gt;% select(label, est, ci.lower, ci.upper) Both three-step IEs from Peer Attachment to Self Esteem are significant (Prosocial Behavior: \\(\\hat{\\textit{IE}} = 0.037\\), \\(95\\% ~ CI = [0.009; \\infty]\\) ; Aggressive Behavior: \\(\\hat{\\textit{IE}} = -0.017\\), \\(95\\% ~ CI = [-\\infty; -0.003]\\)). The two-step IE from Parent Attachment to Self Esteem through Prosocial Behavior is significant (\\(\\hat{\\textit{IE}} = 0.049\\), \\(95\\% ~ CI = [0.01; \\infty]\\)). The total IE from Parent Attachment to Self Esteem is significant (\\(\\hat{\\textit{IE}} = 0.058\\), \\(95\\% ~ CI = [0.002; \\infty]\\)). Interestingly, the total IE from Peer Attachment to Self Esteem is not significant even though two of the underlying specific indirect effects are significant. Can you think of an explanation for this (apparent) inconsistency? If we want to compare the size of indirect effects (specific or total), we only need to consider the scaling of the input and outcome variables. The indirect effect is interpreted as the expected change in Y for each unit of change in X that is transmitted through M; the scaling of the mediators does not enter into the interpretation. 4.5.10 In terms of a qualitative comparison of effect sizes, which of each of the following pairs of IEs is larger? The two three-step IEs from Peer Attachment to Self Esteem. The total IEs from Parent Attachment vs. Peer Attachment to Self Esteem. Note: Both Parent Attachment and Peer Attachment have the same scale. Click for explanation The specific IE through Prosocial Behavior (0.037) is stronger than the specific IE through Aggressive Behavior (-0.017). The total IE for Parent Attachment (0.058) is stronger than the total IE for Peer Attachment (0.036). If we want to conduct a formal statistical test for these comparisons, we can define a new parameter that quantifies the difference we wish to evaluate and use bootstrapping to estimate a sampling distribution for this defined parameter. We then make the inferences exactly as we did for the IEs. 4.5.11 Modify the model syntax from 4.5.8 to include parameters that quantify the differences need to test the comparisons in 4.5.10. Click for explanation Rather than copying the entire syntax string, I’ll use the paste() function to add only the two new lines to the existing syntax. ## Modify the model syntax: mod &lt;- paste(mod, &#39;sie_diff := ie_m21_m1_x2 + ie_m22_m1_x2&#39;, &#39;tie_diff := y_x1 - y_x2&#39;, sep = &quot;\\n&quot;) ## Check the result: cat(mod) ## ## ## Equation for outcome: ## SelfEst ~ y_m21 * ProSoc + ## y_m22 * Aggr + ## y_m1 * Emp + ## ParAtt + ## PeerAtt ## ## ## Equations for stage 2 mediators: ## ProSoc ~ m21_x2 * PeerAtt + ## m21_x1 * ParAtt + ## m21_m1 * Emp ## Aggr ~ m22_x2 * PeerAtt + ## m22_x1 * ParAtt + ## m22_m1 * Emp ## ## ## Equation for stage 1 mediator: ## Emp ~ m1_x1 * ParAtt + m1_x2 * PeerAtt ## ## ## Covariances: ## ProSoc ~~ Aggr ## ParAtt ~~ PeerAtt ## ## ## Specific indirect effects: ## ie_m21_m1_x1 := m1_x1 * m21_m1 * y_m21 ## ie_m22_m1_x1 := m1_x1 * m22_m1 * y_m22 ## ## ie_m21_m1_x2 := m1_x2 * m21_m1 * y_m21 ## ie_m22_m1_x2 := m1_x2 * m22_m1 * y_m22 ## ## ie_m21_x2 := m21_x2 * y_m21 ## ie_m22_x2 := m22_x2 * y_m22 ## ## ie_m21_x1 := m21_x1 * y_m21 ## ie_m22_x1 := m22_x1 * y_m22 ## ## ie_m1_x1 := m1_x1 * y_m1 ## ie_m1_x2 := m1_x2 * y_m1 ## ## ## Total indirect effects: ## y_x1 := ie_m21_m1_x1 + ie_m22_m1_x1 + ie_m21_x1 + ie_m22_x1 + ie_m1_x1 ## y_x2 := ie_m21_m1_x2 + ie_m22_m1_x2 + ie_m21_x2 + ie_m22_x2 + ie_m1_x2 ## ## sie_diff := ie_m21_m1_x2 + ie_m22_m1_x2 ## tie_diff := y_x1 - y_x2 When defining the difference between the two specific indirect effects, sie_diff, we add the effects because the second specific IE (through Aggressive Behavior) is negative. We want to test the difference in magnitude, not the difference between the signed estimates. So, we need to define the difference in terms of the absolute values of the specific indirect effects. 4.5.12 Use lavaan::sem() to estimate the model you defined in 4.5.11. Use B = 1000 bootstrap samples. Other than the se and bootstrap options, use the defaults. Are the tested specific and total indirect effects are significant different according to the bootstrapped CIs? Click for explanation Note that we now need to use two-tailed tests again, since we do not have any particular reason to expect that one effect will be stronger than the other. ## Set a seed to get replicable bootstrap samples: set.seed(235711) ## Estimate the model with bootstrapping: out &lt;- sem(mod, data = seData, se = &quot;bootstrap&quot;, bootstrap = 1000) ## Summarize the model: summary(out) ## lavaan 0.6-12 ended normally after 16 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 21 ## ## Number of observations 143 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Bootstrap ## Number of requested bootstrap draws 1000 ## Number of successful bootstrap draws 1000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## SelfEst ~ ## ProSc (y_21) 0.252 0.098 2.566 0.010 ## Aggr (y_22) 0.185 0.085 2.175 0.030 ## Emp (y_m1) 0.143 0.098 1.465 0.143 ## PrAtt 0.244 0.084 2.925 0.003 ## PrAtt 0.051 0.096 0.526 0.599 ## ProSoc ~ ## PrAtt (m21_2) -0.037 0.078 -0.482 0.630 ## PrAtt (m21_x1) 0.193 0.070 2.740 0.006 ## Emp (m21_m1) 0.477 0.080 5.995 0.000 ## Aggr ~ ## PrAtt (m22_2) -0.095 0.086 -1.099 0.272 ## PrAtt (m22_x1) -0.034 0.078 -0.438 0.662 ## Emp (m22_m1) -0.309 0.090 -3.449 0.001 ## Emp ~ ## PrAtt (m1_1) 0.078 0.068 1.147 0.251 ## PrAtt (m1_2) 0.306 0.077 3.968 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .ProSoc ~~ ## .Aggr -0.086 0.056 -1.527 0.127 ## ParAtt ~~ ## PeerAtt 0.537 0.124 4.318 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .SelfEst 0.796 0.083 9.587 0.000 ## .ProSoc 0.618 0.068 9.044 0.000 ## .Aggr 0.777 0.104 7.488 0.000 ## .Emp 0.779 0.091 8.554 0.000 ## ParAtt 1.277 0.200 6.399 0.000 ## PeerAtt 0.963 0.108 8.953 0.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## ie_m21_m1_x1 0.009 0.010 0.907 0.364 ## ie_m22_m1_x1 -0.004 0.005 -0.840 0.401 ## ie_m21_m1_x2 0.037 0.019 1.931 0.054 ## ie_m22_m1_x2 -0.017 0.010 -1.678 0.093 ## ie_m21_x2 -0.009 0.021 -0.451 0.652 ## ie_m22_x2 -0.018 0.020 -0.882 0.378 ## ie_m21_x1 0.049 0.025 1.913 0.056 ## ie_m22_x1 -0.006 0.016 -0.397 0.691 ## ie_m1_x1 0.011 0.014 0.815 0.415 ## ie_m1_x2 0.044 0.033 1.315 0.189 ## y_x1 0.058 0.034 1.706 0.088 ## y_x2 0.036 0.043 0.840 0.401 ## sie_diff 0.019 0.018 1.057 0.290 ## tie_diff 0.022 0.059 0.380 0.704 parameterEstimates(out, ci = TRUE) %&gt;% filter(label %in% c(&quot;sie_diff&quot;, &quot;tie_diff&quot;)) %&gt;% select(label, est, ci.lower, ci.upper) Neither the two three-step specific indirect effects from Peer Attachment to Self Esteem nor the two total effects from the inputs to Self Esteem are significantly different. 4.5.13 Reconsider the answers you gave to the research questions after the At-Home Exercise analyses. Do the new results you’ve found here change your conclusions? What are your conclusions vis-à-vis the stated hypotheses? Click for explanation The new results further support the hypotheses regarding the indirect effect of Peer Attachment on Self Esteem. Both three-step specific indirect effects were significant. None of the other potential specific indirect effects from Peer Attachment to Self Esteem was significant. As far as our model can tell, it appears that Peer Attachment really does influence Self Esteem primarily through the two hypothesized three-step IEs. The new results provide evidence against the hypothesized stand-alone direct effect of Parent Attachment on Self Esteem. Although the hypothesized direct effect is significant, so is the total indirect effect and the specific indirect effect through Prosocial Behavior. Our model suggests that Parent Attachment influences Self Esteem both directly and indirectly. End of In-Class Exercises "],["moderation.html", "5 Moderation", " 5 Moderation Homework before the lecture Watch the Lecture Recording for this week. Complete the Reading for this week, and answer the associated reading questions. Complete the Formative Assessment for this week. Lecture content In this lecture, we will discuss another common way that we can model the relations among three (or more) variables: moderation. We will also contrast these concepts with the mediation ideas from last week. Homework before the practical Complete the At-Home Exercises. Practical content During the practical you will work on the In-Class Exercises. "],["lecture-4.html", "5.1 Lecture", " 5.1 Lecture Download slides As with mediation, researchers often posit theories involving moderation. Moderation implies that the effect of X on Y depends on another variable, Z. For instance, the effect of feedback (X) on performance (Y) may depend on age (Z). Older children might process feedback more effectively than younger children. Hence, the feedback is more effective for older children than for younger children, and the effect of feedback on performance is stronger for older children than for younger children. In such a case, we would say that age moderates the effect of feedback on performance. "],["reading-4.html", "5.2 Reading", " 5.2 Reading Reference Baron, R. M. &amp; Kenny, D. A. (1986). The moderator-mediator variable distinction in social psychological research: Conceptual, strategic, and statistical Considerations. Journal of Personality and Individual Differences, 51(6), 1173–1182 Questions What is “moderation”, and how is it different from “mediation”? Give an example of moderation. What are the four methods given by Baron and Kenny as suitable ways to to study interaction effects? The authors suggest that one of the most common ways to address unreliability is to use multiple indicators. Thinking back to what you’ve learned about factor analysis, briefly explain why multiple indicators can improve reliability. How can you determine whether a variable is a mediator or moderator? Answers A moderator is a variable that affects the relation between two other variables. Moderation is about context: when does X affect Y (where we define when in terms of the level of some moderator variable). Mediation, on the other hand, is concerned with explaining how X transmits its effect to Y (in terms of intermediary variables). A few possibilities: Quality of recovery moderates the relation between training intensity and strength gains for power lifters. Amount of income moderates the relation between length of employment and level of job satisfaction. Level of fatalistic thinking moderates the efficacy of a new therapy for treating depression. The different recommendations are broken down according to the measurement levels of the IV and moderator. Case 1: Discrete IV, Discrete Moderator \\(\\rightarrow\\) Factorial ANOVA Case 2: Continuous IV, Discrete Moderator \\(\\rightarrow\\) Test the focal effect within each level of the moderator. Case 3: Discrete IV, Continuous Moderator \\(\\rightarrow\\) Include (an) interaction effect(s) in your model. Case 4: Continuous IV, Continuous Moderator \\(\\rightarrow\\) Include an interaction effect in your model. Each observed variable contains measurement error. When forming scales using multiple indicators, random measurement errors should cancel out. Measuring each hypothetical construct with multiple items also produces better operationalizations of constructs. The data cannot tell us if some variable, say Z, should be a moderator or a mediator; we must base this decision on theory and substantive expertise. We must think about the process by which X affects Y and ask: Is Z an intermediary step in the causal chain linking X and Y? If so, we should include Z as a mediator. Does Z stand apart from the causal chain linking X and Y but affect the nature of that linkage via contextual influences? If so, we should include Z as a moderator. Statistically speaking, we would prefer our moderators to be unrelated to X and Y, while we want mediators to relate strongly with both X and Y. Reference Weston, R. &amp; Gore, P. A. (2006). A brief guide to structural equation modeling. The Counseling Psychologist 34, 719–752. Notes: This article is quite general and provides an overview of things we have discussed so far in this course. This article also also adds an important new idea: combining factor analysis with path modeling to produce a full Structural Equation Model (SEM). Skip the part on GFI (p. 741). The GFI has been shown to be too dependent on sample size and is not recommended any longer. Skip the part on missing data. There is nothing wrong with this section, but missing data analysis is a broad and difficult topic that we cannot adequately cover in this course. If you would like to learn more about missing data and how to treat them, you can take two courses offered by our department: Conducting a Survey Missing Data Theory and Causal Effects Questions The authors state three similarities and two big differences between SEM and other multivariate statistical techniques (e.g., ANCOVA, regression). What are these similarities and differences? Do you agree with the relative strengths and weaknesses of SEM vs. other methods that the authors present? The authors miss at least one additional advantage of SEM over other multivariate methods. What is this missing advantage? Explain what the terms “measurement model” and “structural model” mean in the SEM context. What are the 6 steps of doing an SEM-based analysis given by the authors? The authors claim that testing an SEM using cross-validation is a good idea. When is cross-validation helpful in SEM? Hint: You may have to do some independent (internet, literature) research to learn how cross-validation can be implemented in SEM. Answers Q1: Similarities: All these methods are different flavors of the general linear model. These methods only produce valid results when their assumptions are met. None of these methods can prove causality. Differences: We can include latent constructs in SEM. Latent variables provide a better operationalization of theoretical constructs than observed scale scores. Latent variables contain less measurement error than observed scale scores. We can test for the model fit in SEM. Q2: The ability to test model fit is presented as a disadvantage, but that doesn’t make much sense. Tests for the model fit can tell us how well our theory comports with the data! Q3: In SEM, we can model more complex relations between constructs/variables. Q4: Measurement model = The factor model part of the overall SEM wherein the latent constructs are defined in terms of their relations to the observed indicators Structural model = The path model part of the overall SEM wherein causal linkages between the constructs/variables are defined and estimated as path/regression coefficients Q5: Data collection Model specification Model identification Model estimation Model evaluation Model modification Q6: Any time you make post hoc modifications to your model based on the model’s estimates/results, you step out of the realm of confirmatory modeling and enter the exploratory regime. If you treat your modified model as though you had hypothesized and estimated the modified structure from the beginning, you risk overfitting the data, and you cannot fully trust the inferential conclusions. After making post hoc model modifications, you can check their effect by refitting your modified model to a set of hold-out validation data. If the modified model still fits the validation data well and the results align with those from the original data, you can safely use the model to evaluate your theoretical questions. "],["formative-assessment-4.html", "5.3 Formative Assessment", " 5.3 Formative Assessment A formative assessment helps you judge your progress in the course and helps you address any blind spots in your understanding of the material. This assessment does not contribute to your course grade. If you get a question wrong, you will receive a hint on how to improve your understanding of the material. Ideally, you should complete this assessment after you have watched the lecture recording, but before the lecture meeting. This way, if you discover gaps in your understanding, we can discuss any topics that need more attention during the lecture meeting. Question 1: Centering predictors in moderation analysis affects both model fit and model interpretation. TRUE FALSE Question 2: When comparing two groups (e.g., experimental vs control group), which one of these is not an advantage of multi-group modeling (MGM) over single-group analysis with a dummy for group membership (dummy)? With MGM, you can have different dependent variables across groups; with dummy analysis, you have only one dependent variable. With MGM, both groups can have unique residual variances; dummy analysis assumes homogeneity of variance. MGM can account for measurement variance (latent variables do not measure the exact same thing across both groups); dummy analysis assumes measurement invariance. With MGM, you can have group-specific predictors; dummy analysis assumes predictors are equal. Question 3: It is not possible to use multi-group analysis to investigate moderation with a continuous moderator. FALSE TRUE Question 4: According to Barron and Kenny, there are many ways to investigate moderation - but all of them can be specified as a structural equation model. TRUE FALSE Question 5: Which of these is not a risk of using modification indices? Overfitting the model; which means: modeling noise unique to the current dataset, thus improving the fit of the model to the current data, but reducing its generalizability and fit to future datasets. Making modifications that reduce fit to the data. Specifying paths that improve fit, but do not make theoretical sense. Conducting exploratory analyses, but still presenting the findings as hypothesis-testing (confirmatory) research. Question 6: Modification indices indicate how much the Chi-square will increase when you fix this parameter to zero. FALSE TRUE Question 7: A moderator is… A third variable that changes the relationship between X and Y. A third variable that explains unique variance in Y relative to X. A third variable that causes both X and Y. A third variable that explains why X and Y are related. "],["at-home-exercises-4.html", "5.4 At-Home Exercises", " 5.4 At-Home Exercises In these exercise we will analyze the suicide_risk.rds dataset. These analyses are based on the following paper. Metha, A., Chen, E, Mulvenon, S., and Dode, I. (1998). A theoretical model of suicide risk. Archives of Suicide Research, 4, 115–133. The suicide_risk.rds dataset was simulated from the covariance matrices presented on p.123 of the above paper. These data contain the following variables. suirisk: A numeric variable representing suicide risk subabuse: A numeric variable representing severity of substance abuse hopeless: A numeric variable representing degree of hopelessness selfesteem: A numeric variable representing levels of self-esteem depression: A numeric variable representing depression levels sex: A factor recording biological sex We will use these data to investigate if sex moderates the effects of substance abuse, hopelessness, self-esteem, and depression on suicide risk. 5.4.1 Load the suicide_risk.rds dataset. Click for explanation suicide &lt;- readRDS(&quot;suicide_risk.rds&quot;) 5.4.2 Use ggplot() to visualize the bivariate relations between the four continuous predictors and suicide risk. Based on these figures, do you think a linear relation between suicide risk and the four continuous predictors is reasonable? Click for explanation library(ggplot2) ## Create the basic plots for each predictor: p1 &lt;- ggplot(suicide, aes(x = subabuse, y = suirisk)) p2 &lt;- ggplot(suicide, aes(x = hopeless, y = suirisk)) p3 &lt;- ggplot(suicide, aes(x = selfesteem, y = suirisk)) p4 &lt;- ggplot(suicide, aes(x = depression, y = suirisk)) ## Add the details and print: p1 + geom_point() + # Add the scatterplot points geom_smooth() + # Add a smoothed trend theme_classic() # Use a nicer theme p2 + geom_point() + geom_smooth() + theme_classic() p3 + geom_point() + geom_smooth() + theme_classic() p4 + geom_point() + geom_smooth() + theme_classic() We can also use the grid.arrange() function from the gridExtra package to join these four plots into a single figure. library(gridExtra) grid.arrange(p1 + geom_point() + geom_smooth() + theme_classic(), p2 + geom_point() + geom_smooth() + theme_classic(), p3 + geom_point() + geom_smooth() + theme_classic(), p4 + geom_point() + geom_smooth() + theme_classic(), ncol = 2) There does appear to be an association between each of the predictors and suicide risk. The trends don’t look perfectly linear, but they don’t look too systematically nonlinear either, so modeling these relationships as linear seems sensible. 5.4.3 Modify the scatterplots from above by coloring the points and trend lines according to sex. Based on these figures, do you think sex may moderate (some of) these bivariate relations? Click for explanation In the ggplot() call, we simply need to define assign sex to the “colour” aesthetic. ## Create the basic plots for each predictor: p1 &lt;- ggplot(suicide, aes(x = subabuse, y = suirisk, colour = sex)) p2 &lt;- ggplot(suicide, aes(x = hopeless, y = suirisk, colour = sex)) p3 &lt;- ggplot(suicide, aes(x = selfesteem, y = suirisk, colour = sex)) p4 &lt;- ggplot(suicide, aes(x = depression, y = suirisk, colour = sex)) grid.arrange(p1 + geom_point() + geom_smooth() + theme_classic(), p2 + geom_point() + geom_smooth() + theme_classic(), p3 + geom_point() + geom_smooth() + theme_classic(), p4 + geom_point() + geom_smooth() + theme_classic(), ncol = 2) Of all the predictors, it looks like substance abuse shows the greatest differences between the male and female trends. The other three predictors demonstrate pretty similar effects for both sexes. To test moderation, we need to include interactions (i.e., product terms) between the moderator variable(s) and the focal predictor(s) on the RHS of our regression equation. If we’re using the lm() function to estimate our models via OLS regression, we need only specify the appropriate product terms in the model syntax. 5.4.4 Use the lm() function to test the moderating influence of sex on the four focal effects with OLS regression. Does sex significantly moderate the four focal effects? Hint: To specify an interaction, simply “multiply” the variable names for the moderator and focal predictor using the * symbol in the formula syntax. E.g., y ~ x * z Click for explanation out &lt;- lm(suirisk ~ sex * subabuse + sex * hopeless + sex * selfesteem + sex * depression, data = suicide) summary(out) ## ## Call: ## lm(formula = suirisk ~ sex * subabuse + sex * hopeless + sex * ## selfesteem + sex * depression, data = suicide) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.9171 -1.1597 0.0047 1.1552 4.1531 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.9862150 0.8119207 2.446 0.01477 * ## sexmale -0.8688675 1.4350193 -0.605 0.54513 ## subabuse 0.0587404 0.0149445 3.931 9.64e-05 *** ## hopeless 0.0452029 0.0444629 1.017 0.30980 ## selfesteem -0.0001569 0.0230584 -0.007 0.99458 ## depression 0.0502616 0.0157588 3.189 0.00151 ** ## sexmale:subabuse 0.0631559 0.0489630 1.290 0.19768 ## sexmale:hopeless -0.0116150 0.0787379 -0.148 0.88278 ## sexmale:selfesteem 0.0128741 0.0388335 0.332 0.74039 ## sexmale:depression 0.0651349 0.0284552 2.289 0.02248 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.62 on 511 degrees of freedom ## Multiple R-squared: 0.2208, Adjusted R-squared: 0.2071 ## F-statistic: 16.09 on 9 and 511 DF, p-value: &lt; 2.2e-16 It doesn’t look like sex has too much affect. Only the effect of depression on suicide risk was significantly moderated by sex (\\(\\beta = 0.065\\), \\(t[511] = 2.29\\), \\(p = 0.022\\)) Of course, we can do an analogous analysis via path modeling in lavaan. Unfortunately, lavaan will not (yet) dummy code our factors, so we need to manually create a dummy coded version of sex. 5.4.5 Dummy code the sex factor. Click for explanation You have seen several ways of doing this in previous practicals. Here, I’ll use the quick-and-dirty ifelse() approach. suicide$female &lt;- ifelse(suicide$sex == &quot;female&quot;, 1, 0) 5.4.6 Define the lavaan model syntax for the same moderated regression model you estimated in 5.4.4. Hints: To define an interaction term in lavaan syntax, use the : operator instead of *. You will need to explicitly include the variable names for the individual predictors (as well as the interaction terms) in the model syntax. Click for explanation mod &lt;- &#39; suirisk ~ subabuse + hopeless + selfesteem + depression + female suirisk ~ female:subabuse + female:hopeless + female:selfesteem + female:depression &#39; Note that I’ve broken the formula definition into two lines. Doing so has no effect on the model’s specification or estimation but keeps the syntax a bit tidier (by keeping the lines from getting too long). 5.4.7 Use lavaan::sem() to estimate the above model. What are your conclusions vis-à-vis the moderating influence of sex? Do these results differ from those of the OLS regression? What proportion of the variability in suicide risk is explained by this model? Click for explanation library(lavaan) out &lt;- sem(mod, data = suicide) summary(out, rsquare = TRUE) ## lavaan 0.6-12 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 10 ## ## Number of observations 521 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## suirisk ~ ## subabuse 0.122 0.046 2.640 0.008 ## hopeless 0.034 0.064 0.522 0.602 ## selfesteem 0.013 0.031 0.411 0.681 ## depression 0.115 0.023 4.918 0.000 ## female 0.869 1.421 0.611 0.541 ## female:subabus -0.063 0.048 -1.302 0.193 ## female:hopelss 0.012 0.078 0.149 0.882 ## female:selfstm -0.013 0.038 -0.335 0.738 ## female:deprssn -0.065 0.028 -2.311 0.021 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .suirisk 2.575 0.160 16.140 0.000 ## ## R-Square: ## Estimate ## suirisk 0.221 The path analysis gives the same results as the OLS regression. So, the conclusions are also the same. The model explains 22.08% of the variability in suicide risk. Based on the results above, it looks like sex does not have much moderating influence. If we are trying to build a statistical model of suicide risk, we may question if the overall moderating effect of sex really improves the model. We may also want to evaluate the significance of the focal effects, themselves. In the moderated model, the estimates for the focal predictors are conditional effects, not ordinary partial regression weights. In this particular case, these conditional effects represent the effect of each focal predictor on the outcome for males (i.e., the reference group of the moderator). If we simply want to know if these focal predictors significantly affect suicide risk, without conditioning on sex, we need to estimate a restricted model containing no moderation (i.e., the additive model with all the same predictors but no interactions). 5.4.8 Specify the lavaan model syntax for the restricted model described above. Click for explanation mod &lt;- &#39; suirisk ~ subabuse + hopeless + selfesteem + depression + female &#39; 5.4.9 Use lavaan::sem() to estimate the above model. Are the focal predictors significantly associated with suicide risk? Is sex significantly associated with suicide risk? How much variability in suicide risk is explained by these predictors? Click for explanation out_res &lt;- sem(mod, data = suicide) summary(out_res, rsquare = TRUE) ## lavaan 0.6-12 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 521 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## suirisk ~ ## subabuse 0.058 0.014 4.093 0.000 ## hopeless 0.046 0.037 1.254 0.210 ## selfesteem -0.004 0.018 -0.211 0.833 ## depression 0.072 0.013 5.508 0.000 ## female -0.133 0.154 -0.862 0.389 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .suirisk 2.636 0.163 16.140 0.000 ## ## R-Square: ## Estimate ## suirisk 0.202 Only substance abuse (\\(\\beta = 0.058\\), \\(z = 4.09\\), \\(p &lt; 0.001\\)) and depression (\\(\\beta = 0.072\\), \\(z = 5.51\\), \\(p &lt; 0.001\\)) are significant predictors of suicide risk. No, sex is not a significant predictor of suicide risk (\\(\\beta = -0.133\\), \\(z = -0.86\\), \\(p = 0.389\\)). These five predictors explain 20.23% of the variability in suicide risk. 5.4.10 Compare the full model (including interactions) and the restricted model (without interactions). How much additional variability in suicide risk do we explain by including the four interaction terms? Can we conduct a \\(\\Delta \\chi^2\\) test to compare these two models? If you think so, conduct the test and interpret the results. If you do not think so, explain why not. Click for explanation To see how much additional variability we explain, we simply subtract the \\(R^2\\) for the restricted model from the \\(R^2\\) for the full model. Adding the interaction terms explains 1.85% more variability in suicide risk. Unfortunately, we cannot use a \\(\\Delta \\chi^2\\) test to compare these models because they are both saturated (hence, they both have perfect fit). Also, these models are not nested because we added four new variables (hence, added four new columns and rows to the covariance matrix) when adding the interaction terms. If we want, we can manually implement a \\(\\Delta R^2\\) test analogous to what we would do in OLS regression, but doing so is a bit of a faff. ## Extract the residual variance: eF &lt;- lavInspect(out, &quot;theta&quot;)[&quot;suirisk&quot;, &quot;suirisk&quot;] eR &lt;- lavInspect(out_res, &quot;theta&quot;)[&quot;suirisk&quot;, &quot;suirisk&quot;] ## Compute the residual DF: n &lt;- lavInspect(out, &quot;nobs&quot;) dfF &lt;- n - fitMeasures(out, &quot;npar&quot;) dfR &lt;- n - fitMeasures(out_res, &quot;npar&quot;) ## Compute the F statistic for the R^2 difference: (f &lt;- ((eR - eF) / (dfR - dfF)) / (eF / dfF)) ## npar ## 3.027 ## Compute the p-value by comparing to an F-distribution (p &lt;- pf(f, dfR - dfF, dfF, lower.tail = FALSE)) ## npar ## 0.017 The F-statistic for the \\(\\Delta R^2\\) test is significant (\\(\\Delta R^2 = 0.018\\), \\(F[4, 511] = 3.03\\), \\(p = 0.017\\)), so we can say that including the moderating influence of sex explains significantly more variability in suicide risk. Of course, you could certainly question if an additional 1.85% of variance explained is a meaningful increase. Regardless of how you judge the increase in explained variance, though, we cannot necessarily say that adding the interaction terms produces a “better” model. Both models perfectly recreate the observed data; they are statistically equivalent representations of the phenomenon. End of At-Home Exercises "],["in-class-exercises-4.html", "5.5 In-Class Exercises", " 5.5 In-Class Exercises As in the At-Home Exercises, these exercises are based on the following paper. Metha, A., Chen, E, Mulvenon, S., and Dode, I. (1998). A theoretical model of suicide risk. Archives of Suicide Research, 4, 115–133. We will now pick-up where we left off with the At-Home Exercises. 5.5.1 Load the suicide_risk.rds dataset. Click for explanation suicide &lt;- readRDS(&quot;suicide_risk.rds&quot;) According to Metha et al (1998), the process of suicide risk is more complicated than what we modeling in the At-Home Exercises. On page 117–118 the authors summarize their expectations in five hypotheses. These hypotheses imply a path model that describes how the four continuous predictors relate to suicide risk. 5.5.2 Write the lavaan model syntax for the path model implied by the hypotheses of Metha et al (1998). Leave sex out of the model for now. Click for explanation mod &lt;- &#39; suirisk ~ depression + b * hopeless + subabuse hopeless ~ a * depression + selfesteem selfesteem ~ depression ie := a*b &#39; 5.5.3 Use lavaan::sem() to estimate the model. Does the model fit well? Are the hypothesized effects significant? Click for explanation library(lavaan) ## First fit without bootstrapping to check model adequacy: fit &lt;- sem(mod, data = suicide) summary(fit, fit.measures = TRUE) ## lavaan 0.6-12 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 9 ## ## Number of observations 521 ## ## Model Test User Model: ## ## Test statistic 9.400 ## Degrees of freedom 3 ## P-value (Chi-square) 0.024 ## ## Model Test Baseline Model: ## ## Test statistic 727.656 ## Degrees of freedom 9 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.991 ## Tucker-Lewis Index (TLI) 0.973 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -3582.981 ## Loglikelihood unrestricted model (H1) -3578.281 ## ## Akaike (AIC) 7183.962 ## Bayesian (BIC) 7222.264 ## Sample-size adjusted Bayesian (BIC) 7193.696 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.064 ## 90 Percent confidence interval - lower 0.020 ## 90 Percent confidence interval - upper 0.112 ## P-value RMSEA &lt;= 0.05 0.250 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.030 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## suirisk ~ ## depression 0.073 0.013 5.779 0.000 ## hopeless (b) 0.048 0.033 1.450 0.147 ## subabuse 0.060 0.014 4.402 0.000 ## hopeless ~ ## depression (a) 0.155 0.014 10.985 0.000 ## selfesteem -0.194 0.020 -9.694 0.000 ## selfesteem ~ ## depression -0.423 0.025 -17.136 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .suirisk 2.639 0.164 16.140 0.000 ## .hopeless 3.854 0.239 16.140 0.000 ## .selfesteem 18.561 1.150 16.140 0.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## ie 0.007 0.005 1.438 0.151 ## Now bootstrap to get appropriate test of indirect effect: set.seed(235711) fit_boot &lt;- sem(mod, data = suicide, se = &quot;boot&quot;, bootstrap = 1000) The model fits the data well (\\(\\chi^2[3] = 9.4\\), \\(p = 0.024\\), \\(\\textit{CFI} = 0.991\\), \\(\\textit{TLI} = 0.973\\), \\(\\textit{RMSEA} = 0.064\\), \\(\\textit{SRMR} = 0.03\\)), and all but the second hypothesis are supported. H1: The direct effect of depression on suicide risk is significant (\\(\\beta = 0.073\\), \\(z = 5.78\\), \\(p &lt; 0.001\\)). H2: The indirect effect of depression on suicide risk through hopelessness is not significant (\\(\\hat{\\textit{IE}} = 0.007\\), \\(95\\%~CI = [-0.003; ~ 0.018]\\)). Depression significantly predicts hopelessness, after controlling for self-esteem (\\(\\beta = 0.155\\), \\(z = 10.98\\), \\(p &lt; 0.001\\)). Hopelessness does not significantly predict suicide risk, after controlling for depression and substance abuse (\\(\\beta = 0.048\\), \\(z = 1.45\\), \\(p = 0.147\\)). H3: Depression significantly predicts decreases in self-esteem (\\(\\beta = -0.423\\), \\(z = -17.14\\), \\(p &lt; 0.001\\)). H4: Self-esteem significantly predicts hopelessness (\\(\\beta = -0.194\\), \\(z = -9.69\\), \\(p &lt; 0.001\\)). H5: Substance abuse significantly predicts suicide risk, after controlling for depression and hopelessness (\\(\\beta = 0.06\\), \\(z = 4.4\\), \\(p &lt; 0.001\\)). After failing to support H2, Metha et al (1998) modified there model based on post hoc analyses. They made two changes to the model structure: Added a path from hopelessness to substance abuse. Removed the path from hopelessness to suicide risk. 5.5.4 Modify the model syntax from 5.5.2 to reflect the post hoc modifications described above. Click for explanation mod &lt;- &#39; suirisk ~ depression + subabuse hopeless ~ depression + selfesteem subabuse ~ hopeless selfesteem ~ depression &#39; 5.5.5 Use lavaan::sem() to estimate the modified model. Does the model fit well? Are the structural paths significant? Click for explanation ## First fit without bootstrapping to check model adequacy: fit &lt;- sem(mod, data = suicide) summary(fit, fit.measures = TRUE) ## lavaan 0.6-12 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 10 ## ## Number of observations 521 ## ## Model Test User Model: ## ## Test statistic 16.024 ## Degrees of freedom 4 ## P-value (Chi-square) 0.003 ## ## Model Test Baseline Model: ## ## Test statistic 776.507 ## Degrees of freedom 10 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.984 ## Tucker-Lewis Index (TLI) 0.961 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -5186.384 ## Loglikelihood unrestricted model (H1) -5178.373 ## ## Akaike (AIC) 10392.769 ## Bayesian (BIC) 10435.326 ## Sample-size adjusted Bayesian (BIC) 10403.584 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.076 ## 90 Percent confidence interval - lower 0.040 ## 90 Percent confidence interval - upper 0.117 ## P-value RMSEA &lt;= 0.05 0.111 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.039 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## suirisk ~ ## depression 0.083 0.009 8.789 0.000 ## subabuse 0.063 0.013 4.714 0.000 ## hopeless ~ ## depression 0.155 0.014 10.985 0.000 ## selfesteem -0.194 0.020 -9.694 0.000 ## subabuse ~ ## hopeless 0.559 0.082 6.799 0.000 ## selfesteem ~ ## depression -0.423 0.025 -17.136 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .suirisk 2.650 0.164 16.140 0.000 ## .hopeless 3.854 0.239 16.140 0.000 ## .subabuse 27.475 1.702 16.140 0.000 ## .selfesteem 18.561 1.150 16.140 0.000 Yes, the modified model also fits the data well (\\(\\chi^2[4] = 16.02\\), \\(p = 0.003\\), \\(\\textit{CFI} = 0.984\\), \\(\\textit{TLI} = 0.961\\), \\(\\textit{RMSEA} = 0.076\\), \\(\\textit{SRMR} = 0.039\\)), and all structural paths are now statistically significant. Although the modified model seems to be a good representation of the data, the original and modified models imply substantially different theoretical processes. Think about what the two models say regarding the process by which depression influences suicide risk and the role that hopelessness and substance abuse play therein. We will discuss these ideas in the plenary lecture session. Sex-based differences were a key component of the Metha et al (1998) analysis. In the original paper, sex was conceptualized as a moderator, and the differences induced thereby were evaluated by analyzing the male and female subsamples separately. As a general rule, subsample analysis is rarely advisable (especially in SEM). We hypothesize moderation by sex, but the subsample analysis only allows us to infer this moderation indirectly through qualitative comparisons of the sex-specific estimates. We will be much better off directly modeling the influence of sex by specifying a multiple-group model. In lavaan, we can easily estimate any model as an unrestricted multiple-group model by adding the group = \"grouping_variable_name\" to the fitting function. You can find a tutorial at https://lavaan.ugent.be/tutorial/groups.html. 5.5.6 Re-estimate the model from 5.5.5 as a multiple-group model with sex as the grouping factor. Specify the same model structure in each group. Do not place any between-group constraints on the estimates. Does the model fit well? Click for explanation fit &lt;- sem(mod, data = suicide, group = &quot;sex&quot;) summary(fit, fit.measures = TRUE) ## lavaan 0.6-12 ended normally after 60 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 28 ## ## Number of observations per group: ## male 192 ## female 329 ## ## Model Test User Model: ## ## Test statistic 22.123 ## Degrees of freedom 8 ## P-value (Chi-square) 0.005 ## Test statistic for each group: ## male 1.091 ## female 21.031 ## ## Model Test Baseline Model: ## ## Test statistic 829.489 ## Degrees of freedom 20 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.983 ## Tucker-Lewis Index (TLI) 0.956 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -5072.461 ## Loglikelihood unrestricted model (H1) -5061.400 ## ## Akaike (AIC) 10200.923 ## Bayesian (BIC) 10320.084 ## Sample-size adjusted Bayesian (BIC) 10231.205 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.082 ## 90 Percent confidence interval - lower 0.042 ## 90 Percent confidence interval - upper 0.124 ## P-value RMSEA &lt;= 0.05 0.085 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.035 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## ## Group 1 [male]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## suirisk ~ ## depression 0.117 0.015 8.031 0.000 ## subabuse 0.126 0.042 2.998 0.003 ## hopeless ~ ## depression 0.161 0.024 6.640 0.000 ## selfesteem -0.166 0.033 -4.973 0.000 ## subabuse ~ ## hopeless 0.346 0.066 5.215 0.000 ## selfesteem ~ ## depression -0.514 0.037 -13.811 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .suirisk 1.616 0.177 9.127 0.000 ## .hopeless 7.420 1.221 6.075 0.000 ## .subabuse 2.056 0.268 7.661 0.000 ## .selfesteem 36.189 0.373 97.030 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .suirisk 2.280 0.233 9.798 0.000 ## .hopeless 3.399 0.347 9.798 0.000 ## .subabuse 6.312 0.644 9.798 0.000 ## .selfesteem 15.860 1.619 9.798 0.000 ## ## ## Group 2 [female]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## suirisk ~ ## depression 0.060 0.012 4.903 0.000 ## subabuse 0.061 0.014 4.194 0.000 ## hopeless ~ ## depression 0.148 0.017 8.482 0.000 ## selfesteem -0.221 0.026 -8.612 0.000 ## subabuse ~ ## hopeless 0.663 0.120 5.523 0.000 ## selfesteem ~ ## depression -0.372 0.031 -11.841 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .suirisk 2.049 0.117 17.461 0.000 ## .hopeless 8.896 0.875 10.172 0.000 ## .subabuse -0.958 0.483 -1.984 0.047 ## .selfesteem 33.640 0.305 110.183 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .suirisk 2.759 0.215 12.826 0.000 ## .hopeless 4.045 0.315 12.826 0.000 ## .subabuse 37.944 2.958 12.826 0.000 ## .selfesteem 18.685 1.457 12.826 0.000 Yes, the model fits the data well (\\(\\chi^2[8] = 22.12\\), \\(p = 0.005\\), \\(\\textit{CFI} = 0.983\\), \\(\\textit{TLI} = 0.956\\), \\(\\textit{RMSEA} = 0.082\\), \\(\\textit{SRMR} = 0.035\\)) 5.5.7 Qualitatively compare the effects in each group. Do you notice any salient differences? Hint: The standardized estimates are probably useful here. Click for explanation summary(fit, standardized = TRUE) ## lavaan 0.6-12 ended normally after 60 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 28 ## ## Number of observations per group: ## male 192 ## female 329 ## ## Model Test User Model: ## ## Test statistic 22.123 ## Degrees of freedom 8 ## P-value (Chi-square) 0.005 ## Test statistic for each group: ## male 1.091 ## female 21.031 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## ## Group 1 [male]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## suirisk ~ ## depression 0.117 0.015 8.031 0.000 0.117 0.493 ## subabuse 0.126 0.042 2.998 0.003 0.126 0.184 ## hopeless ~ ## depression 0.161 0.024 6.640 0.000 0.161 0.456 ## selfesteem -0.166 0.033 -4.973 0.000 -0.166 -0.342 ## subabuse ~ ## hopeless 0.346 0.066 5.215 0.000 0.346 0.352 ## selfesteem ~ ## depression -0.514 0.037 -13.811 0.000 -0.514 -0.706 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .suirisk 1.616 0.177 9.127 0.000 1.616 0.882 ## .hopeless 7.420 1.221 6.075 0.000 7.420 2.714 ## .subabuse 2.056 0.268 7.661 0.000 2.056 0.766 ## .selfesteem 36.189 0.373 97.030 0.000 36.189 6.436 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .suirisk 2.280 0.233 9.798 0.000 2.280 0.679 ## .hopeless 3.399 0.347 9.798 0.000 3.399 0.455 ## .subabuse 6.312 0.644 9.798 0.000 6.312 0.876 ## .selfesteem 15.860 1.619 9.798 0.000 15.860 0.502 ## ## ## Group 2 [female]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## suirisk ~ ## depression 0.060 0.012 4.903 0.000 0.060 0.256 ## subabuse 0.061 0.014 4.194 0.000 0.061 0.219 ## hopeless ~ ## depression 0.148 0.017 8.482 0.000 0.148 0.397 ## selfesteem -0.221 0.026 -8.612 0.000 -0.221 -0.403 ## subabuse ~ ## hopeless 0.663 0.120 5.523 0.000 0.663 0.291 ## selfesteem ~ ## depression -0.372 0.031 -11.841 0.000 -0.372 -0.547 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .suirisk 2.049 0.117 17.461 0.000 2.049 1.148 ## .hopeless 8.896 0.875 10.172 0.000 8.896 3.144 ## .subabuse -0.958 0.483 -1.984 0.047 -0.958 -0.149 ## .selfesteem 33.640 0.305 110.183 0.000 33.640 6.516 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .suirisk 2.759 0.215 12.826 0.000 2.759 0.867 ## .hopeless 4.045 0.315 12.826 0.000 4.045 0.505 ## .subabuse 37.944 2.958 12.826 0.000 37.944 0.915 ## .selfesteem 18.685 1.457 12.826 0.000 18.685 0.701 The direct effect of depression on suicide risk and the effect of depression on self esteem both seem to be quite a bit stronger for males (\\(\\beta_{SR} = 0.493\\), \\(\\beta_{SE} = -0.706\\)) than females (\\(\\beta_{SR} = 0.256\\), \\(\\beta_{SE} = -0.547\\)). 5.5.8 Qualitatively compare the \\(R^2\\) for suicide risk in the two groups. What do you conclude? Click for explanation ## Extract just the R-Squared estimates with the inspect() function: inspect(fit, &quot;r2&quot;) ## $male ## suirisk hopeless subabuse selfesteem ## 0.321 0.545 0.124 0.498 ## ## $female ## suirisk hopeless subabuse selfesteem ## 0.133 0.495 0.085 0.299 The \\(R^2\\) for males (0.321) is larger than the \\(R^2\\) for females (0.133). Hence, the model better explains suicide risk for males than it does for females. To test the moderating influence of the grouping variable, we need to test for significant differences between the group-specific estimates of the structural paths. If a path differs significantly between the two groups, we conclude that the grouping factor moderates that path. We can conceptualize these test in two general ways. An omnibus test: Does group affect any of the structural paths? Path-specific tests: Does group affect an individual path? To implement an omnibus test for moderation by group, we simply need to compare the fit of the full, unrestricted model (such as the one we estimated in 5.5.6) to the fit of a restricted model wherein each structural path is constrained to be equal in both groups. The group.equal argument in lavaan fitting functions makes this process very easy. 5.5.9 Conduct an omnibus test to see if sex moderates any of the paths in the model from 5.5.6. Does sex have a significant, overall moderating influence on the model? Hints: Check the documentation for sem() to see how you need to specify the group.equal argument. You can use the anova() function to compare the full and restricted models. Click for explanation First, estimate the restricted model. fit_res &lt;- sem(mod, data = suicide, group = &quot;sex&quot;, group.equal = &quot;regressions&quot;) ## Summarize the results to check that everything went well: summary(fit_res, fit.measures = TRUE) ## lavaan 0.6-12 ended normally after 43 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 28 ## Number of equality constraints 6 ## ## Number of observations per group: ## male 192 ## female 329 ## ## Model Test User Model: ## ## Test statistic 51.231 ## Degrees of freedom 14 ## P-value (Chi-square) 0.000 ## Test statistic for each group: ## male 17.277 ## female 33.954 ## ## Model Test Baseline Model: ## ## Test statistic 829.489 ## Degrees of freedom 20 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.954 ## Tucker-Lewis Index (TLI) 0.934 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -5087.015 ## Loglikelihood unrestricted model (H1) -5061.400 ## ## Akaike (AIC) 10218.031 ## Bayesian (BIC) 10311.657 ## Sample-size adjusted Bayesian (BIC) 10241.825 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.101 ## 90 Percent confidence interval - lower 0.072 ## 90 Percent confidence interval - upper 0.131 ## P-value RMSEA &lt;= 0.05 0.003 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.082 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## ## Group 1 [male]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## suirisk ~ ## deprssn (.p1.) 0.086 0.009 9.157 0.000 ## subabus (.p2.) 0.061 0.014 4.376 0.000 ## hopeless ~ ## deprssn (.p3.) 0.151 0.014 10.681 0.000 ## selfstm (.p4.) -0.199 0.020 -9.856 0.000 ## subabuse ~ ## hopelss (.p5.) 0.420 0.058 7.185 0.000 ## selfesteem ~ ## deprssn (.p6.) -0.431 0.024 -17.787 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .suirisk 2.012 0.131 15.375 0.000 ## .hopeless 8.576 0.737 11.640 0.000 ## .subabuse 1.836 0.252 7.288 0.000 ## .selfesteem 35.658 0.330 108.161 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .suirisk 2.388 0.244 9.798 0.000 ## .hopeless 3.420 0.349 9.798 0.000 ## .subabuse 6.353 0.648 9.798 0.000 ## .selfesteem 16.273 1.661 9.798 0.000 ## ## ## Group 2 [female]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## suirisk ~ ## deprssn (.p1.) 0.086 0.009 9.157 0.000 ## subabus (.p2.) 0.061 0.014 4.376 0.000 ## hopeless ~ ## deprssn (.p3.) 0.151 0.014 10.681 0.000 ## selfstm (.p4.) -0.199 0.020 -9.856 0.000 ## subabuse ~ ## hopelss (.p5.) 0.420 0.058 7.185 0.000 ## selfesteem ~ ## deprssn (.p6.) -0.431 0.024 -17.787 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .suirisk 1.891 0.108 17.473 0.000 ## .hopeless 8.199 0.700 11.719 0.000 ## .subabuse -0.262 0.380 -0.690 0.490 ## .selfesteem 33.995 0.281 120.946 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .suirisk 2.798 0.218 12.826 0.000 ## .hopeless 4.056 0.316 12.826 0.000 ## .subabuse 38.417 2.995 12.826 0.000 ## .selfesteem 18.882 1.472 12.826 0.000 OK, that looks good. The regression paths are constrained to be equal across groups. Now, we need to compare the full and restricted models to see if we’ve lost a significant amount of model fit through the constraints. anova(fit, fit_res) The \\(\\Delta \\chi^2\\) test tells us that we have lost a significant level of fit by constraining the paths to equality across groups (\\(\\Delta \\chi^2[6] = 29.11\\), \\(p &lt; 0.001\\)). Hence, we can infer overall moderation by sex. Finding significant omnibus moderation is interesting, but we probably want to know which paths, in particular, show significant sex-based differences. Hence, we need to test for path-specific moderation. We have several options when it comes to implementing such tests. One obvious idea would be to specify a restricted model that constrains only one path to equality and conduct the same type of \\(\\Delta \\chi^2\\) test that we used for the omnibus test. While this approach will certainly work, we would have to specify and estimate a separate restricted model for every path that we wanted to test. Thankfully, we have a couple simpler alternatives. The most direct approach entails the following steps: Label the relevant parameters Define new parameters that represent the differences between the group-specific versions of whatever parameters we want to test Estimate the model and evaluate the path-specific moderation by the significance tests for these difference parameters 5.5.10 Define the lavaan model syntax that uses the approach described above to test if sex moderates the following effects Depression \\(\\rightarrow\\) Suicide Risk Substance Abuse \\(\\rightarrow\\) Suicide Risk Hopelessness \\(\\rightarrow\\) Substance Abuse Click for explanation mod &lt;- &#39; suirisk ~ c(m1, f1) * depression + c(m2, f2) * subabuse hopeless ~ depression + selfesteem subabuse ~ c(m3, f3) * hopeless selfesteem ~ depression d1 := m1 - f1 d2 := m2 - f2 d3 := m3 - f3 &#39; 5.5.11 Estimate the model defined above. Does sex moderate these three focal effects? Click for explanation fit &lt;- sem(mod, data = suicide, group = &quot;sex&quot;) summary(fit) ## lavaan 0.6-12 ended normally after 60 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 28 ## ## Number of observations per group: ## male 192 ## female 329 ## ## Model Test User Model: ## ## Test statistic 22.123 ## Degrees of freedom 8 ## P-value (Chi-square) 0.005 ## Test statistic for each group: ## male 1.091 ## female 21.031 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## ## Group 1 [male]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## suirisk ~ ## depressin (m1) 0.117 0.015 8.031 0.000 ## subabuse (m2) 0.126 0.042 2.998 0.003 ## hopeless ~ ## depressin 0.161 0.024 6.640 0.000 ## selfestem -0.166 0.033 -4.973 0.000 ## subabuse ~ ## hopeless (m3) 0.346 0.066 5.215 0.000 ## selfesteem ~ ## depressin -0.514 0.037 -13.811 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .suirisk 1.616 0.177 9.127 0.000 ## .hopeless 7.420 1.221 6.075 0.000 ## .subabuse 2.056 0.268 7.661 0.000 ## .selfesteem 36.189 0.373 97.030 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .suirisk 2.280 0.233 9.798 0.000 ## .hopeless 3.399 0.347 9.798 0.000 ## .subabuse 6.312 0.644 9.798 0.000 ## .selfesteem 15.860 1.619 9.798 0.000 ## ## ## Group 2 [female]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## suirisk ~ ## depressin (f1) 0.060 0.012 4.903 0.000 ## subabuse (f2) 0.061 0.014 4.194 0.000 ## hopeless ~ ## depressin 0.148 0.017 8.482 0.000 ## selfestem -0.221 0.026 -8.612 0.000 ## subabuse ~ ## hopeless (f3) 0.663 0.120 5.523 0.000 ## selfesteem ~ ## depressin -0.372 0.031 -11.841 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .suirisk 2.049 0.117 17.461 0.000 ## .hopeless 8.896 0.875 10.172 0.000 ## .subabuse -0.958 0.483 -1.984 0.047 ## .selfesteem 33.640 0.305 110.183 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .suirisk 2.759 0.215 12.826 0.000 ## .hopeless 4.045 0.315 12.826 0.000 ## .subabuse 37.944 2.958 12.826 0.000 ## .selfesteem 18.685 1.457 12.826 0.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## d1 0.057 0.019 2.975 0.003 ## d2 0.065 0.044 1.465 0.143 ## d3 -0.317 0.137 -2.311 0.021 Sex significantly moderates the effect of depression on suicide risk (\\(\\beta = 0.057\\), \\(z = 2.98\\), \\(p = 0.003\\)) such that males have significantly stronger associations. Sex also moderates the effect of hopelessness on substance abuse (\\(\\beta = -0.317\\), \\(z = -2.31\\), \\(p = 0.021\\)) such that males have significantly weaker associations. Sex does not moderate the effect of substance abuse on suicide risk (\\(\\beta = 0.065\\), \\(z = 1.47\\), \\(p = 0.143\\)). We can also conduct analogous tests without explicitly defining the parameter differences as new parameters. To do so, we use the lavaan::lavTestWald() function and the following procedure. Label the relevant paths Estimate the model Submit the fitted model object and a syntax segment defining the desired constraint(s) to lavTestWald(). This approach is nice when we have already estimated the model with the paths labeled but no difference parameters defined. In such situations, we can do our tests without rewriting the model syntax and re-estimating the model. 5.5.12 Use lavTestWald() to test for moderation in the same three paths that you tested in 5.5.11. What are your conclusions? Are the results the same as what you get when defining difference parameters? Hints: You do not need to re-specify your model syntax; you can use the fitted model from 5.5.11 as input to lavTestWald(). To get tests of moderation for individual paths, you need to run lavTestWald() with a single parameter constraint specified. You will need to call lavTestWald() three times to replicate the results from 5.5.11. Click for explanation ## Depression -&gt; Suicide Risk: lavTestWald(fit, constraints = &#39;m1 == f1&#39;) ## $stat ## [1] 8.852584 ## ## $df ## [1] 1 ## ## $p.value ## [1] 0.002926742 ## ## $se ## [1] &quot;standard&quot; ## Substance Abuse -&gt; Suicide Risk: lavTestWald(fit, constraints = &#39;m2 == f2&#39;) ## $stat ## [1] 2.147055 ## ## $df ## [1] 1 ## ## $p.value ## [1] 0.1428436 ## ## $se ## [1] &quot;standard&quot; ## Hopelessness -&gt; Substance Abuse: lavTestWald(fit, constraints = &#39;m3 == f3&#39;) ## $stat ## [1] 5.342553 ## ## $df ## [1] 1 ## ## $p.value ## [1] 0.02081098 ## ## $se ## [1] &quot;standard&quot; This approach produces the same inference as the defined parameters approach. In fact the p-values are identical across the two approaches, and the test statistics produced by lavTestWald() are equal to the square of those produced by the defined parameters approach. This latter equivalence arises because the test statistic in the summarized sem() output is a Z statistic while the test statistic in the lavTestWald() output is a \\(\\chi^2\\) statistic. Notice that although Metha et al (1998) only hypothesize a single indirect effect, their model implies several potential indirect effects. Further notice that the original model and the modified model imply different sets of indirect effects. 5.5.13 List the potential indirect effects implied by the original and modified models from Metha et al (1998). Click for explanation Original: Depression \\(\\rightarrow\\) Self-Esteem \\(\\rightarrow\\) Hopelessness \\(\\rightarrow\\) Suicide Risk Depression \\(\\rightarrow\\) Hopelessness \\(\\rightarrow\\) Suicide Risk Self-Esteem \\(\\rightarrow\\) Hopelessness \\(\\rightarrow\\) Suicide Risk Modified: Depression \\(\\rightarrow\\) Self-Esteem \\(\\rightarrow\\) Hopelessness \\(\\rightarrow\\) Substance Abuse \\(\\rightarrow\\) Suicide Risk Depression \\(\\rightarrow\\) Hopelessness \\(\\rightarrow\\) Substance Abuse \\(\\rightarrow\\) Suicide Risk Self-Esteem \\(\\rightarrow\\) Hopelessness \\(\\rightarrow\\) Substance Abuse \\(\\rightarrow\\) Suicide Risk An indirect effect is defined by the product of some set of regression paths. These underlying regression paths can be moderated by a grouping factor, so indirect effects can also be moderated by grouping factors. Such a process is called moderated mediation because some moderator variable influences the indirect effects. In our case, we want to know if sex influences the indirect effects listed above such that males and females show different indirect effects. We can use the same logic that we applied to test the moderating influence of sex on individual regression paths to for moderation of the indirect effects. There is one caveat to this generalization, though. The validity of the Wald test implemented by the lavTestWald() function depends on the tested parameters having normal sampling distributions. The sampling distributions for indirect effects are not normal, so we should not use the lavTestWald() approach to evaluate moderation of the indirect effects. Rather, we should use the defined parameter approach in combination with bootstrapping. 5.5.14 Modify the syntax for the modified model (i.e., the model estimated in 5.5.6) to specify sex-specific versions of the the potential indirect effects you listed in 5.5.13. Click for explanation mod &lt;- &#39; suirisk ~ depression + c(sr_saM, sr_saF) * subabuse hopeless ~ c(h_dM, h_dF) * depression + c(h_seM, h_seF) * selfesteem subabuse ~ c(sa_hM, sa_hF) * hopeless selfesteem ~ c(se_dM, se_dF) * depression ## Indirect Effects: ie1_m := se_dM * h_seM * sa_hM * sr_saM ie2_m := h_dM * sa_hM * sr_saM ie3_m := h_seM * sa_hM * sr_saM ie1_f := se_dF * h_seF * sa_hF * sr_saF ie2_f := h_dF * sa_hF * sr_saF ie3_f := h_seF * sa_hF * sr_saF &#39; 5.5.15 Estimate the model defined in 5.5.14. Use bootstrapping to evaluate the indirect effects. Are the indirect effects significant? Click for explanation set.seed(235711) out &lt;- sem(mod, data = suicide, group = &quot;sex&quot;, se = &quot;boot&quot;, bootstrap = 1000) summary(out, ci = TRUE) ## lavaan 0.6-12 ended normally after 60 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 28 ## ## Number of observations per group: ## male 192 ## female 329 ## ## Model Test User Model: ## ## Test statistic 22.123 ## Degrees of freedom 8 ## P-value (Chi-square) 0.005 ## Test statistic for each group: ## male 1.091 ## female 21.031 ## ## Parameter Estimates: ## ## Standard errors Bootstrap ## Number of requested bootstrap draws 1000 ## Number of successful bootstrap draws 1000 ## ## ## Group 1 [male]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## suirisk ~ ## deprssn 0.117 0.014 8.442 0.000 0.088 0.143 ## subabus (sr_M) 0.126 0.040 3.136 0.002 0.050 0.203 ## hopeless ~ ## deprssn (h_dM) 0.161 0.024 6.644 0.000 0.112 0.210 ## selfstm (h_sM) -0.166 0.033 -4.976 0.000 -0.231 -0.103 ## subabuse ~ ## hopelss (s_hM) 0.346 0.067 5.150 0.000 0.212 0.484 ## selfesteem ~ ## deprssn (s_dM) -0.514 0.034 -14.960 0.000 -0.583 -0.448 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .suirisk 1.616 0.171 9.464 0.000 1.288 1.949 ## .hopeless 7.420 1.240 5.982 0.000 5.048 9.901 ## .subabuse 2.056 0.298 6.897 0.000 1.442 2.600 ## .selfesteem 36.189 0.368 98.414 0.000 35.510 36.892 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .suirisk 2.280 0.263 8.663 0.000 1.771 2.790 ## .hopeless 3.399 0.336 10.111 0.000 2.730 4.043 ## .subabuse 6.312 0.595 10.601 0.000 5.158 7.476 ## .selfesteem 15.860 1.425 11.133 0.000 13.131 18.579 ## ## ## Group 2 [female]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## suirisk ~ ## deprssn 0.060 0.014 4.370 0.000 0.033 0.087 ## subabus (sr_F) 0.061 0.015 3.956 0.000 0.032 0.090 ## hopeless ~ ## deprssn (h_dF) 0.148 0.019 7.633 0.000 0.107 0.184 ## selfstm (h_sF) -0.221 0.028 -7.787 0.000 -0.278 -0.166 ## subabuse ~ ## hopelss (s_hF) 0.663 0.110 6.050 0.000 0.445 0.873 ## selfesteem ~ ## deprssn (s_dF) -0.372 0.029 -12.924 0.000 -0.430 -0.314 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .suirisk 2.049 0.118 17.321 0.000 1.816 2.281 ## .hopeless 8.896 0.976 9.111 0.000 7.042 10.884 ## .subabuse -0.958 0.438 -2.189 0.029 -1.842 -0.103 ## .selfesteem 33.640 0.307 109.459 0.000 32.993 34.230 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .suirisk 2.759 0.182 15.185 0.000 2.390 3.106 ## .hopeless 4.045 0.314 12.881 0.000 3.377 4.622 ## .subabuse 37.944 2.888 13.137 0.000 32.259 43.642 ## .selfesteem 18.685 1.370 13.640 0.000 16.074 21.407 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## ie1_m 0.004 0.002 2.264 0.024 0.001 0.008 ## ie2_m 0.007 0.003 2.317 0.021 0.002 0.014 ## ie3_m -0.007 0.003 -2.300 0.021 -0.015 -0.002 ## ie1_f 0.003 0.001 2.714 0.007 0.001 0.006 ## ie2_f 0.006 0.002 2.967 0.003 0.002 0.010 ## ie3_f -0.009 0.003 -2.843 0.004 -0.016 -0.004 None of the 95% bootstrapped CIs for the indirect effects contain zero, so all three indirect effects are significant in both groups. 5.5.16 Modify the model syntax from 5.5.14 to include the difference parameters necessary for testing the moderating influence of sex on the indirect effects. Click for explanation Rather than re-typing the previous syntax, I’ll use the paste() function to add on only the new lines. ## Add the new parameter definitions to the existing syntax: mod &lt;- paste(mod, &#39; ie_diff1 := ie1_m - ie1_f ie_diff2 := ie2_m - ie2_f ie_diff3 := ie3_m - ie3_f &#39;, sep = &#39;\\n&#39;) ## Check the results: cat(mod) ## ## suirisk ~ depression + c(sr_saM, sr_saF) * subabuse ## hopeless ~ c(h_dM, h_dF) * depression + c(h_seM, h_seF) * selfesteem ## subabuse ~ c(sa_hM, sa_hF) * hopeless ## selfesteem ~ c(se_dM, se_dF) * depression ## ## ## Indirect Effects: ## ie1_m := se_dM * h_seM * sa_hM * sr_saM ## ie2_m := h_dM * sa_hM * sr_saM ## ie3_m := h_seM * sa_hM * sr_saM ## ## ie1_f := se_dF * h_seF * sa_hF * sr_saF ## ie2_f := h_dF * sa_hF * sr_saF ## ie3_f := h_seF * sa_hF * sr_saF ## ## ## ie_diff1 := ie1_m - ie1_f ## ie_diff2 := ie2_m - ie2_f ## ie_diff3 := ie3_m - ie3_f 5.5.17 Estimate the model defined in 5.5.16. Use bootstrapping to evaluate moderation of the indirect effects. Does sex moderate the indirect effects? Click for explanation set.seed(235711) out &lt;- sem(mod, data = suicide, group = &quot;sex&quot;, se = &quot;boot&quot;, bootstrap = 1000) summary(out, ci = TRUE) ## lavaan 0.6-12 ended normally after 60 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 28 ## ## Number of observations per group: ## male 192 ## female 329 ## ## Model Test User Model: ## ## Test statistic 22.123 ## Degrees of freedom 8 ## P-value (Chi-square) 0.005 ## Test statistic for each group: ## male 1.091 ## female 21.031 ## ## Parameter Estimates: ## ## Standard errors Bootstrap ## Number of requested bootstrap draws 1000 ## Number of successful bootstrap draws 1000 ## ## ## Group 1 [male]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## suirisk ~ ## deprssn 0.117 0.014 8.442 0.000 0.088 0.143 ## subabus (sr_M) 0.126 0.040 3.136 0.002 0.050 0.203 ## hopeless ~ ## deprssn (h_dM) 0.161 0.024 6.644 0.000 0.112 0.210 ## selfstm (h_sM) -0.166 0.033 -4.976 0.000 -0.231 -0.103 ## subabuse ~ ## hopelss (s_hM) 0.346 0.067 5.150 0.000 0.212 0.484 ## selfesteem ~ ## deprssn (s_dM) -0.514 0.034 -14.960 0.000 -0.583 -0.448 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .suirisk 1.616 0.171 9.464 0.000 1.288 1.949 ## .hopeless 7.420 1.240 5.982 0.000 5.048 9.901 ## .subabuse 2.056 0.298 6.897 0.000 1.442 2.600 ## .selfesteem 36.189 0.368 98.414 0.000 35.510 36.892 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .suirisk 2.280 0.263 8.663 0.000 1.771 2.790 ## .hopeless 3.399 0.336 10.111 0.000 2.730 4.043 ## .subabuse 6.312 0.595 10.601 0.000 5.158 7.476 ## .selfesteem 15.860 1.425 11.133 0.000 13.131 18.579 ## ## ## Group 2 [female]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## suirisk ~ ## deprssn 0.060 0.014 4.370 0.000 0.033 0.087 ## subabus (sr_F) 0.061 0.015 3.956 0.000 0.032 0.090 ## hopeless ~ ## deprssn (h_dF) 0.148 0.019 7.633 0.000 0.107 0.184 ## selfstm (h_sF) -0.221 0.028 -7.787 0.000 -0.278 -0.166 ## subabuse ~ ## hopelss (s_hF) 0.663 0.110 6.050 0.000 0.445 0.873 ## selfesteem ~ ## deprssn (s_dF) -0.372 0.029 -12.924 0.000 -0.430 -0.314 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .suirisk 2.049 0.118 17.321 0.000 1.816 2.281 ## .hopeless 8.896 0.976 9.111 0.000 7.042 10.884 ## .subabuse -0.958 0.438 -2.189 0.029 -1.842 -0.103 ## .selfesteem 33.640 0.307 109.459 0.000 32.993 34.230 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .suirisk 2.759 0.182 15.185 0.000 2.390 3.106 ## .hopeless 4.045 0.314 12.881 0.000 3.377 4.622 ## .subabuse 37.944 2.888 13.137 0.000 32.259 43.642 ## .selfesteem 18.685 1.370 13.640 0.000 16.074 21.407 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## ie1_m 0.004 0.002 2.264 0.024 0.001 0.008 ## ie2_m 0.007 0.003 2.317 0.021 0.002 0.014 ## ie3_m -0.007 0.003 -2.300 0.021 -0.015 -0.002 ## ie1_f 0.003 0.001 2.714 0.007 0.001 0.006 ## ie2_f 0.006 0.002 2.967 0.003 0.002 0.010 ## ie3_f -0.009 0.003 -2.843 0.004 -0.016 -0.004 ## ie_diff1 0.000 0.002 0.199 0.842 -0.003 0.005 ## ie_diff2 0.001 0.004 0.287 0.774 -0.006 0.009 ## ie_diff3 0.002 0.004 0.380 0.704 -0.007 0.010 All three 95% bootstrapped CIs for the difference parameters contain zero, so sex does not moderate any of the indirect effects. End of In-Class Exercises "],["full-sem.html", "6 Full SEM", " 6 Full SEM This week, you will focus on integrating all of the disparate methods we’ve covered so far into full-fledged structural equation models. We don’t have much in the way of new material. The only additional content for this week is a final set of At-Home Exercises. Lecture content This week, we have no new lecture content. This week’s lecture meeting will be an open Q&amp;A session wherein you can ask any questions you have about combining factor analysis and path modeling in full SEM. Homework before the practical Complete the At-Home Exercises. Practical content There are no In-Class Exercises this week, but the instructors will still be available. So, you have a few options for how to use this practical session. Ask questions about the At-Home Exercises Clear up any residual confusion or problems from previous practical exercises Work with your group on Assignment 2 Ask R-related questions about Assignment 2 Keep in mind that it is up to the instructors how much assistance they will offer for assignment questions "],["at-home-exercises-5.html", "6.1 At-Home Exercises", " 6.1 At-Home Exercises In these exercises, you will use full structural equation modeling (SEM) to evaluate the Theory of Reasoned Action (TORA), which is a popular psychological theory of social behavior developed by Ajzen and Fishbein. The theory states that actual behavior is predicted by behavioral intention, which is in turn predicted by the attitude toward the behavior and subjective norms about the behavior. Later, a third determinant was added, perceived behavioral control. The extent to which people feel that they have control over their behavior also influences their behavior. The data we will use for this practical are available in the toradata.csv file. These data were synthesized according to the results of Reinecke (1998)’s investigation of condom use by young people between 16 and 24 years old. The data contain the following variables: respnr: Numeric participant ID behavior: The dependent variable condom use Measured on a 5-point frequency scale (How often do you…) intent: A single item assessing behavioral intention Measured on a similar 5-point scale (In general, do you intend to…). attit_1:attit_3: Three indicators of attitudes about condom use Measured on a 5-point rating scale (e.g., using a condom is awkward) norm_1:norm_3: Three indicators of social norms about condom use Measured on a 5-point rating scale (e.g., I think most of my friends would use…) control_1:control_3: Three indicators of perceived behavioral control Measured on a 5-point rating scale (e.g., I know well how to use a condom) sex: Binary factor indicating biological sex 6.1.1 Load the data contained in the toradata.csv file. Click for explanation condom &lt;- read.csv(&quot;toradata.csv&quot;, stringsAsFactors = TRUE) 6.1.2 The data contain multiple indicators of attitudes, norms, and control. Run a CFA for these three latent variables. Correlate the latent factors. Do the data support the measurement model for these latent factors? Are the three latent factors significantly correlated? Is it reasonable to proceed with our evaluation of the TORA theory? Click for explanation library(lavaan) mod_cfa &lt;- &#39; attitudes =~ attit_1 + attit_2 + attit_3 norms =~ norm_1 + norm_2 + norm_3 control =~ control_1 + control_2 + control_3 &#39; fit &lt;- cfa(mod_cfa, data = condom) summary(fit, fit.measures = TRUE) ## lavaan 0.6-12 ended normally after 29 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 21 ## ## Number of observations 250 ## ## Model Test User Model: ## ## Test statistic 35.611 ## Degrees of freedom 24 ## P-value (Chi-square) 0.060 ## ## Model Test Baseline Model: ## ## Test statistic 910.621 ## Degrees of freedom 36 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.987 ## Tucker-Lewis Index (TLI) 0.980 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -2998.290 ## Loglikelihood unrestricted model (H1) -2980.484 ## ## Akaike (AIC) 6038.580 ## Bayesian (BIC) 6112.530 ## Sample-size adjusted Bayesian (BIC) 6045.959 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.044 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.073 ## P-value RMSEA &lt;= 0.05 0.599 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.037 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## attitudes =~ ## attit_1 1.000 ## attit_2 1.036 0.068 15.308 0.000 ## attit_3 -1.002 0.067 -14.856 0.000 ## norms =~ ## norm_1 1.000 ## norm_2 1.031 0.098 10.574 0.000 ## norm_3 0.932 0.093 10.013 0.000 ## control =~ ## control_1 1.000 ## control_2 0.862 0.129 6.699 0.000 ## control_3 0.968 0.133 7.290 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## attitudes ~~ ## norms 0.340 0.069 4.957 0.000 ## control 0.475 0.073 6.468 0.000 ## norms ~~ ## control 0.338 0.064 5.254 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .attit_1 0.418 0.052 8.047 0.000 ## .attit_2 0.310 0.047 6.633 0.000 ## .attit_3 0.369 0.049 7.577 0.000 ## .norm_1 0.504 0.071 7.130 0.000 ## .norm_2 0.469 0.071 6.591 0.000 ## .norm_3 0.635 0.075 8.465 0.000 ## .control_1 0.614 0.078 7.905 0.000 ## .control_2 0.865 0.091 9.520 0.000 ## .control_3 0.762 0.087 8.758 0.000 ## attitudes 0.885 0.116 7.620 0.000 ## norms 0.743 0.116 6.423 0.000 ## control 0.497 0.099 5.002 0.000 Yes, the model fits the data well, and the measurement parameters (e.g., factor loadings, residual variances) look reasonable. So, the data seem to support this measurement structure. Yes, all three latent variables are significantly, positively correlated. Yes. The measurement structure is supported, so we can use the latent variables to represent the respective constructs in our subsequent SEM. The TORA doesn’t actually say anything about the associations between these three factors, but it makes sense that they would be positively associated. So, we should find this result comforting. 6.1.3 Estimate the basic TORA model as an SEM. Predict intention from attitudes and norms. Predict condom use from intention. Use the latent versions of attitudes and norms. Covary the attitudes and norms factors. Does the model fit well? Do the estimates align with the TORA? How much variance in intention and condom use are explained by the model? Click for explanation mod &lt;- &#39; ## Define the latent variables: attitudes =~ attit_1 + attit_2 + attit_3 norms =~ norm_1 + norm_2 + norm_3 ## Define the structural model: intent ~ attitudes + norms behavior ~ intent &#39; fit &lt;- sem(mod, data = condom) summary(fit, fit.measures = TRUE, rsquare = TRUE) ## lavaan 0.6-12 ended normally after 24 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 18 ## ## Number of observations 250 ## ## Model Test User Model: ## ## Test statistic 27.890 ## Degrees of freedom 18 ## P-value (Chi-square) 0.064 ## ## Model Test Baseline Model: ## ## Test statistic 1089.407 ## Degrees of freedom 28 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.991 ## Tucker-Lewis Index (TLI) 0.986 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -2533.616 ## Loglikelihood unrestricted model (H1) -2519.671 ## ## Akaike (AIC) 5103.232 ## Bayesian (BIC) 5166.618 ## Sample-size adjusted Bayesian (BIC) 5109.557 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.047 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.079 ## P-value RMSEA &lt;= 0.05 0.523 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.036 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## attitudes =~ ## attit_1 1.000 ## attit_2 1.039 0.068 15.365 0.000 ## attit_3 -1.002 0.067 -14.850 0.000 ## norms =~ ## norm_1 1.000 ## norm_2 0.983 0.087 11.333 0.000 ## norm_3 0.935 0.087 10.778 0.000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## intent ~ ## attitudes 0.439 0.063 6.990 0.000 ## norms 0.693 0.077 8.977 0.000 ## behavior ~ ## intent 0.746 0.045 16.443 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## attitudes ~~ ## norms 0.347 0.069 5.027 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .attit_1 0.420 0.052 8.103 0.000 ## .attit_2 0.306 0.046 6.604 0.000 ## .attit_3 0.372 0.049 7.651 0.000 ## .norm_1 0.483 0.064 7.581 0.000 ## .norm_2 0.521 0.065 7.954 0.000 ## .norm_3 0.610 0.070 8.713 0.000 ## .intent 0.423 0.048 8.769 0.000 ## .behavior 0.603 0.054 11.180 0.000 ## attitudes 0.884 0.116 7.614 0.000 ## norms 0.765 0.113 6.767 0.000 ## ## R-Square: ## Estimate ## attit_1 0.678 ## attit_2 0.757 ## attit_3 0.705 ## norm_1 0.613 ## norm_2 0.587 ## norm_3 0.523 ## intent 0.639 ## behavior 0.520 Yes, the model still fits the data very well. Yes, the estimates all align with the TORA. Specifically, attitudes and norms both significantly predict intention, and intention significantly predicts condom use. The model explains 63.93% of the variance in intention and 51.96% of the variance in condom use. 6.1.4 Update your model to represent the extended TORA model that includes perceived behavioral control. Regress condom use onto perceived behavioral control. Use the latent variable representation of control. Covary all three exogenous latent factors. Does the model fit well? Do the estimates align with the updated TORA? How much variance in intention and condom use are explained by the model? Click for explanation mod_tora &lt;- &#39; attitudes =~ attit_1 + attit_2 + attit_3 norms =~ norm_1 + norm_2 + norm_3 control =~ control_1 + control_2 + control_3 intent ~ attitudes + norms behavior ~ intent + control &#39; fit_tora &lt;- sem(mod_tora, data = condom) summary(fit_tora, fit.measures = TRUE, rsquare = TRUE) ## lavaan 0.6-12 ended normally after 31 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 27 ## ## Number of observations 250 ## ## Model Test User Model: ## ## Test statistic 48.757 ## Degrees of freedom 39 ## P-value (Chi-square) 0.136 ## ## Model Test Baseline Model: ## ## Test statistic 1333.695 ## Degrees of freedom 55 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.992 ## Tucker-Lewis Index (TLI) 0.989 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -3551.160 ## Loglikelihood unrestricted model (H1) -3526.782 ## ## Akaike (AIC) 7156.320 ## Bayesian (BIC) 7251.400 ## Sample-size adjusted Bayesian (BIC) 7165.807 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.032 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.057 ## P-value RMSEA &lt;= 0.05 0.870 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.033 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## attitudes =~ ## attit_1 1.000 ## attit_2 1.033 0.068 15.221 0.000 ## attit_3 -1.025 0.068 -15.097 0.000 ## norms =~ ## norm_1 1.000 ## norm_2 0.984 0.087 11.256 0.000 ## norm_3 0.955 0.088 10.881 0.000 ## control =~ ## control_1 1.000 ## control_2 0.859 0.127 6.789 0.000 ## control_3 0.997 0.131 7.609 0.000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## intent ~ ## attitudes 0.447 0.063 7.100 0.000 ## norms 0.706 0.078 9.078 0.000 ## behavior ~ ## intent 0.563 0.063 8.923 0.000 ## control 0.454 0.119 3.805 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## attitudes ~~ ## norms 0.342 0.068 5.011 0.000 ## control 0.474 0.072 6.548 0.000 ## norms ~~ ## control 0.352 0.064 5.521 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .attit_1 0.432 0.052 8.381 0.000 ## .attit_2 0.330 0.046 7.220 0.000 ## .attit_3 0.344 0.046 7.439 0.000 ## .norm_1 0.496 0.063 7.820 0.000 ## .norm_2 0.533 0.065 8.152 0.000 ## .norm_3 0.595 0.069 8.643 0.000 ## .control_1 0.625 0.075 8.372 0.000 ## .control_2 0.876 0.090 9.757 0.000 ## .control_3 0.746 0.084 8.874 0.000 ## .intent 0.409 0.047 8.769 0.000 ## .behavior 0.542 0.052 10.423 0.000 ## attitudes 0.872 0.115 7.566 0.000 ## norms 0.751 0.112 6.709 0.000 ## control 0.485 0.096 5.059 0.000 ## ## R-Square: ## Estimate ## attit_1 0.668 ## attit_2 0.738 ## attit_3 0.727 ## norm_1 0.602 ## norm_2 0.577 ## norm_3 0.535 ## control_1 0.437 ## control_2 0.290 ## control_3 0.392 ## intent 0.651 ## behavior 0.566 Yes, the model still fits the data very well. Yes, the estimates all align with the updated TORA. Specifically, attitudes and norms both significantly predict intention, while intention and control both significantly predict condom use. The model explains 65.11% of the variance in intention and 56.62% of the variance in condom use. The TORA model explicitly forbids direct paths from attitudes and norms to behaviors; these effects should be fully mediated by the behavioral intention. The theory does not specify how perceived behavioral control should affect behaviors. There may be a direct effect of control on behavior, or the effect may be (partially) mediated by intention. 6.1.5 Evaluate the hypothesized indirect effects of attitudes and norms. Include attitudes, norms, and control in your model as in 6.1.4. Does intention significantly mediate the effects of attitudes and norms on behavior? Don’t forget to follow all the steps we covered for testing mediation. Are both of the above effects completely mediated? Do these results comport with the TORA? Why or why not? Click for explanation mod &lt;- &#39; attitudes =~ attit_1 + attit_2 + attit_3 norms =~ norm_1 + norm_2 + norm_3 control =~ control_1 + control_2 + control_3 intent ~ a1 * attitudes + a2 * norms behavior ~ b * intent + control + attitudes + norms ie_att := a1 * b ie_norm := a2 * b &#39; set.seed(235711) fit &lt;- sem(mod, data = condom, se = &quot;bootstrap&quot;, bootstrap = 1000) summary(fit, ci = TRUE) ## lavaan 0.6-12 ended normally after 36 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 29 ## ## Number of observations 250 ## ## Model Test User Model: ## ## Test statistic 48.629 ## Degrees of freedom 37 ## P-value (Chi-square) 0.096 ## ## Parameter Estimates: ## ## Standard errors Bootstrap ## Number of requested bootstrap draws 1000 ## Number of successful bootstrap draws 1000 ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## attitudes =~ ## attit_1 1.000 1.000 1.000 ## attit_2 1.033 0.058 17.837 0.000 0.913 1.148 ## attit_3 -1.025 0.062 -16.588 0.000 -1.149 -0.910 ## norms =~ ## norm_1 1.000 1.000 1.000 ## norm_2 0.984 0.070 14.140 0.000 0.846 1.123 ## norm_3 0.955 0.093 10.255 0.000 0.789 1.160 ## control =~ ## control_1 1.000 1.000 1.000 ## control_2 0.860 0.118 7.264 0.000 0.656 1.128 ## control_3 0.996 0.144 6.903 0.000 0.746 1.293 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## intent ~ ## attitudes (a1) 0.447 0.064 6.985 0.000 0.332 0.580 ## norms (a2) 0.706 0.076 9.337 0.000 0.576 0.873 ## behavior ~ ## intent (b) 0.545 0.073 7.477 0.000 0.402 0.693 ## control 0.428 0.243 1.758 0.079 0.055 0.997 ## attitudes 0.010 0.125 0.082 0.935 -0.262 0.233 ## norms 0.041 0.115 0.354 0.724 -0.200 0.266 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## attitudes ~~ ## norms 0.342 0.070 4.907 0.000 0.213 0.486 ## control 0.475 0.066 7.223 0.000 0.346 0.603 ## norms ~~ ## control 0.350 0.062 5.673 0.000 0.233 0.472 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .attit_1 0.432 0.049 8.774 0.000 0.330 0.532 ## .attit_2 0.330 0.045 7.285 0.000 0.241 0.415 ## .attit_3 0.343 0.049 6.961 0.000 0.244 0.436 ## .norm_1 0.496 0.057 8.720 0.000 0.383 0.609 ## .norm_2 0.533 0.078 6.813 0.000 0.386 0.700 ## .norm_3 0.594 0.067 8.921 0.000 0.464 0.733 ## .control_1 0.624 0.074 8.479 0.000 0.473 0.764 ## .control_2 0.875 0.095 9.214 0.000 0.686 1.064 ## .control_3 0.745 0.080 9.369 0.000 0.578 0.889 ## .intent 0.409 0.050 8.161 0.000 0.310 0.503 ## .behavior 0.544 0.059 9.291 0.000 0.407 0.637 ## attitudes 0.872 0.100 8.697 0.000 0.685 1.077 ## norms 0.751 0.097 7.714 0.000 0.564 0.940 ## control 0.486 0.096 5.065 0.000 0.311 0.682 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## ie_att 0.244 0.050 4.849 0.000 0.154 0.349 ## ie_norm 0.385 0.063 6.075 0.000 0.269 0.517 Yes, both indirect effects are significant according to the 95% bootstrapped CIs. Yes, both effects are completely moderated by behavioral intention. We can infer as much because the direct effects of attitudes and norms on condom use are both nonsignificant. Yes, these results comport with the TORA. Both effects are fully mediated, as the theory stipulates. In addition to evaluating the significance of the indirect and direct effects, we can also take a model-comparison perspective. We can use model comparisons to test if removing the direct effects of attitudes and norms on condom use significantly decreases model fit. In other words, are those paths needed to accurately represent the data, or are they “dead weight”. 6.1.6 Use a \\(\\Delta \\chi^2\\) test to evaluate the necessity of including the direct effects of attitudes and norms on condom use in the model. What is your conclusion? Click for explanation We only need to compare the fit of the model with the direct effects included to the fit of the model without the direct effects. We’ve already estimated both models, so we can simply submit the fitted lavaan objects to the anova() function. anova(fit, fit_tora) The \\(\\Delta \\chi^2\\) test is not significant. So, we have not lost a significant amount of fit by fixing the direct effects to zero. In other words, the complete mediation model explains the data just as well as the partial mediation model. So, we should probably prefer the more parsimonious model. 6.1.7 Use some statistical means of evaluating the most plausible way to include perceived behavioral control into the model. Choose between the following three options: control predicts behavior via a direct, un-mediated effect. control predicts behavior via an indirect effect that is completely mediated by intention. control predicts behavior via both an indirect effect through intention and a residual direct effect. Hint: There is more than one way to approach this problem. Click for explanation We can tackle this problem in a few different ways. One possibility entails testing the indirect, direct, and total effects. ## Allow for partial mediation: mod1 &lt;- &#39; attitudes =~ attit_1 + attit_2 + attit_3 norms =~ norm_1 + norm_2 + norm_3 control =~ control_1 + control_2 + control_3 intent ~ attitudes + norms + a * control behavior ~ b * intent + c * control ie := a * b total := ie + c &#39; set.seed(235711) fit1 &lt;- sem(mod1, data = condom, se = &quot;bootstrap&quot;, bootstrap = 1000) summary(fit1, ci = TRUE) ## lavaan 0.6-12 ended normally after 33 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 28 ## ## Number of observations 250 ## ## Model Test User Model: ## ## Test statistic 47.389 ## Degrees of freedom 38 ## P-value (Chi-square) 0.141 ## ## Parameter Estimates: ## ## Standard errors Bootstrap ## Number of requested bootstrap draws 1000 ## Number of successful bootstrap draws 1000 ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## attitudes =~ ## attit_1 1.000 1.000 1.000 ## attit_2 1.034 0.058 17.748 0.000 0.914 1.149 ## attit_3 -1.021 0.062 -16.552 0.000 -1.142 -0.907 ## norms =~ ## norm_1 1.000 1.000 1.000 ## norm_2 0.985 0.070 14.123 0.000 0.848 1.123 ## norm_3 0.948 0.092 10.297 0.000 0.788 1.147 ## control =~ ## control_1 1.000 1.000 1.000 ## control_2 0.861 0.119 7.233 0.000 0.654 1.126 ## control_3 0.996 0.139 7.143 0.000 0.749 1.288 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## intent ~ ## attitudes 0.357 0.116 3.066 0.002 0.118 0.596 ## norms 0.646 0.094 6.843 0.000 0.470 0.851 ## control (a) 0.199 0.205 0.974 0.330 -0.181 0.618 ## behavior ~ ## intent (b) 0.551 0.073 7.543 0.000 0.392 0.685 ## control (c) 0.469 0.148 3.174 0.002 0.188 0.811 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## attitudes ~~ ## norms 0.344 0.070 4.908 0.000 0.214 0.490 ## control 0.471 0.066 7.190 0.000 0.339 0.604 ## norms ~~ ## control 0.345 0.061 5.651 0.000 0.230 0.469 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .attit_1 0.429 0.049 8.717 0.000 0.328 0.527 ## .attit_2 0.325 0.045 7.202 0.000 0.236 0.410 ## .attit_3 0.347 0.050 7.003 0.000 0.250 0.440 ## .norm_1 0.490 0.057 8.658 0.000 0.377 0.603 ## .norm_2 0.525 0.078 6.698 0.000 0.377 0.693 ## .norm_3 0.599 0.068 8.835 0.000 0.466 0.736 ## .control_1 0.626 0.072 8.746 0.000 0.474 0.758 ## .control_2 0.875 0.095 9.243 0.000 0.688 1.068 ## .control_3 0.748 0.079 9.493 0.000 0.584 0.894 ## .intent 0.412 0.049 8.437 0.000 0.305 0.499 ## .behavior 0.541 0.055 9.847 0.000 0.427 0.644 ## attitudes 0.875 0.101 8.688 0.000 0.687 1.083 ## norms 0.757 0.097 7.792 0.000 0.568 0.943 ## control 0.484 0.094 5.139 0.000 0.310 0.681 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## ie 0.110 0.107 1.025 0.305 -0.110 0.321 ## total 0.578 0.194 2.979 0.003 0.233 1.000 From the above results, we can see that the direct and total effects are both significant, but the indirect effect is not. Hence, it probably makes the most sense to include control via a direct (non-mediated) effect on behavior. We can also approach this problem from a model-comparison perspective. We can fit models that encode each pattern of constraints and check which one best represents the data. ## Force complete mediation: mod2 &lt;- &#39; attitudes =~ attit_1 + attit_2 + attit_3 norms =~ norm_1 + norm_2 + norm_3 control =~ control_1 + control_2 + control_3 intent ~ attitudes + norms + control behavior ~ intent &#39; ## Force no mediation: mod3 &lt;- &#39; attitudes =~ attit_1 + attit_2 + attit_3 norms =~ norm_1 + norm_2 + norm_3 control =~ control_1 + control_2 + control_3 intent ~ attitudes + norms behavior ~ intent + control &#39; ## Estimate the two restricted models: fit2 &lt;- sem(mod2, data = condom) fit3 &lt;- sem(mod3, data = condom) ## Check the results: summary(fit2) ## lavaan 0.6-12 ended normally after 33 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 27 ## ## Number of observations 250 ## ## Model Test User Model: ## ## Test statistic 62.797 ## Degrees of freedom 39 ## P-value (Chi-square) 0.009 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## attitudes =~ ## attit_1 1.000 ## attit_2 1.033 0.068 15.295 0.000 ## attit_3 -1.018 0.068 -15.087 0.000 ## norms =~ ## norm_1 1.000 ## norm_2 0.985 0.087 11.305 0.000 ## norm_3 0.947 0.087 10.845 0.000 ## control =~ ## control_1 1.000 ## control_2 0.864 0.126 6.855 0.000 ## control_3 0.958 0.129 7.417 0.000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## intent ~ ## attitudes 0.352 0.096 3.669 0.000 ## norms 0.644 0.088 7.347 0.000 ## control 0.207 0.163 1.268 0.205 ## behavior ~ ## intent 0.746 0.045 16.443 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## attitudes ~~ ## norms 0.345 0.069 5.023 0.000 ## control 0.476 0.073 6.513 0.000 ## norms ~~ ## control 0.346 0.065 5.361 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .attit_1 0.427 0.051 8.295 0.000 ## .attit_2 0.325 0.046 7.101 0.000 ## .attit_3 0.349 0.047 7.477 0.000 ## .norm_1 0.490 0.064 7.702 0.000 ## .norm_2 0.524 0.065 8.025 0.000 ## .norm_3 0.600 0.069 8.652 0.000 ## .control_1 0.610 0.076 8.015 0.000 ## .control_2 0.861 0.090 9.580 0.000 ## .control_3 0.769 0.086 8.938 0.000 ## .intent 0.412 0.046 8.890 0.000 ## .behavior 0.603 0.054 11.180 0.000 ## attitudes 0.877 0.115 7.596 0.000 ## norms 0.757 0.112 6.733 0.000 ## control 0.500 0.098 5.076 0.000 summary(fit3) ## lavaan 0.6-12 ended normally after 31 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 27 ## ## Number of observations 250 ## ## Model Test User Model: ## ## Test statistic 48.757 ## Degrees of freedom 39 ## P-value (Chi-square) 0.136 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## attitudes =~ ## attit_1 1.000 ## attit_2 1.033 0.068 15.221 0.000 ## attit_3 -1.025 0.068 -15.097 0.000 ## norms =~ ## norm_1 1.000 ## norm_2 0.984 0.087 11.256 0.000 ## norm_3 0.955 0.088 10.881 0.000 ## control =~ ## control_1 1.000 ## control_2 0.859 0.127 6.789 0.000 ## control_3 0.997 0.131 7.609 0.000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## intent ~ ## attitudes 0.447 0.063 7.100 0.000 ## norms 0.706 0.078 9.078 0.000 ## behavior ~ ## intent 0.563 0.063 8.923 0.000 ## control 0.454 0.119 3.805 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## attitudes ~~ ## norms 0.342 0.068 5.011 0.000 ## control 0.474 0.072 6.548 0.000 ## norms ~~ ## control 0.352 0.064 5.521 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .attit_1 0.432 0.052 8.381 0.000 ## .attit_2 0.330 0.046 7.220 0.000 ## .attit_3 0.344 0.046 7.439 0.000 ## .norm_1 0.496 0.063 7.820 0.000 ## .norm_2 0.533 0.065 8.152 0.000 ## .norm_3 0.595 0.069 8.643 0.000 ## .control_1 0.625 0.075 8.372 0.000 ## .control_2 0.876 0.090 9.757 0.000 ## .control_3 0.746 0.084 8.874 0.000 ## .intent 0.409 0.047 8.769 0.000 ## .behavior 0.542 0.052 10.423 0.000 ## attitudes 0.872 0.115 7.566 0.000 ## norms 0.751 0.112 6.709 0.000 ## control 0.485 0.096 5.059 0.000 ## Do either of the restricted models fit worse than the partial mediation model? anova(fit1, fit2) anova(fit1, fit3) The above \\(\\Delta \\chi^2\\) tests tell us that the full mediation model fits significantly worse than the partial mediation model. Hence, forcing full mediation by fixing the direct effect to zero is an unreasonable restraint. The total effect model, on the other hand, does not fit significantly worse than the partial mediation model. So, we can conclude that removing the indirect effect and modeling the influence of control on behavior as an un-mediated direct association represents the data just as well as a model that allows for both indirect and direct effects. Hence, we should prefer the more parsimonious total effects model. While the two tests above lead us to prefer the non-mediated model, we cannot directly say that the the complete mediation model fits significantly worse than the non-mediated model. We have not directly compared those two models, and we cannot do so with the \\(\\Delta \\chi^2\\). We cannot do such a test because these two models are not nested: we must both add and remove a path to get from one model specification to the other. Also, both models have the same degrees of freedom, so we cannot define a sampling distribution against which we would compare the \\(\\Delta \\chi^2\\), anyway. We are not completely without options, though. We can use information criteria to compare non-nested models. The two most popular information criteria are the Akaike’s Information Criterion (AIC) and the Bayesian Information Criterion (BIC). ## Which model is the most parsimonious representation of the data? AIC(fit1, fit2, fit3) BIC(fit1, fit2, fit3) When comparing models based on information criteria, a lower value indicates a better model in the sense of a better balance of fit and parsimony. The above results show that both the AIC and the BIC agree that the no-mediation model is the best. So, in the end, regardless of how we approach the question, all of our results suggest modeling perceived behavioral control as a direct, non-mediated predictor of condom use. Up to this point, we’ve implemented multiple group models simply by specifying some grouping factor in the lavaan fitting function and proceeding with our primary analysis (e.g., testing for moderation by group). When we have latent variables in our model, however, we get a new opportunity: we can test measurement invariance. Basically, measurement invariance testing allows us to empirically test for differences in the measurement model between the groups. If we can establish measurement invariance, we can draw the following (equivalent) conclusions: Our latent constructs are defined equivalently in all groups. The participants in every group are interpreting our items in the same way. Participants in different groups who have the same values for the observed indicators will also have the same score on the latent variable. Between-group differences in latent parameters are due to true differences in the underlying latent constructs and not caused by differences in measurement. Anytime we make between-group comparisons (e.g., ANOVA, t-tests, moderation by group, etc.) we assume invariant measurement. That is, we assume that the scores we’re comparing have the same meaning in each group. When doing multiple group SEM, however, we’re apprised of the incredibly powerful capability of actually testing this—very important, and often violated—assumption. The process of testing measurement invariance can get quite complex, but the basic procedure boils down to using model comparison tests to evaluate the plausibility of increasingly strong between-group constraints. For most problems, these constraints amount to the following three levels: Configural: The same pattern of free and fixed effects in all groups Weak (aka Metric): Configural + Equal factor loadings in all groups Strong (aka Scalar): Weak + Equal item intercepts in all groups You can read more about measurement invariance here and here, and you can find a brief discussion of how to conduct measurement invariance tests in lavaan here. 6.1.8 Test for measurement invariance across sex groups in the three latent variables of the TORA model above. Test configural, weak, and strong invariance. Test for invariance in all three latent factors simultaneously. Is full measurement invariance (i.e., up to and including strong invariance) supported? Click for explanation ## Estimate the models: config &lt;- cfa(mod_cfa, data = condom, group = &quot;sex&quot;) weak &lt;- cfa(mod_cfa, data = condom, group = &quot;sex&quot;, group.equal = &quot;loadings&quot;) strong &lt;- cfa(mod_cfa, data = condom, group = &quot;sex&quot;, group.equal = c(&quot;loadings&quot;, &quot;intercepts&quot;) ) ## Check that everything went well: summary(config, fit.measures = TRUE) ## lavaan 0.6-12 ended normally after 54 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 60 ## ## Number of observations per group: ## woman 161 ## man 89 ## ## Model Test User Model: ## ## Test statistic 66.565 ## Degrees of freedom 48 ## P-value (Chi-square) 0.039 ## Test statistic for each group: ## woman 42.623 ## man 23.941 ## ## Model Test Baseline Model: ## ## Test statistic 948.362 ## Degrees of freedom 72 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.979 ## Tucker-Lewis Index (TLI) 0.968 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -2950.738 ## Loglikelihood unrestricted model (H1) -2917.456 ## ## Akaike (AIC) 6021.476 ## Bayesian (BIC) 6232.764 ## Sample-size adjusted Bayesian (BIC) 6042.559 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.056 ## 90 Percent confidence interval - lower 0.013 ## 90 Percent confidence interval - upper 0.086 ## P-value RMSEA &lt;= 0.05 0.366 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.048 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## ## Group 1 [woman]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## attitudes =~ ## attit_1 1.000 ## attit_2 1.005 0.075 13.427 0.000 ## attit_3 -0.965 0.075 -12.878 0.000 ## norms =~ ## norm_1 1.000 ## norm_2 0.952 0.101 9.470 0.000 ## norm_3 0.879 0.101 8.742 0.000 ## control =~ ## control_1 1.000 ## control_2 0.794 0.144 5.526 0.000 ## control_3 0.989 0.152 6.523 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## attitudes ~~ ## norms 0.450 0.087 5.200 0.000 ## control 0.468 0.089 5.249 0.000 ## norms ~~ ## control 0.387 0.079 4.912 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .attit_1 2.839 0.090 31.702 0.000 ## .attit_2 2.907 0.084 34.728 0.000 ## .attit_3 3.174 0.084 37.969 0.000 ## .norm_1 2.832 0.080 35.342 0.000 ## .norm_2 2.832 0.079 35.775 0.000 ## .norm_3 2.795 0.081 34.694 0.000 ## .control_1 2.851 0.082 34.755 0.000 ## .control_2 2.857 0.081 35.104 0.000 ## .control_3 2.888 0.081 35.877 0.000 ## attitudes 0.000 ## norms 0.000 ## control 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .attit_1 0.398 0.059 6.739 0.000 ## .attit_2 0.227 0.045 5.011 0.000 ## .attit_3 0.294 0.048 6.092 0.000 ## .norm_1 0.346 0.065 5.328 0.000 ## .norm_2 0.385 0.065 5.962 0.000 ## .norm_3 0.513 0.072 7.108 0.000 ## .control_1 0.587 0.090 6.531 0.000 ## .control_2 0.754 0.096 7.815 0.000 ## .control_3 0.557 0.086 6.453 0.000 ## attitudes 0.893 0.142 6.273 0.000 ## norms 0.688 0.121 5.706 0.000 ## control 0.497 0.119 4.184 0.000 ## ## ## Group 2 [man]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## attitudes =~ ## attit_1 1.000 ## attit_2 1.167 0.149 7.843 0.000 ## attit_3 -1.060 0.142 -7.443 0.000 ## norms =~ ## norm_1 1.000 ## norm_2 1.070 0.215 4.965 0.000 ## norm_3 0.922 0.189 4.869 0.000 ## control =~ ## control_1 1.000 ## control_2 0.995 0.290 3.435 0.001 ## control_3 0.949 0.285 3.332 0.001 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## attitudes ~~ ## norms 0.086 0.103 0.837 0.403 ## control 0.388 0.113 3.430 0.001 ## norms ~~ ## control 0.200 0.101 1.976 0.048 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .attit_1 3.270 0.117 28.063 0.000 ## .attit_2 3.180 0.128 24.905 0.000 ## .attit_3 2.787 0.126 22.187 0.000 ## .norm_1 3.236 0.131 24.692 0.000 ## .norm_2 3.337 0.132 25.293 0.000 ## .norm_3 3.303 0.131 25.136 0.000 ## .control_1 3.157 0.111 28.415 0.000 ## .control_2 3.135 0.129 24.249 0.000 ## .control_3 3.213 0.130 24.805 0.000 ## attitudes 0.000 ## norms 0.000 ## control 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .attit_1 0.440 0.095 4.631 0.000 ## .attit_2 0.405 0.109 3.699 0.000 ## .attit_3 0.541 0.112 4.822 0.000 ## .norm_1 0.740 0.175 4.230 0.000 ## .norm_2 0.647 0.182 3.555 0.000 ## .norm_3 0.868 0.175 4.972 0.000 ## .control_1 0.673 0.146 4.602 0.000 ## .control_2 1.066 0.197 5.417 0.000 ## .control_3 1.110 0.199 5.582 0.000 ## attitudes 0.768 0.182 4.220 0.000 ## norms 0.788 0.242 3.259 0.001 ## control 0.426 0.168 2.537 0.011 summary(weak) ## lavaan 0.6-12 ended normally after 37 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 60 ## Number of equality constraints 6 ## ## Number of observations per group: ## woman 161 ## man 89 ## ## Model Test User Model: ## ## Test statistic 68.557 ## Degrees of freedom 54 ## P-value (Chi-square) 0.088 ## Test statistic for each group: ## woman 43.148 ## man 25.409 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## ## Group 1 [woman]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## attitudes =~ ## attit_1 1.000 ## attit_2 (.p2.) 1.048 0.068 15.413 0.000 ## attit_3 (.p3.) -0.995 0.067 -14.762 0.000 ## norms =~ ## norm_1 1.000 ## norm_2 (.p5.) 0.977 0.091 10.708 0.000 ## norm_3 (.p6.) 0.889 0.089 9.996 0.000 ## control =~ ## cntrl_1 1.000 ## cntrl_2 (.p8.) 0.843 0.130 6.506 0.000 ## cntrl_3 (.p9.) 0.983 0.135 7.306 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## attitudes ~~ ## norms 0.431 0.082 5.256 0.000 ## control 0.450 0.083 5.395 0.000 ## norms ~~ ## control 0.378 0.075 5.031 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .attit_1 2.839 0.088 32.194 0.000 ## .attit_2 2.907 0.084 34.445 0.000 ## .attit_3 3.174 0.084 37.913 0.000 ## .norm_1 2.832 0.080 35.504 0.000 ## .norm_2 2.832 0.080 35.571 0.000 ## .norm_3 2.795 0.080 34.727 0.000 ## .control_1 2.851 0.082 34.882 0.000 ## .control_2 2.857 0.082 34.746 0.000 ## .control_3 2.888 0.080 36.037 0.000 ## attitudes 0.000 ## norms 0.000 ## control 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .attit_1 0.408 0.058 6.987 0.000 ## .attit_2 0.221 0.045 4.896 0.000 ## .attit_3 0.293 0.048 6.128 0.000 ## .norm_1 0.353 0.063 5.580 0.000 ## .norm_2 0.380 0.064 5.952 0.000 ## .norm_3 0.512 0.071 7.178 0.000 ## .control_1 0.590 0.088 6.695 0.000 ## .control_2 0.744 0.096 7.731 0.000 ## .control_3 0.565 0.085 6.663 0.000 ## attitudes 0.844 0.129 6.540 0.000 ## norms 0.672 0.113 5.921 0.000 ## control 0.485 0.110 4.417 0.000 ## ## ## Group 2 [man]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## attitudes =~ ## attit_1 1.000 ## attit_2 (.p2.) 1.048 0.068 15.413 0.000 ## attit_3 (.p3.) -0.995 0.067 -14.762 0.000 ## norms =~ ## norm_1 1.000 ## norm_2 (.p5.) 0.977 0.091 10.708 0.000 ## norm_3 (.p6.) 0.889 0.089 9.996 0.000 ## control =~ ## cntrl_1 1.000 ## cntrl_2 (.p8.) 0.843 0.130 6.506 0.000 ## cntrl_3 (.p9.) 0.983 0.135 7.306 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## attitudes ~~ ## norms 0.092 0.114 0.807 0.420 ## control 0.425 0.109 3.912 0.000 ## norms ~~ ## control 0.217 0.103 2.100 0.036 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .attit_1 3.270 0.120 27.254 0.000 ## .attit_2 3.180 0.125 25.501 0.000 ## .attit_3 2.787 0.125 22.275 0.000 ## .norm_1 3.236 0.132 24.423 0.000 ## .norm_2 3.337 0.130 25.610 0.000 ## .norm_3 3.303 0.132 25.086 0.000 ## .control_1 3.157 0.112 28.208 0.000 ## .control_2 3.135 0.127 24.750 0.000 ## .control_3 3.213 0.131 24.540 0.000 ## attitudes 0.000 ## norms 0.000 ## control 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .attit_1 0.419 0.093 4.528 0.000 ## .attit_2 0.438 0.099 4.436 0.000 ## .attit_3 0.540 0.107 5.057 0.000 ## .norm_1 0.704 0.158 4.456 0.000 ## .norm_2 0.692 0.153 4.520 0.000 ## .norm_3 0.864 0.164 5.271 0.000 ## .control_1 0.668 0.139 4.797 0.000 ## .control_2 1.110 0.186 5.960 0.000 ## .control_3 1.094 0.193 5.663 0.000 ## attitudes 0.862 0.166 5.200 0.000 ## norms 0.859 0.193 4.443 0.000 ## control 0.447 0.137 3.260 0.001 summary(strong) ## lavaan 0.6-12 ended normally after 60 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 63 ## Number of equality constraints 15 ## ## Number of observations per group: ## woman 161 ## man 89 ## ## Model Test User Model: ## ## Test statistic 72.050 ## Degrees of freedom 60 ## P-value (Chi-square) 0.137 ## Test statistic for each group: ## woman 43.961 ## man 28.089 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## ## Group 1 [woman]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## attitudes =~ ## attit_1 1.000 ## attit_2 (.p2.) 1.028 0.065 15.693 0.000 ## attit_3 (.p3.) -0.990 0.065 -15.114 0.000 ## norms =~ ## norm_1 1.000 ## norm_2 (.p5.) 0.998 0.089 11.182 0.000 ## norm_3 (.p6.) 0.918 0.088 10.467 0.000 ## control =~ ## cntrl_1 1.000 ## cntrl_2 (.p8.) 0.848 0.126 6.736 0.000 ## cntrl_3 (.p9.) 0.987 0.131 7.558 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## attitudes ~~ ## norms 0.428 0.081 5.259 0.000 ## control 0.454 0.084 5.438 0.000 ## norms ~~ ## control 0.372 0.073 5.060 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .attit_1 (.25.) 2.864 0.085 33.535 0.000 ## .attit_2 (.26.) 2.887 0.083 34.826 0.000 ## .attit_3 (.27.) 3.166 0.082 38.500 0.000 ## .norm_1 (.28.) 2.816 0.078 36.330 0.000 ## .norm_2 (.29.) 2.838 0.078 36.453 0.000 ## .norm_3 (.30.) 2.812 0.078 36.253 0.000 ## .cntrl_1 (.31.) 2.847 0.078 36.562 0.000 ## .cntrl_2 (.32.) 2.859 0.076 37.381 0.000 ## .cntrl_3 (.33.) 2.891 0.077 37.531 0.000 ## attitds 0.000 ## norms 0.000 ## control 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .attit_1 0.405 0.058 6.931 0.000 ## .attit_2 0.226 0.045 5.051 0.000 ## .attit_3 0.291 0.048 6.084 0.000 ## .norm_1 0.362 0.062 5.795 0.000 ## .norm_2 0.377 0.064 5.931 0.000 ## .norm_3 0.506 0.071 7.109 0.000 ## .control_1 0.592 0.088 6.743 0.000 ## .control_2 0.743 0.096 7.739 0.000 ## .control_3 0.566 0.085 6.684 0.000 ## attitudes 0.861 0.130 6.607 0.000 ## norms 0.650 0.109 5.950 0.000 ## control 0.482 0.108 4.477 0.000 ## ## ## Group 2 [man]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## attitudes =~ ## attit_1 1.000 ## attit_2 (.p2.) 1.028 0.065 15.693 0.000 ## attit_3 (.p3.) -0.990 0.065 -15.114 0.000 ## norms =~ ## norm_1 1.000 ## norm_2 (.p5.) 0.998 0.089 11.182 0.000 ## norm_3 (.p6.) 0.918 0.088 10.467 0.000 ## control =~ ## cntrl_1 1.000 ## cntrl_2 (.p8.) 0.848 0.126 6.736 0.000 ## cntrl_3 (.p9.) 0.987 0.131 7.558 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## attitudes ~~ ## norms 0.093 0.113 0.825 0.409 ## control 0.428 0.109 3.926 0.000 ## norms ~~ ## control 0.213 0.101 2.102 0.036 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .attit_1 (.25.) 2.864 0.085 33.535 0.000 ## .attit_2 (.26.) 2.887 0.083 34.826 0.000 ## .attit_3 (.27.) 3.166 0.082 38.500 0.000 ## .norm_1 (.28.) 2.816 0.078 36.330 0.000 ## .norm_2 (.29.) 2.838 0.078 36.453 0.000 ## .norm_3 (.30.) 2.812 0.078 36.253 0.000 ## .cntrl_1 (.31.) 2.847 0.078 36.562 0.000 ## .cntrl_2 (.32.) 2.859 0.076 37.381 0.000 ## .cntrl_3 (.33.) 2.891 0.077 37.531 0.000 ## attitds 0.356 0.133 2.680 0.007 ## norms 0.480 0.133 3.602 0.000 ## control 0.318 0.116 2.733 0.006 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .attit_1 0.420 0.094 4.484 0.000 ## .attit_2 0.456 0.100 4.557 0.000 ## .attit_3 0.537 0.107 5.023 0.000 ## .norm_1 0.724 0.157 4.599 0.000 ## .norm_2 0.686 0.153 4.489 0.000 ## .norm_3 0.859 0.165 5.220 0.000 ## .control_1 0.669 0.139 4.821 0.000 ## .control_2 1.109 0.186 5.958 0.000 ## .control_3 1.094 0.193 5.664 0.000 ## attitudes 0.872 0.167 5.214 0.000 ## norms 0.830 0.186 4.455 0.000 ## control 0.445 0.136 3.280 0.001 ## Test measurement invariance: anova(config, weak, strong) ## Make sure the strongly invariant model still fits well in an absolute sense: fitMeasures(strong) ## npar fmin chisq df ## 48.000 0.144 72.050 60.000 ## pvalue baseline.chisq baseline.df baseline.pvalue ## 0.137 948.362 72.000 0.000 ## cfi tli nnfi rfi ## 0.986 0.983 0.983 0.909 ## nfi pnfi ifi rni ## 0.924 0.770 0.986 0.986 ## logl unrestricted.logl aic bic ## -2953.481 -2917.456 6002.962 6171.992 ## ntotal bic2 rmsea rmsea.ci.lower ## 250.000 6019.828 0.040 0.000 ## rmsea.ci.upper rmsea.pvalue rmr rmr_nomean ## 0.071 0.669 0.062 0.067 ## srmr srmr_bentler srmr_bentler_nomean crmr ## 0.051 0.051 0.055 0.056 ## crmr_nomean srmr_mplus srmr_mplus_nomean cn_05 ## 0.059 0.052 0.054 275.398 ## cn_01 gfi agfi pgfi ## 307.658 0.997 0.994 0.554 ## mfi ## 0.976 Yes, we have been able to establish full measurement invariance. The configurally invariant model fits the data well. The \\(\\Delta \\chi^2\\) tests support the tenability of the weak and strong invariance constraints. The strongly invariance model still fits the data well. Once you’ve established measurement invariance, you can move on to testing hypotheses about between-group differences secure in the knowledge that your latent factors represent the same hypothetical constructs in all groups. 6.1.9 Using a strongly invariant model, conduct an omnibus test to see if sex moderates any of the regression paths in the TORA model. What do you conclude. Click for explanation fit_full &lt;- sem(mod_tora, data = condom, group = &quot;sex&quot;, group.equal = c(&quot;loadings&quot;, &quot;intercepts&quot;) ) fit_res &lt;- sem(mod_tora, data = condom, group = &quot;sex&quot;, group.equal = c(&quot;loadings&quot;, &quot;intercepts&quot;, &quot;regressions&quot;) ) anova(fit_full, fit_res) Equating all regression paths across groups produces a significant loss of fit. Therefore, sex must moderate at least some of these paths. End of At-Home Exercises "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
