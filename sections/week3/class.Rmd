## In-Class Exercises

```{r include = FALSE}
library(haven)
library(lavaan)
library(tidySEM)
library(dplyr)
library(rockchalk)

dataDir <- "../../../../data/"
seData <- read_sav(paste0(dataDir, "SelfEsteem.sav"))
```

---

### Mediation

---

In this practical, we'll go back to the data from the at-home exercises,
*SelfEsteem.sav*. Recall that these data comprise 143 observations of the
following variables. 

- *case*: Participant ID number
- *ParAtt*: Parental Attachment
- *PeerAtt*: Peer Attachment
- *Emp*: Empathy
- *ProSoc*: Prosocial behavior
- *Aggr*: Aggression
- *SelfEst*: Self-esteem

When we last worked with the data, we built a model with one mediator (*Emp*),
creating indirect effects between our predictors *ParAtt* and *PeerAtt*, and our
outcome variable *SelfEst*. Below, you will estimate a more complex, 
multiple-mediator model.

---

#### 

Load the data into the object `seData` using `haven::read_sav()`

<details>
  <summary>Click to show code</summary>

```{r, eval = FALSE}
library(haven)
seData <- read_sav("SelfEsteem.sav")
```
</details>
---

For this analysis, we are interested in the (indirect) effects of parental and 
peer attachment on self-esteem. Furthermore, we want to evaluate the mediating 
roles of empathy and social behavior (i.e., prosocial behavior and aggression).

Specifically, we have the following hypotheses.

1. Better peer relationships will promote higher self-esteem via a three-step 
indirect process.
    a. Better peer relationships will increase empathy levels.
    a. Higher empathy will increase prosocial behavior and decrease aggressive 
    behavior.
    a. More prosocial behaviors and less aggressive behavior will both produce 
    higher self-esteem.
1. Better relationships with parents directly increase self-esteem.

---

To evaluate these hypotheses, we will use **lavaan** to estimate the following 
multiple mediator model as a path model. 

```{r, echo=FALSE}
modex <-
'
## Equation for outcome:
SelfEst ~ y_m21 * ProSoc + y_m22 * Aggr + Emp + ParAtt + PeerAtt

## Equations for stage 2 mediators:
ProSoc ~ m21_x2 * PeerAtt + ParAtt + m21_m1 * Emp
Aggr ~ m22_x2 * PeerAtt + ParAtt + m22_m1 * Emp

## Equation for stage 1 mediator:
Emp ~ ParAtt + m1_x2 * PeerAtt

## Covariances:
ProSoc ~~ Aggr
ParAtt ~~ PeerAtt
'

out <- sem(modex, data = seData)

prepare_graph(out,
              layout = get_layout(
                NA,        NA, NA,    "ProSoc", NA,
                "ParAtt",  NA, NA,    NA,      NA,
                NA,        NA, "Emp", NA,      "SelfEst",
                "PeerAtt", NA, NA,    NA,      NA,
                NA,        NA, NA,    "Aggr",  NA,
                rows = 5
              ),
              angle = 0,
              spacing_x = 2,
              text_size = 3,
              rect_height = 1.5) %>% 
  if_nodes(condition = {shape == "rect"},
           expr = {label = c("Aggressive\nBehavior\nM22", "Empathy\nM1", "Parent\nAttachment\nX1", "Peer\nAttachment\nX2", "Prosocial\nBehavior\nM21", "Self Esteem\nY")}) %>% 
  edit_edges({
    label <- NA
  }) %>%
  all_cov({
    arrow <- "both"
  }) %>% 
  hide_var() %>% 
  plot()  
```
<!-- ![](images/week5_home_full_model.png) -->

---

#### {#w3csyntax}

Specify the lavaan model syntax implied by the path diagram shown above.

- Save the resulting character string as an object in your environment.

<details>
  <summary>Click to show code</summary>

```{r}
mod0 <- '
## Equation for outcome:
SelfEst ~ ProSoc + Aggr + Emp + ParAtt + PeerAtt

## Equations for stage 2 mediators:
ProSoc ~ PeerAtt + ParAtt + Emp
Aggr ~ PeerAtt + ParAtt + Emp

## Equation for stage 1 mediator:
Emp ~ ParAtt + PeerAtt

## Covariances:
ProSoc ~~ Aggr
ParAtt ~~ PeerAtt
'
```

</details>

---

#### {#w3cfit}

Use the `lavaan::sem()` function to estimate the model defined in \@ref(w3csyntax).

- Use the default settings in `sem()`.
- Summarize the fitted model.

<details>
  <summary>Click to show code</summary>

```{r}
library(lavaan)
out <- sem(mod0, data = seData)
summary(out)
```
  
</details>

---

#### 

Considering the parameter estimates from \@ref(w3cfit), what can you say about
the hypotheses? 

<details>
  <summary>Click for explanation</summary>

```{r, include = FALSE}
tmp <- parameterEstimates(out)

parent <- tmp %>% filter(lhs == "SelfEst", op == "~", rhs == "ParAtt") 
peer   <- tmp %>% filter(lhs == "SelfEst", op == "~", rhs == "PeerAtt") 
```

Notice that all of the hypotheses stated above are explicitly directional. Hence,
when evaluating the significance of the structural paths that speak to these
hypotheses, we should use one-tailed tests. We cannot ask **lavaan** to return 
one-tailed p-values, but we have no need to do so. We can simply divide the 
two-tailed p-values in half.

The significant direct effect of `ParAtt` on `SelfEst` 
($\beta = `r parent$est %>% round(3)`$, 
$Z = `r parent$z %>% round(2)`$,
$p = `r (parent$p / 2) %>% round(3)`$) and the lack of a significant direct 
effect of `PeerAtt` on `SelfEst` ($\beta = `r peer$est %>% round(3)`$, 
$Z = `r peer$z %>% round(3)`$,
$p = `r (peer$p / 2) %>% round(3)`$) align with our hypotheses.

The remaining patterns of individual estimates also seem to conform to the 
hypotheses (e.g., all of the individual paths comprising the indirect effects of 
`PeerAtt` on `SelfEst` are significant). We cannot make any firm conclusions 
until we actually estimate and test the indirect effects, though.

</details>

---

####

Modify the model syntax from \@ref(w3csyntax) by adding definitions of the two
hypothesized IEs from `PeerAtt` to `SelfEst`.

<details>
  <summary>Click to show code</summary>

You can use any labeling scheme that makes sense to you, but I recommend adopting
some kind of systematic rule. Here, I will label the individual estimates in 
terms of the short variable names used in the path diagram above. 

```{r}
mod <- '
## Equation for outcome:
SelfEst ~ y_m21 * ProSoc + y_m22 * Aggr + Emp + ParAtt + PeerAtt

## Equations for stage 2 mediators:
ProSoc ~ m21_x2 * PeerAtt + ParAtt + m21_m1 * Emp
Aggr ~ m22_x2 * PeerAtt + ParAtt + m22_m1 * Emp

## Equation for stage 1 mediator:
Emp ~ ParAtt + m1_x2 * PeerAtt

## Covariances:
ProSoc ~~ Aggr
ParAtt ~~ PeerAtt

## Indirect effects:
ie_pro := m1_x2 * m21_m1 * y_m21
ie_agg := m1_x2 * m22_m1 * y_m22
'
```

</details>

---

#### {#w3ctest}

Use `lavaan::sem()` to estimate the model with the IEs defined.

- Use the default settings for `sem()`.
- Are the hypothesized IEs significant according to the default tests?

<details>
  <summary>Click to show code</summary>

```{r}
out <- sem(mod, data = seData)
summary(out)
```

```{r, include = FALSE}
tmp <- parameterEstimates(out)

pro <- tmp %>% filter(label == "ie_pro")
agg <- tmp %>% filter(label == "ie_agg")
```
  
  <details>
    <summary>Click for explanation</summary>
    
  The IE of *Peer Attachment* on *Self Esteem* through *Empathy* and *Prosocial 
  Behavior* is significant ($\hat{\textit{IE}} = `r pro$est %>% round(3)`$, 
  $Z = `r pro$z %>% round(2)`$, $p = `r (pro$p / 2) %>% round(3)`$), as is the 
  analogous IE through *Aggressive Behavior* 
  ($\hat{\textit{IE}} = `r agg$est %>% round(3)`$,
  $Z = `r agg$z %>% round(2)`$, $p = `r (agg$p / 2) %>% round(3)`$). Though,
  this latter effect is just barely significant at the $\alpha = 0.05$ level.

  </details>

</details>

---

The tests we used to evaluate the significance of the IEs in \@ref(w3ctest) are 
flawed because they assume normal sampling distributions for the IEs. However 
the IEs are defined as products of multiple, normally distributed, regression 
slopes. So the IEs themselves cannot be normally distributed (at least in finite
samples), and the results of the normal-theory significance tests may be 
misleading. 

To get an accurate test of the IEs, we should use *bootstrapping* to generate an 
empirical sampling distribution for each IE. In **lavaan**, we implement 
bootstrapping by specifying the `se = "bootstrap"` option in the fitting 
function (i.e., the `cfa()` or `sem()` function) and specifying the number of 
bootstrap samples via the `bootstrap` option. 

***Workflow Tip*** 

To draw reliable conclusions from bootstrapped results, we need many 
bootstrap samples (i.e., B > 1000), but we must estimate the full model for each 
of these samples, so the estimation can take a long time. To avoid too much 
frustration, you should first estimate the model without bootstrapping to make 
sure everything is specified correctly. Only after you are certain that your 
code is correct do you want to run the full bootstrapped version.

---

#### {#w3cboot}

Re-estimate the model from \@ref(w3ctest) using 1000 bootstrap samples.

- Other than the `se` and `bootstrap` options, use the defaults.
- Are the hypothesized IEs significant according to the bootstrap-based test
statistics?

<details>
  <summary>Click to show code</summary>

```{r boot_estimate, cache = TRUE}
## Set a seed to get replicable bootstrap samples:
set.seed(235711)

## Estimate the model with bootstrapping:
out_boot <- sem(mod, data = seData, se = "bootstrap", bootstrap = 1000)

## Summarize the model:
summary(out_boot)
```

  <details>
    <summary>Click for explanation</summary>

```{r, include = FALSE}
tmp <- parameterEstimates(out_boot)

pro <- tmp %>% filter(label == "ie_pro")
agg <- tmp %>% filter(label == "ie_agg")
```

  As with the normal-theory tests, the hypothesized IE of *Peer Attachment* on
  *Self Esteem* was significant ($\hat{\textit{IE}} = `r pro$est %>% round(3)`$, 
  $Z = `r pro$z %>% round(2)`$,
  $p = `r (pro$p / 2) %>% round(3)`$), but the IE of *Aggressive Behavior* has
  crossed into nonsignificant territory
  ($\hat{\textit{IE}} = `r agg$est %>% round(3)`$, 
  $Z = `r agg$z %>% round(2)`$,
  $p = `r (agg$p / 2) %>% round(3)`$).

  *Note:* Bootstrapping is a stochastic method, so each run can provide different
  results. Since the indirect effect of aggressive behavior is so close to the 
  critical value, you may come to a different conclusions vis-á-vis statistical
  significance if you run this analysis with a different random number seed or a 
  different number of bootstrap samples.

  </details>

</details>

---

When you use the `summary()` function to summarize the bootstrapped model from 
\@ref(w3cboot), the output will probably look pretty much the same as it did in 
\@ref(w3ctest), but it's not. The standard errors and test statistics in the 
bootstrapped summary are derived from empirical sampling distributions, whereas 
these values are based on an assumed normal sampling distribution in
\@ref(w3ctest).

The standard method of testing IEs with bootstrapping is to compute confidence 
intervals (CIs) from the empirical sampling distribution of the IEs. In
**lavaan**, we can compute basic (percentile, 95%) CIs by adding the `ci = TRUE`
option to the `summary()` function. To evaluate our directional hypotheses at an 
$\alpha = 0.05$ level, however, we need to compute 90% CIs. We can get more 
control over the summary statistics (include the CIs) with the 
`parameterEstimates()` function. 

---

####

Check the documentation for `lavaan::parameterEstimates()`.

<details>
  <summary>Click to show code</summary>

```{r, eval = FALSE}
?parameterEstimates
```

</details>

---

####

Use the `parameterEstimates()` function to compute bootstrapped CIs for the 
hypothesized IEs.

- Compute percentile CIs.
- Are the IEs significant according to the bootstrapped CIs?

<details>
  <summary>Click to show code</summary>

```{r}
parameterEstimates(out_boot, ci = TRUE, level = 0.9)
```
  
  <details>
    <summary>Click for explanation</summary>
    
```{r, include = FALSE}
tmp <- parameterEstimates(out_boot, ci = TRUE, level = 0.9)

pro <- tmp %>% filter(label == "ie_pro")
agg <- tmp %>% filter(label == "ie_agg")
```

When evaluating a directional hypothesis with a CI, we only consider one of the
interval's boundaries. 

- For a hypothesized positive effect, we check only if the lower boundary is
  greater than zero. 
- For a hypothesized negative effect, we check if the upper boundary is less 
  than zero.

As with the previous tests, the IE of *Peer Attachment* on *Self Esteem*
through *Empathy* and *Prosocial Behavior* is significant 
($\hat{\textit{IE}} = `r pro$est %>% round(3)`$, 
$95\% ~ CI = [`r pro$ci.lower %>% round(3)`; \infty]$), but the analogous IE 
through *Aggressive Behavior* is not quite significant
($\hat{\textit{IE}} = `r agg$est %>% round(3)`$, 
$95\% ~ CI = [-\infty; `r agg$ci.upper %>% round(3)`]$).

  </details>
    
</details>

---

####

Based on the analyses you've conducted here, what do you conclude vis-à-vis the 
original hypotheses?

<details>
  <summary>Click for explanation</summary>

When using normal-theory tests, both hypothesized indirect effects between 
*Peer Attachment* and *Self Esteem* were supported in that the IE through 
*Empathy* and *Prosocial Behavior* as well as the IE through *Empathy* and 
*Aggressive Behavior* were both significant. The hypothesized direct effect of 
*Parent Attachment* on *Self Esteem* was also born out via a significant direct
effect in the model.

When testing the indirect effects with bootstrapping, however, the effect through
*Aggressive Behavior* was nonsignificant. Since bootstrapping gives a more
accurate test of the indirect effect, we should probably trust these results
more than the normal-theory results. We should not infer a significant indirect
effect of *Peer Attachment* on *Self Esteem* transmitted through *Empathy* and
*Aggressive Behavior*.

These results may not tell the whole story, though. We have not tested for 
indirect effects between *Parent Attachment* and *Self Esteem*, and we have not 
evaluated simpler indirect effects between *Peer Attachment* and *Self Esteem*
(e.g., `PeerAtt` $\rightarrow$ `Emp` $\rightarrow$ `SelfEst`). 

</details>

---

### Moderation

---

```{r, include = FALSE}
outlook <- readRDS(paste0(dataDir, "outlook.rds"))
```

We will first analyze a synthetic version of the [*Outlook on Life Survey*][outlook0]
data. The original data were collected in the United States in 2012 to measure, 
among other things, attitudes about racial issues, opinions of the Federal 
government, and beliefs about the future.

We will work with a synthesized subset of the original data. You can access
these synthetic data as [*outlook.rds*][outlook1]. This dataset comprises 
`r nrow(outlook)` observations of the following `r ncol(outlook)` variables.

- `d1:d3`: Three observed indicators of a construct measuring disillusionment 
with the US Federal government.
   - Higher scores indicate more disillusionment
- `s1:s4`: Four observed indicators of a construct measuring the perceived 
achievability of material success.
   - Higher scores indicate greater perceived achievability
- `progress`: A single item assessing perceived progress toward achieving the 
"American Dream"
   - Higher scores indicate greater perceived progress
- `merit`: A single item assessing endorsement of the meritocratic ideal that 
hard work leads to success.
   - Higher scores indicate stronger endorsement of the meritocratic ideal
- `lib2Con`: A single item assessing liberal-to-conservative orientation
   - Lower scores are more liberal, higher scores are more conservative
- `party`: A four-level factor indicating self-reported political party affiliation
- `disillusion`: A scale score representing disillusionment with the US Federal 
government
   - Created as the mean of `d1:d3`
- `success`: A scale score representing the perceived achievability of material 
success
   - Created as the mean of `s1:s4`

To satisfy the [access and licensing][access] conditions under which the original 
data are distributed, the data contained in *outlook.rds* were synthesized from 
the original variables using the methods described by [Volker and Vink (2021)][miceSyn]. 
You can access the original data [here][outlook0], and you can access the code 
used to process the data [here][outlook_code].

---

#### 

Read in the `outlook.rds` dataset.

*Hint:* An RDS file is an R object that's been saved to a file. To read in this
type of file, we use `readRDS()` from base R.

<details>
  <summary>Click to show code</summary>
  
```{r, eval=FALSE}
outlook <- readRDS("outlook.rds")
```
  
</details>

---

####

Summarize the outlook data to get a sense of their characteristics.

<details>
  <summary>Click to show code</summary>
  
```{r}
head(outlook)
summary(outlook)
str(outlook)
```
  
</details>

---

We will first use OLS regression to estimate a model encoding the following relations:

- Belief in the achievability of success, *success*, predicts perceived progress 
toward the American Dream, *progress*, as the focal effect.
- Disillusionment with the US Federal government, *disillusion* moderates the 
*success* $\rightarrow$ *progress* effect.
- Placement on the liberal-to-conservative continuum, *lib2Con* is partialed out 
as a covariate.

---

####

Draw the conceptual path diagram for the model described above.

<details>
  <summary>Click for explanation</summary>
  
```{r, echo=FALSE}
knitr::include_graphics("images/lab5_conceptual1.png")
```
  
</details>

---

####

Write out the regression equation necessary to evaluate the moderation 
hypothesis described above.

<details>
  <summary>Click for explanation</summary>

\[
Y_{progress} = \beta_0 + \beta_1 W_{lib2Con} + \beta_2 X_{success} + \beta_3 Z_{disillusion} + \beta_4 XZ + \varepsilon
\]
  
</details>

---

#### {#olsFit}

Use `lm()` to estimate the moderated regression model via OLS regression.

<details>
  <summary>Click to show code</summary>

```{r, cache.lazy = FALSE}
olsFit <- lm(progress ~ lib2Con + success * disillusion, data = outlook)
```

</details>

---

####

Summarize the fitted model and interpret the results.

- Is the moderation hypothesis supported?
- How does disillusionment level affect the focal effect?

<details>
  <summary>Click to show code</summary>
  
```{r}
summary(olsFit)
```
  
  <details>
    <summary>Click for explanation</summary>
    
```{r, include = FALSE}
tmp <- summary(olsFit)$coef

b  <- tmp["success:disillusion", "Estimate"] %>% round(3)
t  <- tmp["success:disillusion", "t value"] %>% round(3)
df <- olsFit$df.residual
p  <- tmp["success:disillusion", "Pr(>|t|)"] %>% round(3)
```

  Yes, *disillusion* significantly moderates the relation between *success* and 
  *progress* ($\beta = `r b`$, $t[`r df`] = `r t`$, $p = `r p`$) such that the
  effect of *success* on *progress* increases as levels of *disillusion*
  increase, after controlling for *lib2Con*.
 
  </details>

</details>

---

The [**rockchalk**][rockchalk] package contains some useful routines for probing
interactions estimated via `lm()`. Specifically, the `plotslopes()` function will
estimate and plot simple slopes, and the `testSlopes()` function tests the simple
slopes estimated by `plotSlopes()`.

---

#### {#ssOls}

Probe the interaction.

- Use the `plotSlopes()` and `testSlopes()` functions from the **rockchalk** 
package to conduct a simple slopes analysis for the model from \@ref(olsFit).

<details>
  <summary>Click to show code</summary>

```{r}
library(rockchalk)

## Estimate and plot simple slopes:
psOut <- plotSlopes(olsFit,
                    plotx    = "success", 
                    modx     = "disillusion",
                    modxVals = "std.dev")

## Test the simple slopes:
tsOut <- testSlopes(psOut)

## View the results:
tsOut$hypotests
```

*Note:* The message printed by `testSlopes()` gives the boundaries of the 
*Johnson-Neyman Region of Significance* [(Johnson & Neyman, 1936)][jn]. 
Johnson-Neyman analysis is an alternative method of probing interactions that we 
have not covered in this course. For more information, check out 
[Preacher, et al. (2006)][preacher_et_al_2006].

</details>

---

We will now use **lavaan** to estimate the moderated regression model from above 
as a path analysis.

---

####

Define the model syntax for the path analytic version of the model described
above.

- Parameterize the model as in the OLS regression.
- Use only observed items and scale scores.

<details>
  <summary>Click to show code</summary>

```{r}
pathMod <- '
progress ~ 1 + lib2Con + success + disillusion + success:disillusion
'
```

</details>

---

#### {#pathFit}

Estimate the path model on the outlook data.

<details>
  <summary>Click to show code</summary>

```{r}
pathFit <- sem(pathMod, data = outlook)
```

</details>

---

####

Summarize the fitted path model and interpret the results.

- Do the results match the OLS regression results?
- What proportion of the variability in *progress* is explained by this model?

*Hint:* the function `lavInspect()` can be used to extract information from
models

<details>
  <summary>Click to show code</summary>
  
```{r}
summary(pathFit)
lavInspect(pathFit, "r2")
```
  
  <details>
    <summary>Click for explanation</summary>

  Yes, the estimates and inferential conclusions are all the same as in the OLS 
  regression model. The model explains 
  `r (100 * lavInspect(pathFit, "r2")) %>% round(2)`\% of the variability in 
  *progress*.
  
  </details>
</details>

---

The [**semTools**][semTools] package contains some helpful routines for probing 
interactions estimated via the `lavaan()` function (or one of it's wrappers). 
Specifically, the `probe2WayMC()` and `plotProbe()` functions will estimate/test 
simple slopes and plot the estimated simple slopes, respectively.

---

####

Probe the interaction from \@ref(pathFit) using *semTools* utilities.

- Use `probe2WayMC()` to estimate and test the simple slopes.
- Use `plotProbe()` to visualize the simple slopes.
- Define the simple slopes with the same conditional values of *disillusion*
  that you used in \@ref(ssOls). 
- Which simple slopes are significant?
- Do these results match the results from \@ref(ssOls)?

<details>
  <summary>Click to show code</summary>
  
```{r}
library(semTools)

## Define the conditional values at which to calculate simple slopes:
condVals <- summarise(outlook,
                      "m-sd" = mean(disillusion) - sd(disillusion),
                      mean = mean(disillusion),
                      "m+sd" = mean(disillusion) + sd(disillusion)
                      ) %>% unlist()
 
## Compute simple slopes and intercepts:
ssOut <- probe2WayMC(pathFit, 
                     nameX    = c("success", "disillusion", "success:disillusion"), 
                     nameY    = "progress", 
                     modVar   = "disillusion",
                     valProbe = condVals)

## Check the results:
ssOut

## Visualize the simple slopes:
plotProbe(ssOut,
          xlim = range(outlook$success), 
          xlab = "Ease of Personal Success", 
          ylab = "Progress toward American Dream",
          legendArgs = list(legend = names(condVals))
          )
```
  
  <details>
    <summary>Click for explanation</summary>

Each of the simple slopes is significant. As level of disillusionment increases,
the effect of *success* on *progress* also increases, and this effect is
significant for all levels of *disillusion* considered here. These results
match the simple slopes from the OLS regression analysis.

  </details>
</details>

--------------------------------------------------------------------------------

End of In-Class Exercises

--------------------------------------------------------------------------------

[lavaan]: https://cran.r-project.org/web/packages/lavaan/
[outlook0]: https://doi.org/10.3886/ICPSR35348.v1
[outlook1]: https://github.com/kylelang/lavaan-e-learning/raw/main/5_moderation/data/outlook.rds
[access]: https://www.icpsr.umich.edu/web/pages/datamanagement/lifecycle/access.html
[miceSyn]: https://doi.org/10.3390/psych3040045
[outlook_code]: https://github.com/kylelang/lavaan-e-learning/blob/main/code/lab_prep/process_outlook_data.R
[rockchalk]: https://cran.r-project.org/web/packages/rockchalk/index.html
[jn]: http://www.worldcat.org/oclc/29082872
[preacher_et_al_2006]: https://doi.org/10.3102%2F10769986031004437
[semTools]: https://cran.r-project.org/web/packages/semTools/index.html
[hs_data]: https://github.com/kylelang/lavaan-e-learning/raw/main/data/holzinger_swineford.rds
[mbess]: https://cran.r-project.org/web/packages/MBESS/index.html
[hs_code]: https://github.com/kylelang/lavaan-e-learning/blob/main/code/lab_prep/process_hs_data.R
[psych]: https://cran.r-project.org/web/packages/psych/index.html
