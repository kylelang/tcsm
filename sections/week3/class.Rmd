## In-Class Exercises

```{r include = FALSE}
library(haven)
library(lavaan)
library(tidySEM)
library(dplyr)
library(rockchalk)


seData <- read_sav(paste0(dataDir, "SelfEsteem.sav"))
```

---

### Part 1: Mediation {.unnumbered}

---


In this practical, we'll go back to the data from the at-home exercises from
*SelfEsteem.sav*. A reminder -- these data comprise 143 observations of the
following variables. 

- *case*: Participant ID number
- *ParAtt*: Parental Attachment
- *PeerAtt*: Peer Attachment
- *Emp*: Empathy
- *ProSoc*: Prosocial behavior
- *Aggr*: Aggression
- *SelfEst*: Self-esteem

When we last worked with the data, we built a model with one mediator (*Emp*),
creating indirect effects between our predictors *ParAtt* and *PeerAtt*, and our
outcome variable *SelfEst*.

---

### 

Load the data into the object `seData` using `haven::read_sav()`

<details>
  <summary>Click to show code</summary>

```{r, eval = FALSE}
library(haven)
seData <- read_sav("SelfEsteem.sav")
```
</details>
---

For this analysis, we are interested in the (indirect) effects of parental and 
peer attachment on self-esteem. Furthermore, we want to evaluate the mediating 
roles of empathy and social behavior (i.e., prosocial behavior and aggression).

Specifically, we have the following hypotheses.

1. Better peer relationships will promote higher self-esteem via a three-step 
indirect process.
    a. Better peer relationships will increase empathy levels.
    a. Higher empathy will increase prosocial behavior and decrease aggressive 
    behavior.
    a. More prosocial behaviors and less aggressive behavior will both produce 
    higher self-esteem.
1. Better relationships with parents directly increase self-esteem.

---

To evaluate these hypotheses, we will use **lavaan** to estimate the following 
multiple mediator model as a path model. 

```{r, echo=FALSE}
modex <-
'
## Equation for outcome:
SelfEst ~ y_m21 * ProSoc + y_m22 * Aggr + Emp + ParAtt + PeerAtt

## Equations for stage 2 mediators:
ProSoc ~ m21_x2 * PeerAtt + ParAtt + m21_m1 * Emp
Aggr ~ m22_x2 * PeerAtt + ParAtt + m22_m1 * Emp

## Equation for stage 1 mediator:
Emp ~ ParAtt + m1_x2 * PeerAtt

## Covariances:
ProSoc ~~ Aggr
ParAtt ~~ PeerAtt
'

out <- sem(modex, data = seData)

prepare_graph(out,
              layout = get_layout(
                NA,        NA, NA,    "ProSoc", NA,
                "ParAtt",  NA, NA,    NA,      NA,
                NA,        NA, "Emp", NA,      "SelfEst",
                "PeerAtt", NA, NA,    NA,      NA,
                NA,        NA, NA,    "Aggr",  NA,
                rows = 5
              ),
              angle = 0,
              spacing_x = 2,
              text_size = 3,
              rect_height = 1.5) %>% 
  if_nodes(condition = {shape == "rect"},
           expr = {label = c("Aggressive\nBehavior\nM22", "Empathy\nM1", "Parent\nAttachment\nX1", "Peer\nAttachment\nX2", "Prosocial\nBehavior\nM21", "Self Esteem\nY")}) %>% 
  edit_edges({
    label <- NA
  }) %>%
  all_cov({
    arrow <- "both"
  }) %>% 
  hide_var() %>% 
  plot()  
```
<!-- ![](images/week5_home_full_model.png) -->

---

### {#w3csyntax}

Specify the lavaan model syntax implied by the path diagram shown above.

- Save the resulting character string as an object in your environment.

<details>
  <summary>Click to show code</summary>

```{r}
modb <- '
## Equation for outcome:
SelfEst ~ ProSoc + Aggr + Emp + ParAtt + PeerAtt

## Equations for stage 2 mediators:
ProSoc ~ PeerAtt + ParAtt + Emp
Aggr ~ PeerAtt + ParAtt + Emp

## Equation for stage 1 mediator:
Emp ~ ParAtt + PeerAtt

## Covariances:
ProSoc ~~ Aggr
ParAtt ~~ PeerAtt
'
```

</details>

---

### {#w3cfit}

Use the `lavaan::sem()` function to estimate the model defined in \@ref(w3csyntax).

- Use the default settings in `sem()`.
- Summarize the fitted model.
- Do you notice anything special about the model fit? If so, can you explain?

<details>
  <summary>Click to show code</summary>

```{r}
library(lavaan)
out <- sem(modb, data = seData)
summary(out, fit.measures = TRUE)
```
  
  <details>
    <summary>Click for explanation</summary>
    
  The model fits the data perfectly because the model is saturated (i.e., we
  have estimated all possible paths). So the model-implied covariance matrix
  exactly replicates the observed covariance matrix. We can tell that the model
  is saturated because the degrees of freedom for the $\chi^2$ statistic are 0.
    
  </details>

</details>

---

### 

Considering the parameter estimates from \@ref(w3cfit), what can you say about the 
hypotheses? 

<details>
  <summary>Click for explanation</summary>

```{r, include = FALSE}
tmp <- parameterEstimates(out)

parent <- tmp %>% filter(lhs == "SelfEst", op == "~", rhs == "ParAtt") 
peer   <- tmp %>% filter(lhs == "SelfEst", op == "~", rhs == "PeerAtt") 
```

Notice that all of the hypotheses stated above are explicitly directional. Hence,
when evaluating the significance of the structural paths that speak to these
hypotheses, we should use one-tailed tests. We cannot ask **lavaan** to return 
one-tailed p-values, but we have no need to do so. We can simply divide the 
two-tailed p-values in half.

The significant direct effect of `ParAtt` on `SelfEst` 
($\beta = `r parent$est %>% round(3)`$, 
$Z = `r parent$z %>% round(2)`$,
$p = `r (parent$p / 2) %>% round(3)`$) and the lack of a significant direct 
effect of `PeerAtt` on `SelfEst` ($\beta = `r peer$est %>% round(3)`$, 
$Z = `r peer$z %>% round(3)`$,
$p = `r (peer$p / 2) %>% round(3)`$) align with our hypotheses.

The remaining patterns of individual estimates also seem to conform to the 
hypotheses (e.g., all of the individual paths comprising the indirect effects of 
`PeerAtt` on `SelfEst` are significant). We cannot make any firm conclusions 
until we actually estimate and test the indirect effects, though.

</details>

---

Remember that an indirect effect (IE) is the product of multiple regression 
slopes. Therefore, to estimate an IE, we must define these products in our 
model syntax. In **lavaan**, we define the new IE parameters in two steps. 

1. Label the relevant regression paths.
1. Use the labels to define new parameters that represent the desired IEs.

We can define new parameters in lavaan model syntax via the `:=` operator. The 
**lavaan** website contains a tutorial on this procedure: <http://lavaan.ugent.be/tutorial/mediation.html>

---

###

Use the procedure described above to modify the model syntax from \@ref(w3csyntax) 
by adding definitions of the two hypothesized IEs from `PeerAtt` to `SelfEst`.

<details>
  <summary>Click to show code</summary>

You can use any labeling scheme that makes sense to you, but I recommend adopting
some kind of systematic rule. Here, I will label the individual estimates in 
terms of the short variable names used in the path diagram above. 

Notice that I only label the parameters that I will use to define the IEs. You 
are free to label any parameter that you like, but I choose the to label only 
the minimally sufficient set to avoid cluttering the code/output.

```{r}
mod <- '
## Equation for outcome:
SelfEst ~ y_m21 * ProSoc + y_m22 * Aggr + Emp + ParAtt + PeerAtt

## Equations for stage 2 mediators:
ProSoc ~ m21_x2 * PeerAtt + ParAtt + m21_m1 * Emp
Aggr ~ m22_x2 * PeerAtt + ParAtt + m22_m1 * Emp

## Equation for stage 1 mediator:
Emp ~ ParAtt + m1_x2 * PeerAtt

## Covariances:
ProSoc ~~ Aggr
ParAtt ~~ PeerAtt

## Indirect effects:
ie_pro := m1_x2 * m21_m1 * y_m21
ie_agg := m1_x2 * m22_m1 * y_m22
'
```

</details>

---

### {#w3ctest}

Use `lavaan::sem()` to estimate the model with the IEs defined.

- Use the default settings for `sem()`.
- Are the hypothesized IEs significant according to the default tests?

<details>
  <summary>Click to show code</summary>

```{r}
out <- sem(mod, data = seData)
summary(out)
```

```{r, include = FALSE}
tmp <- parameterEstimates(out)

pro <- tmp %>% filter(label == "ie_pro")
agg <- tmp %>% filter(label == "ie_agg")
```
  
  <details>
    <summary>Click for explanation</summary>
    
  The IE of *Peer Attachment* on *Self Esteem* through *Empathy* and *Prosocial 
  Behavior* is significant ($\hat{\textit{IE}} = `r pro$est %>% round(3)`$, 
  $Z = `r pro$z %>% round(2)`$, $p = `r (pro$p / 2) %>% round(3)`$), as is the 
  analogous IE through *Aggressive Behavior* 
  ($\hat{\textit{IE}} = `r agg$est %>% round(3)`$, 
  $Z = `r agg$z %>% round(2)`$, $p = `r (agg$p / 2) %>% round(3)`$).

  </details>

</details>

---

The tests we used to evaluate the significance of the IEs in \@ref(w3ctest) are 
flawed because they assume normal sampling distributions for the IEs. However 
the IEs are defined as products of multiple, normally distributed, regression 
slopes. So the IEs themselves cannot be normally distributed (at least in finite
samples), and the results of the normal-theory significance tests may be 
misleading. 

To get an accurate test of the IEs, we should use *bootstrapping* to generate an 
empirical sampling distribution for each IE. In **lavaan**, we implement 
bootstrapping by specifying the `se = "bootstrap"` option in the fitting 
function (i.e., the `cfa()` or `sem()` function) and specifying the number of 
bootstrap samples via the `bootstrap` option. 

***Workflow Tip*** 

To draw reliable conclusions from bootstrapped results, we need many 
bootstrap samples (i.e., B > 1000), but we must estimate the full model for each 
of these samples, so the estimation can take a long time. To avoid too much 
frustration, you should first estimate the model without bootstrapping to make 
sure everything is specified correctly. Only after you are certain that your 
code is correct do you want to run the full bootstrapped version.

---

### {#w3cboot}

Re-estimate the model from \@ref(w3ctest) using 1000 bootstrap samples.

- Other than the `se` and `bootstrap` options, use the defaults.
- Are the hypothesized IEs significant according to the bootstrap-based test
statistics?

<details>
  <summary>Click to show code</summary>

```{r boot_estimate, cache = TRUE}
## Set a seed to get replicable bootstrap samples:
set.seed(235711)

## Estimate the model with bootstrapping:
out_boot <- sem(mod, data = seData, se = "bootstrap", bootstrap = 1000)

## Summarize the model:
summary(out_boot)
```

  <details>
    <summary>Click for explanation</summary>

```{r, include = FALSE}
tmp <- parameterEstimates(out_boot)

pro <- tmp %>% filter(label == "ie_pro")
agg <- tmp %>% filter(label == "ie_agg")
```

  As with the normal-theory tests, both of the hypothesized IEs of *Peer Attachment* 
  on *Self Esteem* are significant (*Prosocial Behavior*:
  $\hat{\textit{IE}} = `r pro$est %>% round(3)`$, 
  $Z = `r pro$z %>% round(2)`$,
  $p = `r (pro$p / 2) %>% round(3)`$
  ; *Aggressive Behavior*: 
  $\hat{\textit{IE}} = `r agg$est %>% round(3)`$, 
  $Z = `r agg$z %>% round(2)`$,
  $p = `r (agg$p / 2) %>% round(3)`$).
    
  </details>

</details>

---

When you use the `summary()` function to summarize the bootstrapped model from 
\@ref(w3cboot), the output will probably look pretty much the same as it did in 
\@ref(w3ctest), but it's not. The standard errors and test statistics in the 
bootstrapped summary are derived from empirical sampling distributions, whereas 
these values are based on an assumed normal sampling distribution in
\@ref(w3ctest).

The standard method of testing IEs with bootstrapping is to compute confidence 
intervals (CIs) from the empirical sampling distribution of the IEs. In
**lavaan**, we can compute basic (percentile, 95%) CIs by adding the `ci = TRUE`
option to the `summary()` function. To evaluate our directional hypotheses at an 
$\alpha = 0.05$ level, however, we need to compute 90% CIs. We can get more 
control over the summary statistics (include the CIs) with the 
`parameterEstimates()` function. 

---

###

Check the documentation for `lavaan::parameterEstimates()`.

<details>
  <summary>Click to show code</summary>

```{r, eval = FALSE}
?parameterEstimates
```

</details>

---

###

Use the `parameterEstimates()` function to compute bootstrapped CIs for the 
hypothesized IEs.

- Compute percentile CIs.
- Are the IEs significant according to the bootstrapped CIs?

<details>
  <summary>Click to show code</summary>

```{r}
parameterEstimates(out_boot, ci = TRUE, level = 0.9)
```
  
  <details>
    <summary>Click for explanation</summary>
    
```{r, include = FALSE}
tmp <- parameterEstimates(out_boot, ci = TRUE, level = 0.9)

pro <- tmp %>% filter(label == "ie_pro")
agg <- tmp %>% filter(label == "ie_agg")
```

  When evaluating a directional hypothesis with a CI, we only consider one of the
  interval's boundaries. 

  - For a hypothesized positive effect, we check only if the lower boundary is 
  greater than zero. 
  - For a hypothesized negative effect, we check if the upper boundary is less 
  than zero.

  As with the previous tests, the IE of *Peer Attachment* on *Self Esteem*
  through *Empathy* and *Prosocial Behavior* is significant 
  ($\hat{\textit{IE}} = `r pro$est %>% round(3)`$, 
  $95\% ~ CI = [`r pro$ci.lower %>% round(3)`; \infty]$), as is the analogous IE 
  through *Aggressive Behavior* ($\hat{\textit{IE}} = `r agg$est %>% round(3)`$, 
  $95\% ~ CI = [-\infty; `r agg$ci.upper %>% round(3)`]$).

  </details>
    
</details>

---

###

Based on the analyses you've conducted here, what do you conclude vis-Ã -vis the 
original hypotheses?

<details>
  <summary>Click for explanation</summary>

The hypothesized indirect effects between *Peer Attachment*  and *Self Esteem* 
were supported in that the IE through *Empathy* and *Prosocial Behavior* as well
as the IE through *Empathy* and *Aggressive Behavior* were both significant. The 
hypothesized direct effect of *Parent Attachment* on *Self Esteem* was also born 
out via a significant direct effect in the model.

These results may not tell the whole story, though. We have not tested for 
indirect effects between *Parent Attachment* and *Self Esteem*, and we have not 
evaluated simpler indirect effects between *Peer Attachment* and *Self Esteem*
(e.g., `PeerAtt` $\rightarrow$ `Emp` $\rightarrow$ `SelfEst`). 

</details>

---

### Part 2: Moderation {.unnumbered}

---

In this part of the lab, you will practice moderation analysis using path
analysis and SEM in [**lavaan**][lavaan]. You will work with both continuous and
categorical moderators.

---

### Continuous Moderators {.unnumbered}


```{r, include = FALSE}
dataDir <- "../../../../data/"
outlook <- readRDS(paste0(dataDir, "outlook.rds"))
head(outlook)
```
We will first analyze a synthetic version of the [*Outlook on Life Survey*][outlook0]
data. The original data were collected in the United States in 2012 to measure, 
among other things, attitudes about racial issues, opinions of the Federal 
government, and beliefs about the future.

We will work with a synthesized subset of the original data. You can access
these synthetic data as [*outlook.rds*][outlook1]. This dataset comprises 
`r nrow(outlook)` observations of the following `r ncol(outlook)` variables.

- `d1:d3`: Three observed indicators of a construct measuring disillusionment 
with the US Federal government.
   - Higher scores indicate more disillusionment
   $\vb$
- `s1:s4`: Four observed indicators of a construct measuring the perceived 
achievability of material success.
   - Higher scores indicate greater perceived achievability
   $\vb$
- `progress`: A single item assessing perceived progress toward achieving the 
"American Dream"
   - Higher scores indicate greater perceived progress
   $\vb$
- `merit`: A single item assessing endorsement of the meritocratic ideal that 
hard work leads to success.
   - Higher scores indicate stronger endorsement of the meritocratic ideal
   $\vb$
- `lib2Con`: A single item assessing liberal-to-conservative orientation
   - Lower scores are more liberal, higher scores are more conservative
   $\vb$
- `party`: A four-level factor indicating self-reported political party affiliation
   $\vb$
- `disillusion`: A scale score representing disillusionment with the US Federal 
government
   - Created as the mean of `d1:d3`
   $\vb$
- `success`: A scale score representing the perceived achievability of material 
success
   - Created as the mean of `s1:s4`

To satisfy the [access and licensing][access] conditions under which the original 
data are distributed, the data contained in *outlook.rds* were synthesized from 
the original variables using the methods described by [Volker and Vink (2021)][miceSyn]. 
You can access the original data [here][outlook0], and you can access the code 
used to process the data [here][outlook_code].

---


### 

Read in the `outlook.rds` dataset.

*Hint* An .rds file is an R object that's been saved to a file. To read in this
type of file, we use `readRDS()` from base R.

<details>
  <summary>Click to show code</summary>
  
```{r, eval=FALSE}
outlook <- readRDS("outlook.rds")
```
  
</details>

---

###

Summarize the outlook data to get a sense of their characteristics.

<details>
  <summary>Click to show code</summary>
  
```{r}
head(outlook)
summary(outlook)
str(outlook)
```
  
</details>

---

### OLS Regression {.unnumbered}

We will first consider moderation by a continuous variable. Specifically, we 
will work with a model encoding the following relations:

- Belief in the achievability of success, *success*, predicts perceived progress 
toward the American Dream, *progress*, as the focal effect.
- Disillusionment with the US Federal government, *disillusion* moderates the 
*success* $\rightarrow$ *progress* effect.
- Placement on the liberal-to-conservative continuum, *lib2Con* is partialed out 
as a covariate.

We will first estimate this model as an OLS regression model.

---

###

Draw the conceptual path diagram for the model described above.

<details>
  <summary>Click for explanation</summary>
  
```{r, echo=FALSE}
knitr::include_graphics("images/lab5_conceptual1.png")
```
  
</details>

---

###

Write out the regression equation necessary to evaluate the moderation 
hypothesis described above.

<details>
  <summary>Click for explanation</summary>
```{asis}
\[
Y_{progress} = \beta_0 + \beta_1 W_{lib2Con} + \beta_2 X_{success} + \beta_3 Z_{disillusion} + \beta_4 XZ + \varepsilon
\]
```
  
</details>

---

### {#olsFit}

Estimate the moderated regression model via OLS regression.

*Hint* Use the `lm()` function to estimate the model.

<details>
  <summary>Click to show code</summary>

```{r}
olsFit <- lm(progress ~ lib2Con + success * disillusion, data = outlook)
```

</details>

---

###

Summarize the fitted model and interpret the results.

- Is the moderation hypothesis supported?
- How does disillusionment level affect the focal effect?

<details>
  <summary>Click to show code</summary>
  
```{r}
summary(olsFit)
```
  
  <details>
    <summary>Click for explanation</summary>
    
```{r, include = FALSE}
tmp <- summary(olsFit)$coef

b  <- tmp["success:disillusion", "Estimate"] %>% round(3)
t  <- tmp["success:disillusion", "t value"] %>% round(3)
df <- olsFit$df.residual
p  <- tmp["success:disillusion", "Pr(>|t|)"] %>% round(3)
```

  Yes, *disillusion* significantly moderates the relation between *success* and 
  *progress* (
  $\beta = `r b`$, $t(`r df`) = `r t`$, $p = `r p`$
  ) such that the effect of *success* on *progress* increases as levels of 
  *disillusion* increase, after controlling for *lib2Con*.
  
  </details>

</details>

---

The [**rockchalk**][rockchalk] package contains some useful routines for probing
interactions estimated via `lm()`. Specifically, the `plotslopes()` function will
estimate and plot simple slopes, and the `testSlopes()` function tests the simple
slopes estimated by `plotSlopes()`.

---

### {#ssOls}

Probe the interaction.

- Use the `plotSlopes()` and `testSlopes()` functions from the **rockchalk** 
package to conduct a simple slopes analysis for the model from \@ref(olsFit).

<details>
  <summary>Click to show code</summary>

```{r, error=TRUE, cache.lazy=FALSE}
library(rockchalk)


## Estimate and plot simple slopes:
psOut <- plotSlopes(olsFit,
                    plotx    = "success", 
                    modx     = "disillusion",
                    modxVals = "std.dev")

## Test the simple slopes:
tsOut <- testSlopes(psOut)
## View the results:
tsOut$hypotests

```

NOTE: The message printed by `testSlopes()` gives the boundaries of the 
*Johnson-Neyman Region of Significance* ([Johnson & Neyman, 1936)][jn]. 
Johnson-Neyman analysis is an alternative method of probing interactions that we 
have not covered in this course. For more information, check out 
[Preacher, et al. (2006)][preacher_et_al_2006].

</details>

---

### Path analysis {.unnumbered}

We will now use **lavaan** to estimate the moderated regression model from above 
as a path analysis model.

---

###

Define the model syntax for the path analytic version of the model described
above.

- Parameterize the model as in the OLS regression.
- Use only observed items and scale scores.

<details>
  <summary>Click to show code</summary>

```{r}
pathMod <- '
progress ~ 1 + lib2Con + success + disillusion + success:disillusion
'
```

</details>

---

### {#pathFit}

Estimate the path model on the outlook data.

<details>
  <summary>Click to show code</summary>

```{r}
pathFit <- sem(pathMod, data = outlook)
summary(pathFit)
```

</details>

---

###

Summarize the fitted path model and interpret the results.

- Do the results match the OLS regression results?
- What proportion of the variability in *progress* is explained by this model?

*Hint* the function `lavInspect()` can be used to extract information from
models

<details>
  <summary>Click to show code</summary>
  
```{r}
summary(pathFit)
lavInspect(pathFit, "r2")
```
  
  <details>
    <summary>Click for explanation</summary>
  Yes, the estimates and inferential conclusions are all the same as in the OLS 
  regression model.
  The model explains `r (100 * lavInspect(out, "r2")) %>% round(2)`% 
of the variability in *progress*.
  </details>

</details>

---

The [**semTools**][semTools] package contains some helpful routines for probing 
interactions estimated via the `lavaan()` function (or one of it's wrappers). 
Specifically, the `probe2WayMC()` and `plotProbe()` functions will estimate/test 
simple slopes and plot the estimated simple slopes, respectively.

---

###

Probe the interaction from \@ref(pathFit) using *semTools* utilities.

- Use `probe2WayMC()` to estimate and test the simple slopes and intercepts.
- Use `plotProbe()` to visualize the simple slopes.
- Define the simple slopes with the same conditional values of *disillusion* 
that you used in \@ref(ssOls). 
- Which simple slopes are significant?
- Do these results match the results from \@ref(ssOls)?

<details>
  <summary>Click to show code</summary>
  
```{r}
library(semTools)
  
## Compute simple slopes and intercepts:
ssOut <- probe2WayMC(pathFit, 
                     nameX    = c("success", "disillusion", "success:disillusion"), 
                     nameY    = "progress", 
                     modVar   = "disillusion",
                     valProbe = outlook %>% 
                       summarise("m-sd" = mean(disillusion) - sd(disillusion),
                                 mean = mean(disillusion),
                                 "m+sd" = mean(disillusion) + sd(disillusion)
                                 ) %>% 
                       unlist()
                     )

## Check the results:
ssOut

## Visualize the simple slopes:
plotProbe(ssOut,
          xlim = range(outlook$success), 
          xlab = "Ease of Personal Success", 
          ylab = "Progress toward American Dream",
          legendArgs = list(legend = rownames(ssOut[[1]]))
          )

summary(ssOut, rsquare = TRUE)
```
  
  <details>
    <summary>Click for explanation</summary>
  - Each of the simple slopes is significant. As level of disillusionment increases, 
  the effect of *success* on *progress* also increases, and this effect is 
  significant for all levels of *disillusion* considered here.
  - Yes, these results match the simple slopes from the OLS regression analysis.
  </details>

</details>



--------------------------------------------------------------------------------

End of In-Class Exercises 3

--------------------------------------------------------------------------------

[lavaan]: https://cran.r-project.org/web/packages/lavaan/
[outlook0]: https://doi.org/10.3886/ICPSR35348.v1
[outlook1]: https://github.com/kylelang/lavaan-e-learning/raw/main/5_moderation/data/outlook.rds
[access]: https://www.icpsr.umich.edu/web/pages/datamanagement/lifecycle/access.html
[miceSyn]: https://doi.org/10.3390/psych3040045
[outlook_code]: https://github.com/kylelang/lavaan-e-learning/blob/main/code/lab_prep/process_outlook_data.R
[rockchalk]: https://cran.r-project.org/web/packages/rockchalk/index.html
[jn]: http://www.worldcat.org/oclc/29082872
[preacher_et_al_2006]: https://doi.org/10.3102%2F10769986031004437
[semTools]: https://cran.r-project.org/web/packages/semTools/index.html
[hs_data]: https://github.com/kylelang/lavaan-e-learning/raw/main/data/holzinger_swineford.rds
[mbess]: https://cran.r-project.org/web/packages/MBESS/index.html
[hs_code]: https://github.com/kylelang/lavaan-e-learning/blob/main/code/lab_prep/process_hs_data.R
[psych]: https://cran.r-project.org/web/packages/psych/index.html